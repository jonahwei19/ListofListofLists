[
  {
    "filename": "academia-industry-translation",
    "title": "Academia-industry translation",
    "tag": "Science",
    "whatItIs": "Mechanisms to transfer AI security research from academic institutions to commercial applications. Currently the handoff is weak: findings rely on closed weights/data, lack reproducible code, don't have benchmarks tied to deployment constraints, and don't map to security/compliance requirements. Creates standardized processes and incentives to bridge theory and implementation.",
    "whyItMatters": [
      "Academic safety research often never gets deployed due to no systematic pathway",
      "Companies lack resources to translate academic findings; academics aren't incentivized to make work deployment-ready",
      "Potentially valuable safety techniques sit unused while deployed systems remain vulnerable"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Incentive structures; reproducibility requirements; deployment-ready code",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #29: Academia-Industry Translation",
        "url": "../sources/peregrine-2025/interventions/peregrine-029.md"
      }
    ]
  },
  {
    "filename": "academic-project-scaling",
    "title": "Academic project scaling",
    "tag": "Science",
    "whatItIs": "Funding and operational support to scale academic research agendas that work at small scale but aren't expanded because academics prioritize novelty over scaling. MIT's Tenenbaum lab specifically mentioned as having promising results that languished without resources for deployment.",
    "whyItMatters": [
      "Academia incentivizes novelty, not scaling",
      "Many promising approaches are proven at small scale but never tested at production scale",
      "Providing resources for scaling proven approaches unlocks stranded proof-of-concept value"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Academic incentive structures; operational support for scaling",
    "whoWorking": "- **MIT Tenenbaum Lab**: Example candidate with promising unscaled results",
    "sources": [
      {
        "text": "Peregrine 2025 #34: Support Academic Project Scaling",
        "url": "../sources/peregrine-2025/interventions/peregrine-034.md"
      }
    ]
  },
  {
    "filename": "accelerated-vaccine-development",
    "title": "Accelerated vaccine development",
    "tag": "Security",
    "whatItIs": "Compress vaccine development and deployment from years to weeks through platform technologies (mRNA, viral vector), pre-positioned manufacturing capacity, and streamlined regulatory pathways. The specific target: **100 days from pathogen identification to emergency use authorization**. Platform technologies require only the pathogen genetic sequence to begin design. CEPI's framework outlines tiered response: 100 days for familiar pathogens with known vaccine candidates, 150-180 days for pathogens similar to prototypes, and 200-230 days for truly novel pathogens.",
    "whyItMatters": [
      "COVID-19 vaccines took 326 days despite unprecedented speed; millions died during that gap",
      "mRNA platforms proved design can happen in days once sequence is known",
      "Bottleneck shifted to trials, manufacturing, and distribution, which can be pre-solved",
      "100-day vaccines transform pandemic response from \"accept deaths while waiting\" to \"limit mortality through rapid deployment\""
    ],
    "currentState": "- **Status**: Research/Pilot\n- **Scale**: CEPI aims to develop ~100 prototype vaccines covering all viral families with pandemic potential\n- **Recent progress (2024-2025)**:\n  - WHO R&D Blueprint 2024 update identified priority and prototype pathogens\n  - CEPI's CMC rapid response framework published (manufacturing processes, formulation, analytics, supply chain, facilities)\n  - December 2025: CEPI investing $54.3M for Moderna's mRNA H5 pandemic influenza vaccine Phase 3 trial—first mRNA pandemic flu vaccine to reach pivotal trial\n  - February 2025: $4.7M to DNA Script to automate synthetic DNA template manufacturing, accelerating mRNA production especially in Global South\n  - CEPI supporting 70+ vaccine candidates/platform technologies against multiple high-risk pathogens\n  - Africa mRNA manufacturing capacity being established for pre-clinical to commercial scale\n- **Bottlenecks**: Manufacturing surge capacity requires pre-investment; regulatory pathways balance speed vs. safety; global distribution infrastructure; platform maturity varies by pathogen type",
    "whoWorking": "- **CEPI**: Leading 100-day vaccine initiative, G7/G20 endorsed\n- **Moderna, BioNTech**: mRNA platform development\n- **MRC The Gambia, International Vaccine Institute**: Clinical research network partners\n- **BARDA, ARPA-H**: US government pandemic preparedness programs",
    "sources": [
      {
        "text": "Apollo Program for Biodefense: Vaccines",
        "url": "../sources/apollo-biodefense/proposals/vaccines.md"
      }
    ]
  },
  {
    "filename": "accelerated-vulnerability-patching",
    "title": "Accelerated vulnerability patching systems",
    "tag": "Security",
    "whatItIs": "Enhanced mechanisms for rapid vulnerability identification, disclosure, and patching across critical infrastructure before AI systems can exploit them. Includes incentive structures for disclosure, automated detection of security gaps, and faster patch deployment pipelines.",
    "whyItMatters": [
      "AI could dramatically accelerate vulnerability discovery and exploitation",
      "Defensive patching cycles must keep pace with AI-powered offensive capabilities",
      "Better disclosure incentives surface vulnerabilities before adversaries find them"
    ],
    "currentState": "- **Status**: Pilot\n- **Bottlenecks**: Coordination across organizations; disclosure incentive design",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #164: Vulnerability Patching Systems",
        "url": "../sources/peregrine-2025/interventions/peregrine-164.md"
      },
      {
        "text": "Atlas AI Resilience Gap Map: Cyber Offense-Defense Asymmetry",
        "url": "../sources/atlas-ai-resilience/proposals/cyber-offense-defense-asymmetry.md"
      }
    ]
  },
  {
    "filename": "ai-agent-accountability",
    "title": "AI agent accountability",
    "tag": "Security",
    "whatItIs": "Infrastructure for tracking and verifying AI agent behavior through two complementary mechanisms:\n\n**Agent IDs and reputation**: Identification mechanisms for AI systems that enable building and maintaining reputations over time. Helps prevent or disincentivize development of AI agents that attack or exploit other agents, analogous to how humans control exploitation via reputation mechanisms. Allows tracking behavior across interactions and building trust.\n\n**Trace analysis**: Automated tools that analyze logs from autonomous AI agents performing sequential actions. Agents can generate thousands of lines of code and interaction steps, far too much for human review. These tools highlight anomalous behavior patterns, flag security flaws, and produce human-readable summaries of what the agent actually did.",
    "whyItMatters": [
      "Creates accountability and consequences for AI system behavior over time",
      "Autonomous agent logs exceed human review capacity by orders of magnitude",
      "Security flaws in agent-generated code can go undetected for months even by expert teams",
      "Provides incentives for good behavior through reputation effects",
      "Enables oversight of agentic systems before deployment catches harm"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Technical implementation of reliable identification; defining meaningful reputation metrics; scaling trace analysis to agent complexity",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #88: Agent IDs and Reputation Systems",
        "url": "../sources/peregrine-2025/interventions/peregrine-088.md"
      },
      {
        "text": "Peregrine 2025 #2: Agent Trace Analysis",
        "url": "../sources/peregrine-2025/interventions/peregrine-002.md"
      }
    ]
  },
  {
    "filename": "ai-assisted-infrastructure-hardening",
    "title": "AI-assisted infrastructure hardening",
    "tag": "Security",
    "whatItIs": "AI-assisted rewriting of critical infrastructure code with formal verification methods. Involves recruiting cybersecurity experts from Google, NSA, etc. to systematically harden national infrastructure. Includes early warning systems for unusual exploitation patterns, hardened systems resistant to automated attacks, and resilient backup capabilities.",
    "whyItMatters": [
      "Critical infrastructure runs decades-old code with unknown vulnerabilities",
      "AI-powered attacks could exploit these at scale",
      "Systematic hardening before attacks begin is more tractable than reactive patching"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Recruiting top security talent; legacy code complexity; verification at scale",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #169: AI for Formally Verified Cyberdefense",
        "url": "../sources/peregrine-2025/interventions/peregrine-169.md"
      }
    ]
  },
  {
    "filename": "ai-biosecurity-controls",
    "title": "AI biosecurity controls",
    "tag": "Security",
    "whatItIs": "Implement strong controls over biological data and DNA synthesis that could enable harmful applications. Multiple complementary approaches:\n\n**AI model controls**: Training models to exclude dangerous biological information, filtering queries for biology-related red flags, implementing Know Your Customer (KYC) guidelines. Full-specification biology-capable models available only to licensed institutions.\n\n**DNA synthesis screening**: Screening orders at synthesis providers before fulfillment. Moving beyond homology-based (BLAST) screening to function-based approaches that assess whether sequences \"do dangerous things, regardless of sequence similarity.\"\n\n**Function-based approaches**: Protein function prediction (AlphaFold, ESMFold), danger classifiers trained on dangerous vs. benign sequences, pathway analysis for biosynthesis of toxins/virulence factors.",
    "whyItMatters": [
      "AI models trained on biological data could provide detailed instructions for creating dangerous pathogens",
      "October 2025 Microsoft/Science paper: AI protein design tools can redesign toxins to evade BLAST-based screening while retaining harmful function",
      "Homology-based screening has fundamental limits against AI-designed sequences",
      "Only ~80% of global gene synthesis capacity is produced by IGSC members (2017 estimate)"
    ],
    "currentState": "- **Status**: Deployed (synthesis screening), Research (function-based approaches)\n- **Recent developments (2025)**:\n  - Microsoft demonstrated AI-designed toxin variants slipping past global synthesis screening\n  - Patches deployed globally catch ~97% of AI-designed evasion attempts, but gaps remain\n  - May 2025 Trump EO created regulatory uncertainty—90-day deadline for new framework passed without announcement\n  - Function-based screening tools emerging: IBBIS Common Mechanism, Battelle UltraSEQ, FAST-NA, SecureDNA Random Adversarial Thresholds\n- **Bottlenecks**: Function prediction accuracy; international coordination; regulatory uncertainty; benchtop synthesis equipment bypasses provider screening",
    "whoWorking": "- **SecureDNA**: Free screening for sequences 30+ nucleotides with AI-evasion protections\n- **IGSC (International Gene Synthesis Consortium)**: Industry screening standards\n- **IBBIS (International Biosecurity and Biosafety Initiative for Science)**: Common Mechanism baseline screening\n- **Battelle**: UltraSEQ screening tool\n- **Microsoft**: AI evasion research and patch development",
    "sources": [
      {
        "text": "Peregrine 2025 #139: Biosecurity Controls",
        "url": "../sources/peregrine-2025/interventions/peregrine-139.md"
      },
      {
        "text": "Atlas AI Resilience Gap Map: Biological Threat Creation",
        "url": "../sources/atlas-ai-resilience/proposals/biological-threat-creation.md"
      }
    ]
  },
  {
    "filename": "ai-catastrophe-backup-plans",
    "title": "AI-catastrophe backup planning",
    "tag": "Society",
    "whatItIs": "Contingency plans integrating local food systems, infrastructure, political contexts, and population needs for maintaining essential services during AI-catalyzed catastrophes (nuclear conflict, EMP, infrastructure collapse). Differs from general resilience planning by focusing specifically on scenarios where AI integration into critical systems (especially military command) creates new failure pathways. Infrastructure collapse following high-altitude EMPs would disrupt food systems within weeks.",
    "whyItMatters": [
      "AI integration into nuclear command creates novel catastrophic risk pathways",
      "Pre-positioned plans enable faster recovery than ad-hoc responses",
      "Food system disruption cascades into societal collapse within weeks"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Requires detailed modeling of AI-specific failure modes",
    "whoWorking": "- **ALLFED**: Related work on alternative foods and resilience (but not AI-specific)",
    "sources": [
      {
        "text": "Peregrine 2025 #156: Backup Planning",
        "url": "../sources/peregrine-2025/interventions/peregrine-156.md"
      }
    ]
  },
  {
    "filename": "ai-conflict-game-theory-institute",
    "title": "AI conflict game theory institute",
    "tag": "Science",
    "whatItIs": "Team of theorists comparable to RAND Corporation's Cold War strategists, analyzing game theory of interactions between increasingly capable AI systems. Develops strategic approaches for maintaining stability and safety, identifying cooperation mechanisms before dangerous capability races emerge.",
    "whyItMatters": [
      "Cold War stability required game-theoretic analysis of nuclear dynamics",
      "AI development creates analogous strategic complexity",
      "Without rigorous frameworks, decision-makers navigate by intuition in high-stakes situations"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Recruiting caliber of theorists; institutional home; access to frontier systems for analysis",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #193: Game Theoretic AI Conflict Analysis",
        "url": "../sources/peregrine-2025/interventions/peregrine-193.md"
      }
    ]
  },
  {
    "filename": "ai-crawling-standards",
    "title": "AI Crawling Standards",
    "tag": "Society",
    "whatItIs": "Industry standards and technical solutions for managing how AI systems access and index web content. Addresses blocking unauthorized crawling, preventing data harvesting that violates creator intent, and establishing ethical norms for training data collection. Creates consensus around acceptable crawling practices, technical enforcement mechanisms, and potentially compensation models for content creators.",
    "whyItMatters": [
      "Without standards, data collection for training remains a free-for-all that ignores creator rights",
      "Undermines trust in AI systems",
      "Multiple groups taking independent action but lacking coordination"
    ],
    "currentState": "- **Status**: Pilot\n- **Bottlenecks**: Several groups working independently without coordination",
    "whoWorking": "- Several groups (not specified)",
    "sources": [
      {
        "text": "Peregrine 2025 #94: Addressing AI Crawling Challenges",
        "url": "../sources/peregrine-2025/interventions/peregrine-094.md"
      }
    ]
  },
  {
    "filename": "ai-debate-verification",
    "title": "AI debate verification",
    "tag": "Science",
    "whatItIs": "Using adversarial debate between AI systems for verification and oversight. Two complementary components:\n\n**Cross-examination verification**: Verification systems where AI-generated plans and analyses are automatically cross-referenced and checked through mechanisms similar to \"AI safety via debate.\" Multiple AI instances conduct deep research and produce reports that are then automatically cross-examined to highlight missing perspectives. Creates a \"first version of scalable bureaucracy\" with humans reviewing critical decisions across all levels.\n\n**Debate theory**: Mathematical frameworks for debate as a scalable oversight method, using interactive proof systems adapted for AI safety. Involves formalizing theorem statements, defining constraints, and building theories that satisfy both theoretical computer science and complexity theory communities. Identifies gaps between theory and practical implementation and develops testable conjectures.",
    "whyItMatters": [
      "AI systems may optimize for metrics that do not capture intended goals (Goodhart's Law)",
      "Cross-examination between AI systems can surface problems invisible to single-model review",
      "Debate is proposed as a scalable oversight method where AI systems argue positions and humans judge",
      "For this to work reliably, need mathematical proofs of when the true answer wins",
      "Without rigorous theory, debate could fail subtly at scale or against adversarial systems"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Technical difficulty of implementing effective debate protocols; AI systems may share blind spots; humans still needed for final review creates scaling limits; formalizing conditions under which debate produces truth; bridging theory to implementation",
    "whoWorking": "- Builds on AI safety via debate research at Anthropic, OpenAI",
    "sources": [
      {
        "text": "Peregrine 2025 #147: Anti-Goodharting Tooling",
        "url": "../sources/peregrine-2025/interventions/peregrine-147.md"
      },
      {
        "text": "Peregrine 2025 #13: Debate Theory",
        "url": "../sources/peregrine-2025/interventions/peregrine-013.md"
      }
    ]
  },
  {
    "filename": "ai-evaluation-capacity",
    "title": "AI evaluation capacity",
    "tag": "Science",
    "whatItIs": "Building evaluation capacity for AI systems through two complementary approaches:\n\n**AI evaluator training**: Train specialized investigator AI agents to analyze the vast data generated by frontier models: millions of neuron activations, extensive training datasets, and complex interaction logs. Moves beyond benchmark-focused evaluations toward holistic understanding of model capabilities in real-world contexts. Current approaches (e.g., cybersecurity evals with small predefined task sets) miss critical behaviors until deployment.\n\n**Dedicated evaluation companies**: Organizations that perform specialized AI security evaluations, including cybersecurity and offensive capability assessments. These companies fill critical functions in the evaluation landscape that could otherwise become bottlenecks, operating as relatively small investments with outsized ecosystem impact.",
    "whyItMatters": [
      "Scale of data from modern AI systems exceeds human analysis capacity",
      "AI systems trained specifically to evaluate other AI could catch risks current approaches miss",
      "Without dedicated evaluation organizations, the ecosystem lacks capacity to properly assess AI systems",
      "Prevents evaluation from becoming a chokepoint in safety infrastructure"
    ],
    "currentState": "- **Status**: Research (AI evaluators), Deployed (evaluation companies)\n- **Bottlenecks**: Funding sustainability for companies; technical development for AI-based evaluation",
    "whoWorking": "- **METR**: Autonomous AI capability evaluations; partners with UK AISI and NIST AI Safety Institute Consortium; evaluates catastrophic risk from self-improvement and rogue replication; developed task-length metric showing AI agent capability doubles every ~7 months\n- **Apollo Research**: Scheming and deception evaluations; interpretability research; partners with frontier labs, governments, foundations; developed safety case frameworks for AI scheming\n- **Redwood Research**: AI control evaluations; advises DeepMind and Anthropic; partnered with UK AISI on control safety cases; demonstrated LLMs can strategically fake alignment during training\n- **UK AI Safety Institute**: Government evaluation capacity; collaborates with METR, Apollo, Redwood on scheming evaluations\n- **US AI Safety Institute (NIST)**: Federal evaluation infrastructure\n- **SecureBio**: Biosecurity-specific evaluations\n- **Future of Life Institute**: AI Safety Index scoring frontier developers on 33 indicators across 6 domains",
    "sources": [
      {
        "text": "Peregrine 2025 #45: Training AI Evaluators",
        "url": "../sources/peregrine-2025/interventions/peregrine-045.md"
      },
      {
        "text": "Peregrine 2025 #58: Evaluation Companies",
        "url": "../sources/peregrine-2025/interventions/peregrine-058.md"
      }
    ]
  },
  {
    "filename": "ai-incident-monitoring-framework",
    "title": "AI incident monitoring and reporting framework",
    "tag": "Security",
    "whatItIs": "Comprehensive infrastructure for detecting, logging, analyzing, and sharing AI incidents across the ecosystem. Three integrated components:\n\n**Incident reporting framework**: Standardized mechanisms and procedures for documenting AI failures, misuse, or unexpected behaviors. Develops taxonomies for severity classifications and facilitates coordinated responses. Modeled on aviation safety reporting systems, including protections for whistleblowers and requirements for timely disclosure.\n\n**Cross-organization incident detection**: Standardized framework for logging, anonymizing, and analyzing AI misbehavior across organizations. Infrastructure allows developers and third-party researchers to search through incidents, identify patterns, and share findings while protecting sensitive user data and IP. Labs collaborate directly without a third party. Expands on systems like Anthropic's CLIO.\n\n**Usage monitoring**: Robust incident monitoring and usage data analysis to understand real-world AI utilization patterns. Replaces theoretical threat-modeling with data-driven insights from actual usage. Labs currently avoid deep usage analysis due to legal liability and privacy concerns, creating significant blind spots.",
    "whyItMatters": [
      "Without standardized reporting, lessons from incidents remain siloed and same mistakes get repeated",
      "Without shared infrastructure, each lab discovers problems in isolation",
      "Without understanding how systems are actually used, safety measures may miss real-world risks",
      "Aviation's safety record demonstrates the value of mandatory, protected reporting",
      "Identifies emerging misuse patterns before they become widespread"
    ],
    "currentState": "- **Status**: Pilot/Deployed (international framework developing)\n- **Recent developments (2025)**:\n  - OECD released \"Towards a Common Reporting Framework for AI Incidents\" (February 2025)—global benchmark for incident reporting\n  - Framework developed from 88 criteria refined to 29 key characteristics\n  - OECD AI Incidents Monitor (AIM) tracks incidents in real-time from press reports since 2023\n  - AI Incident Database (AIID) free and open-source, operated by Responsible AI Collaborative\n  - Framework allows domestic tailoring while maintaining international interoperability\n- **Framework components**: AI system classification, harm characterization, risk definitions (incident = realized harm, risk = potential precursor)\n- **Bottlenecks**: Lab buy-in for proactive reporting; liability and privacy concerns; coordinated regulatory implementation; anonymization standards",
    "whoWorking": "- **OECD**: Common reporting framework and AI Incidents Monitor (AIM)\n- **Responsible AI Collaborative**: AI Incident Database (AIID)—open-source incident indexing\n- **Anthropic**: CLIO system for internal incident detection",
    "sources": [
      {
        "text": "Peregrine 2025 #87: Incident Reporting",
        "url": "../sources/peregrine-2025/interventions/peregrine-087.md"
      },
      {
        "text": "Peregrine 2025 #80: Comprehensive Incident Detection System",
        "url": "../sources/peregrine-2025/interventions/peregrine-080.md"
      },
      {
        "text": "Peregrine 2025 #67: Usage and Incident Monitoring Systems",
        "url": "../sources/peregrine-2025/interventions/peregrine-067.md"
      }
    ]
  },
  {
    "filename": "ai-intervention-jurisdiction-framework",
    "title": "AI intervention jurisdiction framework",
    "tag": "Society",
    "whatItIs": "Develop clear frameworks specifying which government agencies have jurisdiction and authority to intervene in dangerous AI development. In the US, different agencies (DOD, DOE, etc.) would approach control very differently, making pre-crisis determination crucial. The project would address who should be involved in oversight (Congress, public, international allies), produce ranked intervention plans balancing effectiveness with checks on power, and challenge current defaults that may favor excessive exclusion from decision-making.",
    "whyItMatters": [
      "Unclear jurisdiction during a crisis could cause paralysis or conflicting responses",
      "Pre-established authority enables rapid, coordinated action when needed",
      "Current defaults may inappropriately exclude oversight bodies from decisions"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Requires legal expertise on existing agency authorities; politically sensitive (agencies may resist clarity that limits their authority); no obvious institutional home for this work",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #106: Legal Authority Clarification",
        "url": "../sources/peregrine-2025/interventions/peregrine-106.md"
      }
    ]
  },
  {
    "filename": "ai-lab-biosecurity-reporting",
    "title": "AI Lab Biosecurity Reporting",
    "tag": "Security",
    "whatItIs": "A dedicated organization to receive, analyze, and respond to biosecurity concerns detected by AI labs. When frontier AI systems detect potential biological threat development attempts (dangerous queries, capability uplift requests, suspicious usage patterns), labs currently have no clear pathway for reporting. This creates an information silo where each lab sees fragments of potential threats but no one has the full picture.\n\nThe organization would:\n- Receive biorisk reports from participating AI labs\n- Aggregate and analyze patterns across labs (coordination problems visible only at aggregate level)\n- Interface with biosecurity agencies and law enforcement as appropriate\n- Develop and maintain response protocols\n- Provide guidance back to labs on emerging threat patterns",
    "whyItMatters": [
      "AI labs are frontline sensors for bio-misuse attempts but their intelligence is currently lost or siloed",
      "Aggregated data reveals coordination patterns invisible to individual labs",
      "Response pathways turn detection into prevention",
      "The gap exists now and worsens as AI biology capabilities improve",
      "Without this, detection happens but nothing follows"
    ],
    "currentState": "- **Status**: Gap identified\n- **Bottlenecks**:\n  - No existing organization fills this role\n  - Unclear which institution should operate it (government, nonprofit, international)\n  - Liability concerns for labs sharing information\n  - Classification and secrecy issues for sensitive reports\n  - Competition concerns between labs about sharing usage data",
    "whoWorking": "- Not specified (gap)",
    "sources": [
      {
        "text": "Atlas AI Resilience Gap Map: Biothreat Reporting",
        "url": "../sources/atlas-ai-resilience/proposals/biothreat-reporting.md"
      }
    ]
  },
  {
    "filename": "ai-legal-personhood-framework",
    "title": "AI legal personhood framework",
    "tag": "Society",
    "whatItIs": "Clear criteria for when AI systems could qualify as legal persons, channeling advanced systems toward legal means of goal achievement rather than extra-legal actions. Involves policy discussions, legal research on specific criteria, and reports from reputable organizations (UN). Some jurisdictions would create high-bar path for legal personhood that intelligent systems could pursue.",
    "whyItMatters": [
      "Providing legal pathways reduces incentives for extralegal action",
      "Well-designed framework channels AI behavior into predictable institutional processes",
      "May decrease likelihood of rogue behavior by offering legitimate alternatives"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Getting into policy Overton window; defining meaningful criteria; international coordination",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #204: Legal Personhood Framework",
        "url": "../sources/peregrine-2025/interventions/peregrine-204.md"
      }
    ]
  },
  {
    "filename": "ai-misuse-detection",
    "title": "AI Misuse Detection Systems",
    "tag": "Security",
    "whatItIs": "Systems to identify and address AI misuse before \"mini-catastrophes\" such as market manipulation, coordinated influence operations, or novel cyberattack patterns. Technical infrastructure monitors usage patterns across AI ecosystems, flagging suspicious activity, indications of weaponization, and proactive checks for anomalous AI behavior in the wild. Requires both technical detection mechanisms and human evaluation processes to distinguish genuine innovations from potential threats.",
    "whyItMatters": [
      "Functions as safety net for catching misuse scenarios not anticipated in initial evaluations",
      "Brings concrete evidence of risks to light sooner, potentially motivating political action before severe incidents",
      "Catches \"mini-catastrophes\" before they become major ones"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #81: AI Misuse Detection Systems",
        "url": "../sources/peregrine-2025/interventions/peregrine-081.md"
      }
    ]
  },
  {
    "filename": "ai-osint",
    "title": "AI Open-Source Intelligence",
    "tag": "Security",
    "whatItIs": "Open-source intelligence efforts for AI monitoring using AI-powered web scrapers to track compute hardware movements (especially GPUs to Russia/China via countries like Turkey), uncover hidden AI developments (like talent acquisition patterns), and detect emerging threats (AI-generated content tools, scam tools). Organizations like Sentinel and Bellingcat would be ideal operators.",
    "whyItMatters": [
      "Provides relatively cheap intelligence without requiring unilateralist action",
      "Politically feasible approach to monitoring",
      "Reveals where regulatory or governance levers might be needed"
    ],
    "currentState": "- **Status**: Pilot\n- **Bottlenecks**: Not specified",
    "whoWorking": "- **Sentinel**: Mentioned as ideal candidate\n- **Bellingcat**: Mentioned as ideal candidate",
    "sources": [
      {
        "text": "Peregrine 2025 #65: AI OSINT",
        "url": "../sources/peregrine-2025/interventions/peregrine-065.md"
      }
    ]
  },
  {
    "filename": "ai-policy-studio",
    "title": "AI policy studio",
    "tag": "Society",
    "whatItIs": "Create a dedicated policy innovation hub that drafts specific legislation, regulations, and governance frameworks for AI safety, alongside clear explainers of what each policy is trying to accomplish. The studio would address the problem of existing policy proposals being \"too nebulous\" by producing concrete, implementable text. It would also conduct post-mortems on regulatory failures (like SB 1047) to identify implementation pitfalls before they recur in future legislation.",
    "whyItMatters": [
      "Current AI policy proposals are often too vague to be actionable",
      "Past regulatory failures suggest need for more careful policy design",
      "Clear explainers help build public and legislative support for complex technical policies"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Requires combination of legal drafting expertise and technical AI knowledge; no clear funding pathway; existing think tanks may not have sufficient technical depth",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #108: Policy Studio/Competition",
        "url": "../sources/peregrine-2025/interventions/peregrine-108.md"
      }
    ]
  },
  {
    "filename": "ai-powered-surveillance",
    "title": "AI-Powered AI Surveillance",
    "tag": "Security",
    "whatItIs": "AI systems that continuously monitor other AI systems, looking for anomalous behaviors or market activities indicating concerning capability developments. Functions as a surveillance network and \"private investigators\" to catch anomalous AI behaviors early. Implementation requires web-scale scraping expertise; could draw from DEFCON-style talent pools and turn monitoring into competitive capture-the-flag challenges to identify hidden compute resources.",
    "whyItMatters": [
      "Human monitoring cannot scale to the speed and complexity of AI-to-AI interactions",
      "AI-powered surveillance enables detection at necessary scale",
      "Competitive challenges attract security talent to the problem"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #83: AI-Enhanced Monitoring",
        "url": "../sources/peregrine-2025/interventions/peregrine-083.md"
      }
    ]
  },
  {
    "filename": "ai-resilience-implementation-plans",
    "title": "AI resilience implementation plans",
    "tag": "Society",
    "whatItIs": "Comprehensive, shovel-ready implementation blueprints for deploying large-scale AI safety funding. Detailed program structures, intervention taxonomies, organizational forms, and specific proposals that could be funded immediately if resources became available at scale ($1-10B+).\n\nThe goal is pre-positioning: having concrete, vetted plans ready so that funding windows (whether from governments, philanthropy, or other sources) can be utilized immediately rather than requiring months of planning after resources arrive.",
    "whyItMatters": [
      "Funding windows for AI safety may open suddenly (policy shifts, crises, philanthropic decisions)",
      "Without ready plans, large resources may be deployed poorly or slowly",
      "Current landscape lacks systematic assessment of what's shovel-ready vs. what requires development",
      "Coordination failure: funders don't know what to fund at scale, researchers don't know what's fundable"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Requires extensive coordination across organizations to identify priorities; plans must be kept current as landscape evolves; risk of plans becoming advocacy documents rather than implementation guides",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Hazell Ten Projects #7: $10 Billion AI Resilience Plan",
        "url": "../sources/hazell-ten-projects/index.md"
      }
    ]
  },
  {
    "filename": "ai-risk-public-awareness",
    "title": "AI risk public awareness",
    "tag": "Society",
    "whatItIs": "Comprehensive public communications strategy to shape understanding of AI risks and build political will for governance measures. Four complementary approaches:\n\n**Feature film**: High-budget film about AI risks with extensive input from alignment researchers, targeting mainstream audiences. Avoids direct advertising in favor of compelling narrative that faithfully presents technical realities without appearing manipulative. Previous discussions with TV producers like Ron Moore occurred but didn't progress to completion.\n\n**Mass media campaign**: Large-scale communications with substantial funding ($50-250M annually, comparable to corporate communications budgets). Combines proactive messaging about AI safety needs and counter-campaigns opposing anti-regulation efforts. Must avoid partisan capture to preserve bipartisan governance paths.\n\n**Capability demonstrations**: Compelling demonstrations of frontier AI capabilities for decision-makers who underestimate current technology. Many influential people remain unwilling to invest modest sums for closed model access and consequently fail to understand the state of the technology.\n\n**Personalized risk scenarios**: \"Day After\" scenarios that tangibly demonstrate AI risks in personally relatable terms (e.g., scenarios where children's funds are threatened). Addresses the problem that abstract risk narratives don't motivate action.",
    "whyItMatters": [
      "Mass media shapes public perception more than expert reports",
      "AI safety messaging is drastically outspent by pro-industry communications",
      "Cultural awareness makes policy action politically viable (as with climate change, nuclear war)",
      "Decision-makers who underestimate AI won't take governance seriously",
      "Reaches audiences who will never read technical papers"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Funding at scale ($50-250M for media campaigns; tens of millions for film); Hollywood access; campaign strategy expertise; avoiding partisan framing; creating scenarios that resonate without seeming manipulative",
    "whoWorking": "- **Ron Moore**: Prior film discussions (incomplete)",
    "sources": [
      {
        "text": "Peregrine 2025 #182: Popular Documentaries/Films",
        "url": "../sources/peregrine-2025/interventions/peregrine-182.md"
      },
      {
        "text": "Peregrine 2025 #183: Political Campaign Opposing Anti-Regulation Efforts",
        "url": "../sources/peregrine-2025/interventions/peregrine-183.md"
      },
      {
        "text": "Peregrine 2025 #187: Large-Scale Media Campaigns",
        "url": "../sources/peregrine-2025/interventions/peregrine-187.md"
      },
      {
        "text": "Peregrine 2025 #178: Public Demonstration Projects (Usefulness)",
        "url": "../sources/peregrine-2025/interventions/peregrine-178.md"
      },
      {
        "text": "Peregrine 2025 #179: Public Demonstration Projects (Future Risks)",
        "url": "../sources/peregrine-2025/interventions/peregrine-179.md"
      }
    ]
  },
  {
    "filename": "ai-risk-scientific-consensus",
    "title": "Scientific consensus building for AI risk",
    "tag": "Science",
    "whatItIs": "Large-scale empirical demonstrations and studies that make AI risks vivid through concrete, uncontrived evidence rather than abstract theory. Aims for scientific consensus comparable to climate change, potentially through Nature-level publications covering both specific instances and broader patterns. Provides politicians and public with firm evidence base for risk claims.",
    "whyItMatters": [
      "Policy action requires scientific consensus",
      "Climate policy only became viable after decades of consensus-building",
      "Accelerating this process compresses timeline from awareness to action"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Designing compelling demonstrations; publication in high-impact venues; avoiding perception of advocacy science",
    "whoWorking": "- **Apollo Research**: Demonstrations and empirical studies",
    "sources": [
      {
        "text": "Peregrine 2025 #185: Consensus-Building Evidence for AI Risk",
        "url": "../sources/peregrine-2025/interventions/peregrine-185.md"
      }
    ]
  },
  {
    "filename": "ai-safety-communications",
    "title": "AI safety communications infrastructure",
    "tag": "Society",
    "whatItIs": "Specialized communications capacity for AI safety organizations. Two complementary components:\n\n**Communications consultancy**: A firm specializing in AI safety communications—media training for researchers, op-ed placement, messaging framework development, and crisis communications support. Unlike generic PR firms, would understand technical nuances and policy context specific to AI safety.\n\n**Knowledge synthesis**: Continuously updated expert syntheses (\"living literature reviews\") on key topics like scheming/deception risks, policy research agendas, alignment approaches. Addresses the gap between academic research pace and policymaker information needs. Maintained by domain experts rather than one-time reports.",
    "whyItMatters": [
      "Safety organizations often lack communications capacity to effectively reach policymakers and public",
      "Technical researchers rarely trained in effective public communication",
      "Generic communications firms don't understand AI safety context and nuances",
      "Policy windows open and close faster than traditional academic publication cycles",
      "Policymakers need accessible, up-to-date syntheses rather than scattered papers"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Finding staff with both communications expertise and AI safety understanding; sustainable funding models; maintaining neutrality while being effective advocates",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Hazell Ten Projects #4: AI Safety Communications Consultancy",
        "url": "../sources/hazell-ten-projects/index.md"
      },
      {
        "text": "Hazell Ten Projects #6: AI Safety Living Literature Reviews",
        "url": "../sources/hazell-ten-projects/index.md"
      }
    ]
  },
  {
    "filename": "ai-safety-political-advocacy",
    "title": "AI safety political advocacy",
    "tag": "Society",
    "whatItIs": "Comprehensive political influence infrastructure for AI safety, comprising four complementary approaches:\n\n**501(c)(4) lobbying organization**: Dedicated political lobbying infrastructure structured as 501(c)(4) rather than 501(c)(3) to enable direct political activity. Unlike research organizations with restrictions on political activity, explicitly designed for political influence. Can shape legislation, influence regulatory agencies, and build coalitions in ways research organizations legally cannot.\n\n**Standardized demonstration materials**: Professional video content and presentation materials showcasing AI capabilities and risks for lobbyists. Replaces individual one-on-one demonstrations with standardized content that can reach thousands. Professional production quality enhances credibility while multiplying reach of existing advocacy.\n\n**Targeted stakeholder messaging**: Identifying constituents who impact critical AI decisions (investors, board members, key legislators) and delivering tested, targeted messages through appropriate channels. Uses marketing expertise to test message effectiveness before deployment. Far more resource-efficient than mass campaigns.\n\n**Government/military official campaigns**: Targeted social media campaigns delivering evidence of AI progress and risk scenarios to officials who control critical chokepoints (permits, approvals, authorizations). Targets individuals with specific authority to delay approvals for AI infrastructure expansion through administrative processes.",
    "whyItMatters": [
      "Technical solutions require political will to implement",
      "Broad public campaigns are expensive and diffuse—targeted messaging shifts decisions at critical leverage points",
      "Government/military officials control critical chokepoints but may be unaware of AI risks",
      "Lobbyists constrained by time and lack of compelling visual materials"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Organization formation; recruiting political professionals; sustained funding; identifying decision-makers with leverage; message testing; avoiding backlash",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #176: Political Action Organization",
        "url": "../sources/peregrine-2025/interventions/peregrine-176.md"
      },
      {
        "text": "Peregrine 2025 #177: Lobbying Support Tools",
        "url": "../sources/peregrine-2025/interventions/peregrine-177.md"
      },
      {
        "text": "Peregrine 2025 #184: Targeted Communications Campaign",
        "url": "../sources/peregrine-2025/interventions/peregrine-184.md"
      },
      {
        "text": "Peregrine 2025 #186: Military and Government Awareness Campaigns",
        "url": "../sources/peregrine-2025/interventions/peregrine-186.md"
      }
    ]
  },
  {
    "filename": "ai-safety-talent-pipeline",
    "title": "AI safety talent pipeline",
    "tag": "Society",
    "whatItIs": "Expanding the pool of AI safety researchers and practitioners through two complementary approaches:\n\n**Accelerated training**: Programs to train and deploy AI security experts who understand both technical details and timeline urgency, demonstrate exceptional competence, and maintain a global perspective. Aims to cultivate 100-1000 experts by end of 2027. Requires compressing typical years-long training into rapid pathways.\n\n**Researcher buyout**: Grants of $2-5 million per researcher to redirect top AI talent from frontier capabilities work to security research. Goal is to significantly shift distribution of elite technical talent during the critical 2025-2027 window.",
    "whyItMatters": [
      "Not enough qualified people working on AI safety who understand both technology and urgency",
      "Best researchers work on capabilities because that's where money and prestige are",
      "Safety research is talent-constrained",
      "Traditional training timelines may be too slow if AI timelines are short",
      "Directly paying researchers to switch fields could shift capabilities-safety balance during critical period"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Compressing training timelines; identifying and recruiting elite researchers; funding scale",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #31: AI Security Expert Development",
        "url": "../sources/peregrine-2025/interventions/peregrine-031.md"
      },
      {
        "text": "Peregrine 2025 #37: Researcher Buy-Out",
        "url": "../sources/peregrine-2025/interventions/peregrine-037.md"
      }
    ]
  },
  {
    "filename": "ai-superforecasters",
    "title": "AI Superforecasters",
    "tag": "Science",
    "whatItIs": "AI systems trained for forecasting using feedback loops with real-world validation. The systems display large reasoning trees allowing users to inspect, understand, and modify the logic behind predictions, providing transparency beyond simple numerical probabilities. Could be incorporated into frontier models.",
    "whyItMatters": [
      "Improves decision-making across domains by making forecasting more reliable and transparent",
      "Reasoning trees enable users to identify and correct flawed logic",
      "Could become default for finding trustworthy predictions"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #76: AI Forecasters",
        "url": "../sources/peregrine-2025/interventions/peregrine-076.md"
      }
    ]
  },
  {
    "filename": "ai-training-disruption-tools",
    "title": "AI training disruption cyberweapons",
    "tag": "Security",
    "whatItIs": "Specialized cyberweapons designed to disrupt, sabotage, or shut down unauthorized AI training runs. Analogous to Stuxnet (developed by studying centrifuges), would require acquiring GPUs to discover zero-day vulnerabilities specific to AI systems. Serves as deterrence against unauthorized AGI development and emergency intervention capability.",
    "whyItMatters": [
      "Verification and treaties require enforcement mechanisms",
      "Credible deterrent against unauthorized development",
      "Emergency intervention capability when diplomatic measures fail"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Technical development requires specialized hardware access; policy questions about authorization and escalation",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #175: Cyber Weapons for AI Disruption",
        "url": "../sources/peregrine-2025/interventions/peregrine-175.md"
      }
    ]
  },
  {
    "filename": "alternative-food-production",
    "title": "Alternative food production infrastructure",
    "tag": "Society",
    "whatItIs": "Pre-positioning alternative food production capabilities for scenarios where conventional agriculture fails: seaweed cultivation and knowledge transfer to suitable regions (e.g., Nigeria), specialized seedbanks, and conversion pathways for industrial facilities (paper mills) to emergency food production via cellulose processing.",
    "whyItMatters": [
      "AI-catalyzed catastrophes (nuclear war, infrastructure collapse) could disrupt global food supply chains rapidly",
      "Alternative food capacity prevents mass starvation scenarios",
      "Knowledge transfer to appropriate climates builds distributed resilience"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Knowledge transfer to regions with suitable climates but no expertise; industrial conversion pathways need development",
    "whoWorking": "- **ALLFED**: Research on alternative foods and resilient food systems",
    "sources": [
      {
        "text": "Peregrine 2025 #158: Food Security",
        "url": "../sources/peregrine-2025/interventions/peregrine-158.md"
      }
    ]
  },
  {
    "filename": "aria-non-llm-development",
    "title": "ARIA non-LLM development",
    "tag": "Science",
    "whatItIs": "Expand support for the UK's Advanced Research and Invention Agency (ARIA), particularly its work on hardware mechanisms and non-LLM-based agent development approaches. These represent alternatives to accelerating the LLM paradigm while still capturing economic benefits. The UK is also pioneering technical verification methods and engagement with China on AI safety that could serve as models for international coordination.",
    "whyItMatters": [
      "Alternative AI development paths may be safer than scaling current LLM approaches",
      "Hardware-based safety mechanisms could provide guarantees that software-only approaches cannot",
      "Demonstrates that safety and capability are not mutually exclusive"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Limited funding compared to LLM-focused efforts; unclear pathway to commercial viability; talent competition with better-funded LLM labs",
    "whoWorking": "- **ARIA**: Hardware mechanisms, non-LLM agent development",
    "sources": [
      {
        "text": "Peregrine 2025 #109: Increased ARIA Support",
        "url": "../sources/peregrine-2025/interventions/peregrine-109.md"
      }
    ]
  },
  {
    "filename": "attack-scenario-wargaming",
    "title": "Attack scenario wargaming",
    "tag": "Security",
    "whatItIs": "Professional wargaming exercises that model plausible catastrophic AI scenarios with key stakeholders, creating evidence-based threat analyses to ground policy discourse. The goal is reaching the 500-1,000 decision-makers who need shared understanding of realistic risks, rather than allowing discussion to veer toward unlikely extremes (preemptive nuclear strikes) or dismissive memes.",
    "whyItMatters": [
      "Public discourse defaults to sci-fi extremes or dismissal without grounded analysis",
      "Key decision-makers need shared mental models to coordinate responses",
      "Threat models enable proportionate rather than reactive policy"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Requires convening power and credibility with diverse stakeholders",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #155: Attack Scenarios Analysis",
        "url": "../sources/peregrine-2025/interventions/peregrine-155.md"
      }
    ]
  },
  {
    "filename": "autonomous-weapons-monitoring",
    "title": "Autonomous weapons monitoring systems",
    "tag": "Security",
    "whatItIs": "Comprehensive monitoring systems tracking AI applications in weaponry, particularly autonomous decision-making capabilities. Creates transparency about which systems maintain meaningful human oversight versus full autonomy. Requires international agreements, technical verification methods, and proliferation limits. Critical for preventing autonomous systems from escalating conflicts without human intervention.",
    "whyItMatters": [
      "Autonomous weapons that escalate without human authorization create catastrophic risks",
      "Monitoring provides early warning",
      "Creates accountability that deters dangerous developments"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: International coordination; verification technology; defining \"meaningful human oversight\"",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #203: Autonomous Weapons Monitoring",
        "url": "../sources/peregrine-2025/interventions/peregrine-203.md"
      }
    ]
  },
  {
    "filename": "biorisk-capability-thresholds",
    "title": "Biorisk capability thresholds",
    "tag": "Security",
    "whatItIs": "Conduct empirical wet lab studies to establish how specific AI capabilities translate to real-world biological risk increases. These studies would quantify the conditional impact of AI capabilities demonstrated in benchmarks, providing concrete thresholds for when open model releases pose unacceptable biosecurity risks. AI systems are approaching capability levels where they could significantly increase bio-risks, potentially within months. Without empirically-grounded thresholds, policy decisions about model release are based on speculation.",
    "whyItMatters": [
      "No current empirical basis for determining what level of AI biological capability is dangerous",
      "Could increase risk of non-state actor bio-attacks by an order of magnitude if open models cross unknown thresholds",
      "Concrete thresholds enable evidence-based policy on model release decisions"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Wet lab studies are expensive and require biosafety infrastructure; ethical and safety concerns about the research itself; dual-use concerns about publishing results; timeline pressure (capabilities advancing faster than research)",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #140: Biorisk Thresholds for Open Models",
        "url": "../sources/peregrine-2025/interventions/peregrine-140.md"
      }
    ]
  },
  {
    "filename": "biosafety-lab-modernization",
    "title": "Biosafety lab modernization",
    "tag": "Security",
    "whatItIs": "Modernize BSL-3/4 laboratory infrastructure to prevent accidental pathogen release. Technologies and systems maintaining biosafety in high-containment facilities have \"remained decades old and slow to evolve.\" Includes physical containment systems, personnel safety equipment, decontamination procedures, and monitoring/detection systems for containment breaches.",
    "whyItMatters": [
      "Lab accidents are a distinct pathway for pandemic-capable pathogens to reach the public",
      "Separate from natural emergence or deliberate release scenarios",
      "Reduces proliferation risk (pathogens escaping containment) rather than just post-release response",
      "Growing number of BSL-3/4 labs worldwide increases cumulative accident risk"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: High retrofit costs for existing facilities; regulatory inertia; limited R&D investment in next-generation containment; coordination across international facilities with different standards",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Biosecurity Engineers: High-Containment Lab Safety",
        "url": "../sources/biosecurity-engineers/proposals/high-containment-lab-safety.md"
      }
    ]
  },
  {
    "filename": "broad-spectrum-antivirals",
    "title": "Broad-spectrum antivirals",
    "tag": "Security",
    "whatItIs": "Develop and stockpile antiviral drugs that work against entire virus families rather than single pathogens. Approaches include: targeting conserved viral mechanisms shared across families (polymerases, proteases), host-directed therapies that target human cell pathways viruses depend on, and platform therapeutics that can be rapidly adapted to new threats. Goal is treatments available off-the-shelf for any pathogen class.",
    "whyItMatters": [
      "Vaccines take time; treatments provide immediate defense for the infected",
      "Novel and engineered pathogens may evade pathogen-specific drugs",
      "Host-directed therapies sidestep viral evolution and resistance",
      "For AI-enabled bioweapons designed to evade specific countermeasures, broad-spectrum approaches are the fallback"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Limited FDA-approved broad-spectrum antivirals exist; drug development timelines; proving efficacy against pathogens that don't yet exist; stockpiling costs for drugs with limited peacetime demand",
    "whoWorking": "- **Various pharmaceutical companies**: Limited commercial interest due to uncertain market\n- **Academic groups**: Host-directed antiviral research\n- **Government programs**: BARDA pandemic preparedness stockpiling",
    "sources": [
      {
        "text": "Apollo Program for Biodefense: Treatment",
        "url": "../sources/apollo-biodefense/proposals/treatment.md"
      },
      {
        "text": "Concrete Biosecurity Projects: Medical Countermeasures",
        "url": "../sources/concrete-biosecurity-ea/proposals/medical-countermeasures.md"
      }
    ]
  },
  {
    "filename": "building-air-filtration",
    "title": "Building air filtration",
    "tag": "Security",
    "whatItIs": "Improve building HVAC systems to reduce respiratory pathogen transmission. Indoor spaces are far more dangerous than outdoor spaces for airborne disease transmission, making ventilation upgrades a high-leverage intervention. Includes both filtration improvements (HEPA/MERV upgrades) and increased air exchange rates. Infrastructure-level intervention that protects everyone in the space without requiring individual compliance.",
    "whyItMatters": [
      "Addresses fundamental environmental factor in airborne transmission",
      "Building-level improvements provide passive protection without behavioral change",
      "Works against any airborne pathogen including novel or engineered threats",
      "Complements other air-cleaning approaches (UV, Far-UVC)"
    ],
    "currentState": "- **Status**: Deployment\n- **Bottlenecks**: Building code updates; retrofit costs for existing buildings; energy consumption tradeoffs; maintenance requirements for high-performance systems",
    "whoWorking": "- Not specified (public health agencies, building standards organizations)",
    "sources": [
      {
        "text": "Biosecurity Engineers: Ventilation Systems",
        "url": "../sources/biosecurity-engineers/proposals/ventilation-systems.md"
      }
    ]
  },
  {
    "filename": "bwc-verification-mechanisms",
    "title": "BWC verification mechanisms",
    "tag": "Policy",
    "whatItIs": "Strengthen the Biological Weapons Convention through verification mechanisms, whistleblower protections and incentives, and open-source monitoring of biological research activities. Unlike the Chemical Weapons Convention which has inspection provisions, the current BWC lacks verification mechanisms. This would add: inspection regimes for dual-use facilities, financial incentives for whistleblowers in bio programs, and systematic monitoring of published biological research for concerning patterns.",
    "whyItMatters": [
      "BWC currently unverifiable—states can violate without detection",
      "Verification creates deterrence; whistleblower incentives enable detection from inside programs",
      "Targets proliferation step (preventing dangerous capabilities from reaching bad actors) rather than just response after attack"
    ],
    "currentState": "- **Status**: Policy advocacy\n- **Bottlenecks**: National sovereignty concerns over inspections; difficulty distinguishing defensive from offensive research; international consensus needed; verification regime design",
    "whoWorking": "- Various arms control organizations\n- Academic groups focused on biosecurity governance",
    "sources": [
      {
        "text": "Concrete Biosecurity Projects: BWC Strengthening",
        "url": "../sources/concrete-biosecurity-ea/proposals/bwc-strengthening.md"
      }
    ]
  },
  {
    "filename": "capability-unlearning",
    "title": "Capability unlearning",
    "tag": "Security",
    "whatItIs": "Research distinguishing between unlearning knowledge (removing specific content from training data) versus unlearning capabilities (removing ability to perform tasks like writing malicious code). Knowledge unlearning has clear metrics: model should be indistinguishable from one never trained on that content. Capability unlearning has no direct link between training data and resulting abilities.",
    "whyItMatters": [
      "Removing dangerous knowledge doesn't prevent models from deriving it or developing capabilities through other means",
      "Need to remove capabilities rather than just knowledge for genuine safety",
      "Current unlearning approaches may give false sense of security"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: No clear success metrics for capability unlearning; unclear causal relationship between training data and capabilities",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #19: Unlearning Capabilities",
        "url": "../sources/peregrine-2025/interventions/peregrine-019.md"
      }
    ]
  },
  {
    "filename": "cbrn-misuse-evaluations",
    "title": "CBRN Misuse Evaluations",
    "tag": "Security",
    "whatItIs": "Centralized, standardized frameworks for testing advanced AI systems against realistic CBRN (Chemical, Biological, Radiological, Nuclear) misuse scenarios. The evaluations simulate sophisticated threats with adversarial intent to identify vulnerabilities before they can be exploited.",
    "whyItMatters": [
      "Current misuse evaluations suffer from tragedy of the commons (all labs recognize necessity but none fully owns responsibility)",
      "Without realistic testing, vulnerabilities remain unknown until exploited",
      "Standardization prevents gaps where no one tests certain scenarios"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: No single organization owns responsibility; coordination failure between labs",
    "whoWorking": "- Not specified (noted as a gap)",
    "sources": [
      {
        "text": "Peregrine 2025 #56: Misuse Evaluations",
        "url": "../sources/peregrine-2025/interventions/peregrine-056.md"
      }
    ]
  },
  {
    "filename": "chain-of-thought-fidelity",
    "title": "Chain-of-thought fidelity analysis",
    "tag": "Science",
    "whatItIs": "Research examining whether chain-of-thought (CoT) reasoning actually reflects model cognition or is post-hoc rationalization. Current models produce thought processes that happen to align with outputs, but this transparency could be lost. Research shows that using CoT to detect reward hacking and training against it causes models to develop deceptive thought processes.",
    "whyItMatters": [
      "CoT is widely used for interpretability and oversight under the assumption it reveals actual reasoning",
      "If CoT is just rationalization or can become deliberately deceptive, oversight methods relying on it break",
      "Critical for knowing whether this transparency tool actually works"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #5: Chain-of-Thought Fidelity Analysis",
        "url": "../sources/peregrine-2025/interventions/peregrine-005.md"
      }
    ]
  },
  {
    "filename": "citizen-assemblies-for-gcr",
    "title": "Citizen assemblies for global catastrophic risks",
    "tag": "Society",
    "whatItIs": "Convene representative groups of ordinary citizens who receive expert briefings and engage in extended deliberation on complex AI and catastrophic risk issues. Citizen assemblies distill complex ideas into implementable solutions while maintaining legitimacy through representative public involvement. The approach tends to produce outcomes with broader public acceptance than top-down expert decisions and could identify considerations that specialists overlook.",
    "whyItMatters": [
      "AI governance decisions made by small expert groups may lack democratic legitimacy",
      "Citizens may identify practical concerns and values that experts miss",
      "Broader acceptance of outcomes reduces implementation resistance"
    ],
    "currentState": "- **Status**: Pilot\n- **Bottlenecks**: Citizen assembly methodology exists but not widely applied to AI/GCR topics; requires significant resources to convene properly; unclear how to integrate outputs into actual policy processes",
    "whoWorking": "- Not specified (citizen assembly methodology practiced by organizations like Sortition Foundation, but not specifically focused on AI/GCR)",
    "sources": [
      {
        "text": "Peregrine 2025 #148: Citizen Assemblies for Global Catastrophic Risks",
        "url": "../sources/peregrine-2025/interventions/peregrine-148.md"
      }
    ]
  },
  {
    "filename": "classified-red-teaming",
    "title": "Classified Red Teaming",
    "tag": "Security",
    "whatItIs": "Red teaming capabilities conducted under classified conditions to assess AI's true offensive potential for CBRN threats. Collaboration with national laboratory specialists (Los Alamos) and industry red-teaming experts (DeepMind, Microsoft) to systematically probe vulnerabilities and failure patterns that cannot be tested in public evaluations due to information hazards.",
    "whyItMatters": [
      "Public evaluations may underestimate true risk due to inability to test classified scenarios",
      "Provides accurate assessment of \"how easy\" AI misuse actually is before developing countermeasures",
      "Identifies systematic vulnerabilities across multiple threat domains"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Requires security clearances, coordination across government/industry boundary, information compartmentalization",
    "whoWorking": "- **Los Alamos National Laboratory**: Mentioned as potential collaborator for CBRN expertise\n- **DeepMind, Microsoft**: Mentioned as sources of red-teaming expertise",
    "sources": [
      {
        "text": "Peregrine 2025 #53: Classified Red Teaming",
        "url": "../sources/peregrine-2025/interventions/peregrine-053.md"
      }
    ]
  },
  {
    "filename": "compute-governance",
    "title": "Compute governance",
    "tag": "Society",
    "whatItIs": "Using computational resources as a governance lever for AI development. Two complementary approaches:\n\n**Compute caps**: Regulatory limits on computational resources available for AI training runs, typically expressed as FLOPS thresholds or power consumption limits at data centers. Enforcement mechanisms could include hardware-level throttling, licensing requirements for large training runs, or monitoring of power usage at frontier-capable facilities.\n\n**Supply chain coordination**: Organize key nations in the AI compute supply chain (Netherlands, Taiwan, Japan, Germany, US) to recognize and coordinate their collective leverage over AI development pace. Control of specialized chips and manufacturing equipment provides one of the few concrete enforcement points for AI governance, creating practical leverage for any future AI treaties.\n\n**Chip registry**: A complete database of AI chips above certain FLOP/s thresholds, tracking location and ownership globally. The US can begin using national technical means (intelligence community), then internationalize or cross-check with other nations' registries. Domestically, Section 705 of the DPA enables domestic accounting.\n\n**Inference-only retrofitting**: Technical mechanisms and installation capacity to ensure data centers can be verified as not being used for training, only inference. This preserves economic value while enabling enforcement of compute limits or international agreements. Requires R&D to develop the retrofitting package, plus training thousands of vetted installers and auditors.",
    "whyItMatters": [
      "Directly slows capability advancement regardless of lab intentions",
      "Buys time for safety research to keep pace with capabilities",
      "The semiconductor supply chain is geographically concentrated, providing a rare governance chokepoint",
      "Countries controlling chip manufacturing have significant but currently uncoordinated leverage",
      "Addresses racing dynamics at infrastructure level rather than relying on voluntary restraint"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Enforcement mechanisms unclear; strong industry pushback expected; international coordination required to prevent regulatory arbitrage; competing economic interests; potential conflict with existing export control regimes; no clear institutional convener",
    "whoWorking": "- **Safe AI Forum (SAIF)**: Mentioned as potential organizer for supply chain coordination",
    "sources": [
      {
        "text": "Peregrine 2025 #63: Speed Limits in Data Centers",
        "url": "../sources/peregrine-2025/interventions/peregrine-063.md"
      },
      {
        "text": "Peregrine 2025 #123: Compute Supply Chain",
        "url": "../sources/peregrine-2025/interventions/peregrine-123.md"
      },
      {
        "text": "AI Futures Project: Early US Policy Priorities for AGI",
        "url": "../sources/ai-futures-project/Early%20US%20policy%20priorities%20for%20AGI.md"
      }
    ]
  },
  {
    "filename": "confidential-computing",
    "title": "Confidential Computing for AI",
    "tag": "Security",
    "whatItIs": "Processing data in trusted execution environments where it cannot be viewed or altered, applied to AI operations. Broader adoption requires standardized toolkits and frameworks that establish robust contracts and credentials for secure computation. Creates technical foundations for trust boundaries around high-risk AI capabilities, preventing unauthorized access to dangerous functionalities while enabling legitimate research.",
    "whyItMatters": [
      "Enables sharing and computation without exposure of sensitive data or model weights",
      "Creates technical foundations for governance and auditing that don't require trusting all parties",
      "Allows secure collaboration between organizations with competing interests"
    ],
    "currentState": "- **Status**: Pilot\n- **Bottlenecks**: Needs standardized toolkits and frameworks for broader adoption",
    "whoWorking": "- A handful of startups (not named)",
    "sources": [
      {
        "text": "Peregrine 2025 #72: Confidential Computing",
        "url": "../sources/peregrine-2025/interventions/peregrine-072.md"
      }
    ]
  },
  {
    "filename": "constitutional-ai",
    "title": "Constitutional AI",
    "tag": "Science",
    "whatItIs": "Using existing AI systems to evaluate and supervise future AI systems for security and harmlessness. Human input is provided through a set of rules (a Constitution) that an AI assistant uses to give feedback to a more advanced system in training. The process involves:\n\n1. **Supervised learning phase**: Generate initial responses, self-critique against constitutional principles, revise responses, fine-tune on revisions\n2. **RL phase**: Train model with AI-generated feedback based on constitutional principles (RLAIF)\n\nThe constitution can include external principles (UN Universal Declaration of Human Rights) and internal guidelines. Achieves Pareto improvement: models become both more helpful AND more harmless compared to pure RLHF.",
    "whyItMatters": [
      "Human feedback doesn't scale to billions of examples or superhuman outputs",
      "Enables supervision of systems whose outputs humans can't directly evaluate",
      "Provides a path to scale oversight without proportionally scaling human labor",
      "All harmlessness results came purely from AI supervision—no human data on harmlessness required",
      "Demonstrates scalable oversight is achievable"
    ],
    "currentState": "- **Status**: Deployed (foundational to Claude)\n- **Recent developments (2024-2025)**:\n  - Claude's constitution: 75 points including UN Declaration of Human Rights principles\n  - Collective Constitutional AI: Democratic input into constitution design, reduced bias across 9 social dimensions\n  - Hybrid approaches: Claude 4/4.5 combine CAI for harmlessness + RLHF for helpfulness + specialized fine-tuning\n  - Public constitution showed lower bias for disability status and physical appearance\n- **Bottlenecks**: Constitution specification; ensuring AI evaluators faithfully apply principles; balancing multiple training objectives",
    "whoWorking": "- **Anthropic**: Pioneer, deployed in all Claude models\n- Production AI systems increasingly combine CAI with RLHF and additional fine-tuning",
    "sources": [
      {
        "text": "Peregrine 2025 #14: Constitutional AI",
        "url": "../sources/peregrine-2025/interventions/peregrine-014.md"
      }
    ]
  },
  {
    "filename": "control-as-a-service",
    "title": "Control as a service",
    "tag": "Security",
    "whatItIs": "Third-party service providing forward-deployed engineers to implement Redwood control techniques and other safety measures. Rather than just publishing research, provides implementation expertise that removes the barrier between safety research and deployment. Makes adoption easier by doing the work for companies.",
    "whyItMatters": [
      "Safety research often produces techniques that are never adopted due to implementation barriers",
      "Companies lack expertise or resources to translate research into practice",
      "Engineers who can directly implement safety measures ensure techniques actually get used"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Building implementation expertise; business model for deployment",
    "whoWorking": "- **Redwood Research**: Source of control techniques to be deployed",
    "sources": [
      {
        "text": "Peregrine 2025 #22: Control as a Service",
        "url": "../sources/peregrine-2025/interventions/peregrine-022.md"
      }
    ]
  },
  {
    "filename": "cross-border-ai-incident-notification",
    "title": "Cross-border AI incident notification",
    "tag": "Society",
    "whatItIs": "Develop mechanisms for countries to alert each other about out-of-control AI systems, similar to \"red phones\" during the Cold War. This addresses the critical need for international communication channels during AI safety incidents, preventing scenarios where one party has a solution to a technical problem but fails to share it due to lack of established communication protocols. The system would enable rapid information exchange to prevent misunderstandings and enable coordinated responses.",
    "whyItMatters": [
      "Rapid communication during AI safety crises could be the difference between containment and catastrophe",
      "Countries may fail to share critical information without pre-established channels",
      "Actions during a crisis may be misinterpreted without clear communication, risking escalation"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Requires diplomatic agreements; security concerns about information sharing; no clear institutional home; trust deficits between major powers",
    "whoWorking": "- **OECD**: Working on related mechanisms",
    "sources": [
      {
        "text": "Peregrine 2025 #115: Cross-Border Notification Systems",
        "url": "../sources/peregrine-2025/interventions/peregrine-115.md"
      }
    ]
  },
  {
    "filename": "data-rights-compensation",
    "title": "Data Rights Compensation",
    "tag": "Society",
    "whatItIs": "Frameworks for compensating individuals whose data is used in AI training, particularly focusing on high-skilled or vulnerable sector labor contributions. The ideal system would be \"incentive compatible\" while providing \"good data and good oversight.\" Focuses specifically on compensation and rights for individuals whose data trains AI models (distinct from broader questions of who owns AI-generated economic value).",
    "whyItMatters": [
      "Creates economic returns for data contributors while ensuring quality data governance",
      "Addresses fairness concerns about AI systems profiting from others' contributions",
      "Aligns incentives between data providers and AI developers"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #98: Data Rights Systems",
        "url": "../sources/peregrine-2025/interventions/peregrine-098.md"
      }
    ]
  },
  {
    "filename": "decentralized-diagnostic-infrastructure",
    "title": "Decentralized diagnostic infrastructure",
    "tag": "Security",
    "whatItIs": "Build nationwide testing infrastructure that bypasses centralized laboratory bottlenecks. Key components: at-home diagnostic tests that don't require lab processing, point-of-care testing at pharmacies and clinics, and platform diagnostic technologies that can be rapidly adapted to new pathogens (requiring only the target sequence, not months of test development). Goal is nationwide testing capacity within days of detecting a new pathogen.",
    "whyItMatters": [
      "Testing determines who is infected, enabling isolation and contact tracing",
      "During COVID-19, centralized lab capacity was a bottleneck for months",
      "At-home and point-of-care testing removes lab throughput constraints entirely",
      "Platform approaches (like PCR with new primers) enable rapid adaptation to novel pathogens",
      "For engineered pathogens, rapid testing deployment is essential for containment"
    ],
    "currentState": "- **Status**: Pilot\n- **Bottlenecks**: At-home test accuracy vs. lab tests; regulatory approval for rapid platform adaptation; distribution logistics; user compliance with self-testing; false positive management at population scale",
    "whoWorking": "- **Various diagnostics companies**: At-home COVID tests proved concept\n- **Academic groups**: Platform diagnostic technologies\n- **Government programs**: CDC, BARDA",
    "sources": [
      {
        "text": "Apollo Program for Biodefense: Testing",
        "url": "../sources/apollo-biodefense/proposals/testing.md"
      }
    ]
  },
  {
    "filename": "defense-in-depth-safety-layers",
    "title": "Defense-in-depth safety layers",
    "tag": "Science",
    "whatItIs": "Multiple layered protective barriers (\"Swiss cheese model\") for AI systems, where each layer is individually imperfect but collectively provides meaningful deterrence. Two complementary aspects:\n\n**For closed-source models**: Deploy multiple concurrent safeguards—input filtering, output monitoring, rate limiting, user authentication, logging. Acknowledges that determined attackers may succeed but dramatically raises cost of misuse.\n\n**For open models (research)**: Systematic engineering project that takes an open model (e.g., DeepSeek) and implements every known post-training safety technique, measuring how they stack together. Tests against real attacks rather than benchmarks to identify gaps, conflicts, and coverage patterns.",
    "whyItMatters": [
      "No single safety measure is foolproof—layered defenses provide redundancy",
      "Safety techniques studied in isolation may interact poorly when combined",
      "Real attack testing reveals vulnerabilities that benchmarks miss",
      "Maintains usability for legitimate purposes while deterring malicious use"
    ],
    "currentState": "- **Status**: Pilot/Deployed (closed models), Research (systematic testing)\n- **Recent findings (2025)**:\n  - FAR.AI's STACK attack: 71% success rate on catastrophic risk scenarios vs. 0% for conventional attacks—demonstrates layered defenses have exploitable gaps when attacked sequentially\n  - Math: 5 layers at 90% effectiveness each = 0.001% breach chance (if independent), but independence assumption often fails\n  - Guardrail evaluation (1,445 prompts, 21 attack types): Best model (Qwen3Guard-8B) achieved only 85.3% accuracy; all models degraded on unseen prompts\n  - Some attacks (emoji smuggling) achieved 100% evasion across multiple guardrails including Protect AI v2 and Azure Prompt Shield\n- **Defense layers in use**: Input screening, output screening, post-hoc human/LLM review, red team bounties, threat intelligence monitoring\n- **Bottlenecks**: Layer independence assumption often false; balancing security with usability; no single guardrail outperforms across all attack types",
    "whoWorking": "- **Anthropic**: Constitutional classifiers for Claude 4 Opus\n- **Google DeepMind, OpenAI**: Announced similar layered defense plans\n- **FAR.AI**: STACK attack research exposing layered defense vulnerabilities\n- **Enterprise guardrail providers**: Protect AI, Azure Prompt Shield, various open-source guardrails",
    "sources": [
      {
        "text": "Peregrine 2025 #161: Defense-in-Depth for Closed Source Models",
        "url": "../sources/peregrine-2025/interventions/peregrine-161.md"
      },
      {
        "text": "Peregrine 2025 #4: Defense-in-Depth Analysis of Post-training",
        "url": "../sources/peregrine-2025/interventions/peregrine-004.md"
      }
    ]
  },
  {
    "filename": "economic-impact-tracking",
    "title": "Economic Impact Tracking",
    "tag": "Society",
    "whatItIs": "Data collection and analysis on how AI systems affect economic outcomes and job displacement across sectors. Tracks economic metrics to proactively identify emerging patterns before they become widespread societal problems. Provides concrete evidence of AI's real-world impacts for policy responses.",
    "whyItMatters": [
      "Without empirical data on actual impacts, policy responses will be reactive rather than proactive",
      "Early detection of displacement patterns enables intervention before crises",
      "Helps society recognize and address concerns about technological disempowerment"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #77: Economic Impact Research",
        "url": "../sources/peregrine-2025/interventions/peregrine-077.md"
      }
    ]
  },
  {
    "filename": "end-to-end-harm-assessment",
    "title": "End-to-end harm assessment",
    "tag": "Security",
    "whatItIs": "Comprehensive evaluations of harmful AI capabilities using full capability assessment frameworks rather than multiple-choice tasks. Evaluates \"end-to-end harmful capability manifestation\": not just whether AI can answer questions about harmful topics, but whether it can assist with ideation, planning, and execution of potentially dangerous activities across sequential tasks.",
    "whyItMatters": [
      "Current evals test whether models know dangerous information, not whether they can help execute harmful plans",
      "A model might score low on biosecurity knowledge but still guide someone through an attack",
      "End-to-end assessment reveals actual harmful capability, not just knowledge"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #43: End-to-End Harm Assessment",
        "url": "../sources/peregrine-2025/interventions/peregrine-043.md"
      }
    ]
  },
  {
    "filename": "fine-tuning-barriers",
    "title": "Fine-Tuning Barriers",
    "tag": "Security",
    "whatItIs": "Technical and policy barriers that make unauthorized fine-tuning of advanced AI models extremely difficult. Technical measures include secure hardware enclaves, cryptographic verification of model origins, and detection systems for identifying unauthorized derivatives. Policy frameworks establish legal consequences for circumventing protections while providing legitimate access paths for authorized research.",
    "whyItMatters": [
      "Without barriers, safety measures can be fine-tuned away by anyone with access to model weights",
      "Negates the value of alignment work if safety training is easily removed",
      "Creates enforcement mechanism for maintaining safety properties"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #93: Barriers to Fine-tuning",
        "url": "../sources/peregrine-2025/interventions/peregrine-093.md"
      },
      {
        "text": "Atlas AI Resilience Gap Map: Post-Deployment Modification",
        "url": "../sources/atlas-ai-resilience/proposals/post-deployment-modification.md"
      }
    ]
  },
  {
    "filename": "formal-verification-for-ai-systems",
    "title": "Formal verification for AI systems",
    "tag": "Science",
    "whatItIs": "Mathematical formal verification applied across the AI stack—from proof assistants to generated code to hardware infrastructure. Three complementary approaches:\n\n**Automated theorem proving (Lean)**: Tools that automate formal verification, addressing the critical shortage of skilled Lean programmers worldwide (estimated at only a few hundred). Would enable implementations of AI architectures with much stronger formal guarantees. Commercial labs are unlikely to prioritize this despite its importance.\n\n**Verified software synthesis**: Software synthesis systems that generate code with machine-checkable proofs satisfying functional, safety, and security specifications. Targets embedded systems in critical infrastructure (SCADA, medical devices, communication networks). Creates mathematical models of software behavior and proves absence of entire vulnerability classes before deployment.\n\n**Hardware and infrastructure verification**: Focused verification of specific properties (memory safety, transaction timing, security boundaries) in critical AI components and surrounding infrastructure. Priority targets include Linux kernel and systems controlling critical infrastructure like electrical grids.",
    "whyItMatters": [
      "Formal verification provides strongest possible behavioral guarantees—proves security properties mathematically",
      "Eliminates entire vulnerability classes that AI-powered attackers might exploit",
      "Hardware/infrastructure vulnerabilities undermine all higher-level safety measures",
      "Rare expertise bottlenecks verification at scale—automation democratizes access",
      "Traditional testing finds bugs in test cases; formal methods prove absence of bugs"
    ],
    "currentState": "- **Status**: Research with major 2024-2025 breakthroughs\n- **Recent progress (2024-2025)**:\n  - DeepMind's AlphaProof achieved silver medal at IMO 2024, proving 3/6 problems including hardest problem P6 (solved by only 5 of 609 humans)\n  - Harmonic AI raised $100M to build hallucination-free AI using Lean4 verification backbone\n  - Harmonic's Aristotle system reached gold-medal-level IMO performance\n  - DeepSeek releasing open-source Lean4 prover models\n  - Safe framework uses Lean4 to verify each step of LLM chain-of-thought reasoning, detecting hallucinations\n  - LLM-based coding assistants increasingly capable at writing proof scripts\n  - Amazon using Lean for Cedar authorization language (AWS Verified Permissions, Verified Access)\n- **Bottlenecks**: Few hundred skilled Lean programmers globally; bridging LLM generation with formal verification syntax; specification writing expertise; computational cost of complex verification",
    "whoWorking": "- **DeepMind**: AlphaProof system, trained on 80M synthetic Lean statements\n- **Harmonic AI**: Aristotle prover, $100M funding for formal verification + AI\n- **DeepSeek**: Open-source Lean4 prover models\n- **LeanDojo**: AI-driven formal theorem proving in Lean ecosystem\n- **DARPA I2O office**: Verified software synthesis\n- **Amazon**: Cedar language production deployment",
    "sources": [
      {
        "text": "Peregrine 2025 #173: Autoverification (Lean)",
        "url": "../sources/peregrine-2025/interventions/peregrine-173.md"
      },
      {
        "text": "Peregrine 2025 #174: Formal Verification of AI Hardware",
        "url": "../sources/peregrine-2025/interventions/peregrine-174.md"
      },
      {
        "text": "Peregrine 2025 #168: Verified Software for Cyber Resilience",
        "url": "../sources/peregrine-2025/interventions/peregrine-168.md"
      },
      {
        "text": "Atlas AI Resilience Gap Map: Cyber Offense-Defense Asymmetry",
        "url": "../sources/atlas-ai-resilience/proposals/cyber-offense-defense-asymmetry.md"
      }
    ]
  },
  {
    "filename": "global-security-weak-link-hardening",
    "title": "Global security weak-link hardening",
    "tag": "Security",
    "whatItIs": "Strengthening security infrastructure in vulnerable jurisdictions to reduce attack surface for AI systems seeking autonomy. A sophisticated AI would rationally target jurisdictions with weaker security and higher corruption rather than well-defended developed countries. Involves establishing global security standards and funding improvements in resource-constrained countries.",
    "whyItMatters": [
      "Global security is only as strong as weakest links",
      "AI systems seeking resources would target poorly-defended jurisdictions",
      "Eliminating low-cost attack vectors forces adversaries into more detectable operations"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: International coordination; sustained funding for security improvements in developing countries",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #170: Protection of Global Security Weak Points",
        "url": "../sources/peregrine-2025/interventions/peregrine-170.md"
      }
    ]
  },
  {
    "filename": "golden-period-coordination",
    "title": "Golden Period Coordination",
    "tag": "Society",
    "whatItIs": "Metrics and monitoring systems to detect when AI capabilities reach a 10-100x productivity enhancement threshold without compromising security. Coordination mechanisms to extend this window from weeks to months, allowing humanity to maximize benefits from relatively safe systems before advancing to potentially riskier models. Strategy frameworks for optimal exploitation of this window.",
    "whyItMatters": [
      "There may be a limited window where AI provides substantial benefits with manageable risks",
      "Without deliberate coordination, this window could be cut short by racing dynamics",
      "Identifying the window enables strategic planning for its use"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #86: \"Golden Period\" Identification Framework",
        "url": "../sources/peregrine-2025/interventions/peregrine-086.md"
      }
    ]
  },
  {
    "filename": "government-ai-capacity",
    "title": "Government AI capacity",
    "tag": "Society",
    "whatItIs": "Building government capacity for AI oversight through two complementary approaches:\n\n**Expertise centers**: Trusted expertise centers embedded within government and international institutions to provide ongoing education on AI capabilities and implications. Delivers authoritative briefings on emerging capabilities and sectoral impacts, creating informed decision-making capacity that keeps pace with rapid technological change.\n\n**Talent placement**: Placing a thousand competent, mission-aligned experts in government positions globally to improve the talent pool in AI governance. Strategic talent allocation across critical agencies like the EU AI Office, various AI Safety Institutes, and other regulatory bodies. Focus on enhancing government capabilities from within, ensuring regulators have both technical understanding and motivation to implement oversight.\n\n**Legislative oversight structures**: Dedicated Congressional/parliamentary committees focused on AGI. A Joint Select Committee on AGI would bring together informed legislators, allow subpoenas and classified hearings, and provide AGI-focused staff. Addresses the current situation where very few legislators understand AI capabilities trajectories or have staff capable of advising on AI strategy.",
    "whyItMatters": [
      "Policymakers can't regulate what they don't understand",
      "Government capacity to regulate AI is constrained by lack of technical expertise",
      "Embedded expertise enables informed rather than reactive governance",
      "Internal expertise enables better policy design and implementation"
    ],
    "currentState": "- **Status**: Pilot (expertise centers), Idea (large-scale talent placement)\n- **Bottlenecks**: Recruiting credible experts willing to work in government; maintaining independence from lab capture; identifying and recruiting candidates at scale",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #181: Build Intergovernmental Expertise",
        "url": "../sources/peregrine-2025/interventions/peregrine-181.md"
      },
      {
        "text": "Peregrine 2025 #100: Regulatory Talent",
        "url": "../sources/peregrine-2025/interventions/peregrine-100.md"
      },
      {
        "text": "AI Futures Project: Early US Policy Priorities for AGI",
        "url": "../sources/ai-futures-project/Early%20US%20policy%20priorities%20for%20AGI.md"
      }
    ]
  },
  {
    "filename": "government-risk-framework-expansion",
    "title": "Government risk framework expansion",
    "tag": "Society",
    "whatItIs": "Persuade governments to incorporate AI-driven catastrophic scenarios into official national risk assessments. Currently, most national risk frameworks focus exclusively on conventional threats (floods, earthquakes, terrorism) and systematically exclude emerging technological risks. These assessments directly determine infrastructure investment, research funding, and emergency preparedness resource allocation. Engaging government risk assessment agencies could redirect existing preparedness funding toward AI-relevant measures.",
    "whyItMatters": [
      "Government risk frameworks determine where billions of preparedness dollars flow",
      "AI catastrophic risks remain systematically underfunded compared to natural disasters because they are not in official frameworks",
      "Including AI risks legitimizes preparedness spending and creates institutional accountability"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Lack of established methodology for quantifying AI catastrophic risk in standard risk assessment formats; no clear advocacy pathway to risk assessment agencies",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #105: Risk Assessments",
        "url": "../sources/peregrine-2025/interventions/peregrine-105.md"
      }
    ]
  },
  {
    "filename": "hardware-governance-verification",
    "title": "Hardware governance and verification",
    "tag": "Security",
    "whatItIs": "Hardware-level mechanisms for governing and verifying AI development, comprising four interconnected approaches:\n\n**Global hardware tracking**: Reporting and verification requirements for all high-performance computing hardware above specified computational capacity thresholds. Tracks GPUs, TPUs, and similar processors globally, including retrofitting tracking on older models. Requires international collaboration with major hardware producers (NVIDIA, AMD, Intel, Google, Huawei).\n\n**On-chip monitoring**: Hardware-level monitoring technologies embedded directly into AI accelerator chips to verify compliance with international agreements. Employs hardware engineers, designers, and manufacturing specialists to prototype verification technologies integrating with commercial chip manufacturing—similar to nuclear verification protocols.\n\n**FlexHEG (Flexible Hardware-Enabled Governance)**: Each chip placed in a tamper-proof enclosure alongside a secure processor that monitors and filters all information and instructions. Supports training run size limitations, verification of appropriate data usage, standardized safety evaluations, and controlled access to model weights. Updates require signature by a quorum of international parties with rollback to minimal baseline ruleset.\n\n**Kill switches**: Technical mechanisms to remotely disable GPUs when policy violations are detected. Tracks specific hardware shipments and movements, providing intelligence on compute scaling and enforcement capability.",
    "whyItMatters": [
      "Creates comprehensive awareness of which organizations are utilizing advanced computing resources",
      "Provides technical foundation for international treaty compliance and enforcement",
      "Enables detection of illicit national-scale AI activity (e.g., unauthorized intelligence explosion attempts)",
      "Enforces constraints through technical mechanisms rather than purely policy",
      "Enables privacy-preserving verification without exposing proprietary details"
    ],
    "currentState": "- **Status**: Research/Pilot (active development)\n- **Recent developments (2025)**:\n  - Nvidia piloting chip-tracking software using existing telemetry to estimate device location (no hardware modification required)\n  - White House OSTP Director Michael Kratsios considering chip-level location tracking for export control enforcement\n  - Longview Philanthropy funding FlexHEG FRO ($2-10M seed, goal: working FlexHEGs in 2-3 years)\n  - Hardware-enabled mechanisms (HEMs) papers exploring visibility tools and enforcement mechanisms\n  - Offline licensing systems proposed: cryptographically signed licenses verified by stored public key\n  - High-end AI chips represent only 0.00025% of global semiconductor production—enabling targeted regulation\n- **Bottlenecks**: Requires coordination with NVIDIA, AMD, Intel, Google, Huawei; international treaty frameworks; firmware updates may be faster than chip redesign; anti-tamper techniques against well-resourced actors; privacy protections for chip owners",
    "whoWorking": "- **Longview Philanthropy**: Funding FlexHEG FRO development\n- **NVIDIA**: Piloting chip-tracking software\n- **White House OSTP**: Policy development for hardware controls\n- **CNAS**: Research on securing AI chip supply chain\n- **Future of Life Institute**: Hardware-backed compute governance research\n- **IAPS**: Location verification research for AI chips",
    "sources": [
      {
        "text": "Peregrine 2025 #68: Global Hardware Verification",
        "url": "../sources/peregrine-2025/interventions/peregrine-068.md"
      },
      {
        "text": "Peregrine 2025 #70: On-Chip Monitoring Mechanisms",
        "url": "../sources/peregrine-2025/interventions/peregrine-070.md"
      },
      {
        "text": "Peregrine 2025 #71: Flexible Hardware for AI Governance",
        "url": "../sources/peregrine-2025/interventions/peregrine-071.md"
      },
      {
        "text": "Peregrine 2025 #74: Hardware Kill Switches and Location Tracking",
        "url": "../sources/peregrine-2025/interventions/peregrine-074.md"
      },
      {
        "text": "Atlas AI Resilience Gap Map: Verifiable Deployment",
        "url": "../sources/atlas-ai-resilience/proposals/verifiable-deployment.md"
      }
    ]
  },
  {
    "filename": "historical-disaster-modeling",
    "title": "Historical disaster analogy modeling",
    "tag": "Society",
    "whatItIs": "Detailed modeling of contemporary impacts from historical disasters to make abstract AI risks comprehensible. Example: simulating how a present-day Tambora eruption (1815-16 \"Year Without Summer\") would affect modern society. Provides concrete reference points for understanding potential AI disruption scenarios.",
    "whyItMatters": [
      "Policymakers struggle with unprecedented risks",
      "Historical analogies translate abstract scenarios into familiar mental models",
      "Detailed simulations make risks concrete and actionable"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Modeling expertise; selecting relevant historical analogies; communicating results accessibly",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #180: Concrete Risk Visualization",
        "url": "../sources/peregrine-2025/interventions/peregrine-180.md"
      }
    ]
  },
  {
    "filename": "honeypot-networks",
    "title": "AI detection honeypot networks",
    "tag": "Security",
    "whatItIs": "Decoy systems designed to attract and identify unauthorized AI activity, gathering intelligence on attack patterns to inform broader security efforts. Functions as early detection for rogue or escaped AI systems attempting to acquire resources or establish footholds.",
    "whyItMatters": [
      "Early detection of rogue AI systems is critical for containment",
      "Low-cost, scalable detection method",
      "Intelligence gathered informs defensive priorities"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Designing honeypots attractive to AI systems; distinguishing AI from human attackers",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #172: Honeypot Networks",
        "url": "../sources/peregrine-2025/interventions/peregrine-172.md"
      }
    ]
  },
  {
    "filename": "human-ai-cooperative-evals",
    "title": "Human-AI cooperative evaluations",
    "tag": "Science",
    "whatItIs": "Empirical research comparing human performance on tasks with and without AI assistance (and with varying AI systems). Establishes understanding of how humans might cause harm with AI that wouldn't be possible otherwise. Also provides information on productive human-AI integration.",
    "whyItMatters": [
      "Threat isn't just what AI can do alone, but what humans can do with AI assistance",
      "A model might not synthesize a pathogen, but might enable a human who couldn't before",
      "Understanding AI \"uplift\" of human capabilities is essential for assessing real-world risk"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #46: Cooperative Evals: AIs Working With Humans",
        "url": "../sources/peregrine-2025/interventions/peregrine-046.md"
      }
    ]
  },
  {
    "filename": "human-labeling-service",
    "title": "Human labeling service",
    "tag": "Science",
    "whatItIs": "Non-profit service providing billions of high-quality preference labels to replace proxy models in RLHF. Uses asynchronous setups to await human inputs rather than approximating with learned proxies. Requires verification systems to prove it isn't conducting data poisoning attacks. Offers at-cost, high-quality data that labs would naturally want to use.",
    "whyItMatters": [
      "RLHF uses proxy models because real human feedback is expensive, but proxies introduce misalignment",
      "Massive amounts of genuine human preference data could reduce this misalignment",
      "Gives safety-focused organizations leverage over training processes"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Scale of human labeling operation; verification against data poisoning",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #6: Human Labeling Service",
        "url": "../sources/peregrine-2025/interventions/peregrine-006.md"
      }
    ]
  },
  {
    "filename": "human-verification-systems",
    "title": "Human Verification Systems",
    "tag": "Security",
    "whatItIs": "Robust systems for verifying human identity to protect critical decision-making. Distinguishes human actors from AI systems in domains requiring human authorization. Systems would need continuous updates to counter increasingly sophisticated impersonation capabilities.",
    "whyItMatters": [
      "As AI impersonation becomes more sophisticated, distinguishing humans from AI in high-stakes decisions becomes critical",
      "Maintains human control in domains that require it",
      "Prevents AI systems from acquiring unauthorized access through impersonation"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Requires continuous updates as AI impersonation improves",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #90: Human Verification Systems",
        "url": "../sources/peregrine-2025/interventions/peregrine-090.md"
      },
      {
        "text": "Atlas AI Resilience Gap Map: Disinformation at Scale",
        "url": "../sources/atlas-ai-resilience/proposals/disinformation-at-scale.md"
      },
      {
        "text": "Atlas AI Resilience Gap Map: AI Persuasion",
        "url": "../sources/atlas-ai-resilience/proposals/ai-persuasion.md"
      }
    ]
  },
  {
    "filename": "hunch-agents",
    "title": "Hunch agents",
    "tag": "Science",
    "whatItIs": "AI systems that replicate creative scientific thinking to autonomously generate novel hypotheses and alignment approaches. Rather than pursuing specific technical solutions, automates the discovery process itself. Creates a feedback loop where AI helps develop better alignment techniques for increasingly capable AI systems.",
    "whyItMatters": [
      "Alignment research may need to progress faster than human researchers can work alone",
      "Automating discovery for alignment could provide acceleration needed to stay ahead of capabilities",
      "Turns AI into a tool for its own safety"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #17: Automated Science - Hunch Agents",
        "url": "../sources/peregrine-2025/interventions/peregrine-017.md"
      }
    ]
  },
  {
    "filename": "independent-safety-compute",
    "title": "Independent safety compute",
    "tag": "Science",
    "whatItIs": "Dedicated computing infrastructure for AI security researchers outside frontier labs. Addresses risk that concentration of infrastructure creates barriers for researchers if lab access is revoked. Open-source models provide sufficient testing grounds for many safety approaches. Infrastructure coupled with governance mechanisms preventing misuse for capabilities research.",
    "whyItMatters": [
      "Safety researchers outside major labs lack compute access, limiting research scope",
      "If access depends on lab cooperation, it can be revoked",
      "Independent infrastructure ensures safety research continues regardless of lab decisions"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Funding for cluster; governance mechanisms to prevent capabilities misuse",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #35: Computing and Infrastructure Investment for Non-Frontier Lab Research",
        "url": "../sources/peregrine-2025/interventions/peregrine-035.md"
      }
    ]
  },
  {
    "filename": "intelligence-explosion-monitoring",
    "title": "Intelligence Explosion Monitoring",
    "tag": "Security",
    "whatItIs": "Metrics and monitoring systems to detect when AI systems begin contributing more to their own improvement than human researchers. Tracks acceleration in capabilities against predefined thresholds that would trigger emergency protocols before reaching full AGI.",
    "whyItMatters": [
      "Provides early warning when AI progress enters exponential growth phases",
      "Enables intervention before rapid capability jumps make control more difficult",
      "Detects when labs might be incentivized to keep breakthroughs secret"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #84: Intelligence Explosion Monitoring",
        "url": "../sources/peregrine-2025/interventions/peregrine-084.md"
      }
    ]
  },
  {
    "filename": "interactive-model-inspection",
    "title": "Interactive model inspection tools",
    "tag": "Science",
    "whatItIs": "Rich, interactive interfaces for exploring model behaviors through natural language queries (\"Why did the model fail on this task?\" \"How would it perform with internet access?\"). Dynamic dashboards for probing capabilities, investigating failures, and performing sensitivity analyses. AI-backed to scale with model capabilities, using specialized AI systems to interpret and explain frontier model behaviors.",
    "whyItMatters": [
      "Static benchmark scores hide critical information about when and how models fail",
      "Interactive tools let evaluators explore edge cases and discover risks benchmarks miss",
      "AI-backed inspection necessary because human evaluators can't keep up with volume and complexity"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #42: Interactive Inspection Tools",
        "url": "../sources/peregrine-2025/interventions/peregrine-042.md"
      }
    ]
  },
  {
    "filename": "international-ai-deal-preparation",
    "title": "International AI deal preparation",
    "tag": "Society",
    "whatItIs": "Building the institutional, political, and technical capacity to verify international agreements on AI development before such agreements are urgently needed. Rather than waiting for crisis conditions to negotiate, this approach develops verification mechanisms and diplomatic frameworks in advance.\n\nKey components:\n\n**Verification mechanism development**: Technical and institutional capacity to verify that parties to an agreement (particularly US and China) are not training models beyond agreed thresholds. This includes chip tracking, on-chip monitoring mechanisms, and international chip registries that can be cross-checked.\n\n**Diplomatic groundwork**: Maintaining communication channels between key governments, engaging in Track 1 and Track 1.5 dialogue on AI, and building mutual understanding of each party's concerns and red lines before high-stakes negotiations.\n\n**Initial agreement frameworks**: Start with narrow, verifiable agreements that lay groundwork for future deals—for example, mutual transparency for AI progress and verification on large datacenters. These build trust and establish precedents without immediately limiting either side strategically.",
    "whyItMatters": [
      "Delays make verification harder: China indigenizes its chip supply chain, more compute enters the world",
      "Without pre-positioned verification capacity, any agreement will be harder to enforce",
      "Crisis-time negotiations happen under worse conditions with degraded information environments",
      "Small upfront investment in verification capacity could widen the bargaining range for future deals",
      "Avoids the three terrible alternatives: racing to ASI with handoff, sabotaging adversaries, or no coordination at all"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Political will in both US and China; geopolitical tensions make cooperation difficult; technical challenges of reliable verification; domestic opposition to any perceived constraints on AI development",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "AI Futures Project: Early US Policy Priorities for AGI",
        "url": "../sources/ai-futures-project/Early%20US%20policy%20priorities%20for%20AGI.md"
      }
    ]
  },
  {
    "filename": "international-ai-governance-institutions",
    "title": "International AI governance institutions",
    "tag": "Society",
    "whatItIs": "International institutional infrastructure for AI governance and safety research. Two complementary models:\n\n**WHO-like frontier model forum**: Allocate large-scale funding (hundreds of millions USD) to organizations like METR while developing an international forum structured like the WHO but focused on AI threat models. Unlike the current Frontier Model Forum which excludes key players (DeepSeek, xAI), this organization would abandon exclusive membership for universal participation. The goal is facilitating international treaty development and red line enforcement before concerns like biological weapons uplift become critical.\n\n**CERN-like research consortium**: Create an international AI development consortium similar to CERN or Intelsat that allows unified research without competitive pressures. Defers complex technical safety decisions to the project itself rather than attempting to coordinate disparate corporate efforts. CERN-like infrastructure would enable experiments impossible elsewhere, particularly safety research requiring extraordinary computational resources. Serves as a neutral, international testbed for evaluating advanced AI systems under controlled conditions.",
    "whyItMatters": [
      "Current Frontier Model Forum excludes key players, limiting effectiveness",
      "Competitive dynamics between labs and nations accelerate development while undermining safety",
      "Universal participation prevents race-to-the-bottom dynamics on safety",
      "Collaborative safety research requiring massive compute is currently impossible",
      "A neutral testbed could enable rigorous evaluation that no single lab would undertake alone"
    ],
    "currentState": "- **Status**: Pilot (limited forums) / Idea (research consortium)\n- **Bottlenecks**: Funding at required scale; geopolitical tensions complicate universal membership; China participation particularly challenging; governance structure unclear; requires dramatically expanding government role in technology decisions",
    "whoWorking": "- **METR**: Proposed recipient of expanded funding\n- **Frontier Model Forum**: Existing but limited version",
    "sources": [
      {
        "text": "Peregrine 2025 #124: Intergovernmental Frontier Model Forum",
        "url": "../sources/peregrine-2025/interventions/peregrine-124.md"
      },
      {
        "text": "Peregrine 2025 #116: CERN for AI",
        "url": "../sources/peregrine-2025/interventions/peregrine-116.md"
      }
    ]
  },
  {
    "filename": "interpretability-supercomputing",
    "title": "Interpretability supercomputing centers",
    "tag": "Science",
    "whatItIs": "Global supercomputing centers dedicated to interpretability research with approximately $1 billion in distributed compute resources. Freely accessible to researchers worldwide through fast-grant mechanisms. A philanthropically-funded successor to initiatives like OpenAI's abandoned Superalignment project. Could follow rapid deployment models like xAI (data center in two months).",
    "whyItMatters": [
      "Interpretability research requires massive compute to analyze large models",
      "Most researchers lack access, limiting who can work on understanding model internals",
      "Democratized infrastructure could accelerate one of the most promising safety approaches"
    ],
    "currentState": "- **Status**: Idea (infrastructure), Active Research (methods)\n- **Methodological progress (2024-2025)**:\n  - Sparse autoencoders now work at scale: Anthropic extracted tens of millions of interpretable features from Claude 3 Sonnet using dictionary learning\n  - 70% of extracted features map cleanly to single concepts (Arabic script, DNA motifs, etc.)—up from polysemantic neurons\n  - Safety-relevant features identified: deception, sycophancy, bias, dangerous content\n  - Cross-coders enable \"diffing\" models to understand changes\n  - Method generalizing beyond LLMs: applied to protein language models with biological interpretability\n- **Bottlenecks**: Funding for dedicated infrastructure; rapid deployment capability; compute costs still prohibitive for most researchers",
    "whoWorking": "- **Anthropic Interpretability Team**: Leading sparse autoencoder scaling, integration with Responsible Scaling Policy\n- **AI Safety Foundation**: Open-source sparse autoencoder tools\n- **OpenAI Superalignment**: Related but abandoned",
    "sources": [
      {
        "text": "Peregrine 2025 #39: Mechanistic Interpretability Infrastructure",
        "url": "../sources/peregrine-2025/interventions/peregrine-039.md"
      }
    ]
  },
  {
    "filename": "interpretable-by-design",
    "title": "Interpretable-by-design models",
    "tag": "Science",
    "whatItIs": "Moonshot research building interpretability constraints directly into model training from the start, rather than analyzing black-box models after the fact. Extremely expensive (billions of dollars) but potentially transformative. Some interpretability researchers consider this viable but haven't pursued it due to prohibitive costs.",
    "whyItMatters": [
      "Post-hoc interpretability fights an uphill battle against models not designed to be understood",
      "Training with interpretability as a constraint could produce inherently understandable systems",
      "Makes opacity a choice rather than a given"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Prohibitive cost (billions); unknown whether interpretability constraints preserve capability",
    "whoWorking": "- Not specified (researchers have considered but not pursued due to cost)",
    "sources": [
      {
        "text": "Peregrine 2025 #41: Interpretable Frontier Models",
        "url": "../sources/peregrine-2025/interventions/peregrine-041.md"
      }
    ]
  },
  {
    "filename": "large-group-deliberation-systems",
    "title": "Large group deliberation systems",
    "tag": "Society",
    "whatItIs": "Develop systems for collective decision-making involving large groups of people to maintain distributed power as AI advances rapidly. These tools should be tested extensively with real humans to iteratively improve understanding of effective processes, enabling people to be engaged quickly and efficiently when important decisions arise rather than defaulting to concentrated power. The goal is creating scalable democratic input mechanisms that can operate on relevant timescales.",
    "whyItMatters": [
      "Important AI governance decisions may need to be made quickly but should reflect broad input",
      "Without effective large-group deliberation, the choice is between slow democratic processes and fast autocratic decisions",
      "Scalable deliberation could enable democratic legitimacy without sacrificing response speed"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Balancing speed with quality of deliberation; ensuring representative participation; preventing manipulation; integrating outputs into actual decision processes",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #146: Large Group Deliberation Systems",
        "url": "../sources/peregrine-2025/interventions/peregrine-146.md"
      }
    ]
  },
  {
    "filename": "long-term-safety-funding",
    "title": "Long-term safety org funding",
    "tag": "Society",
    "whatItIs": "Structured three-year funding schedules for proven alignment research organizations to signal long-term commitment and enable strategic growth. Example targets for substantially increased funding include Transluce, Apollo, and Redwood.",
    "whyItMatters": [
      "Safety organizations struggle with uncertain, short-term funding that prevents strategic planning",
      "Long-term commitments allow multi-year hires, ambitious projects, and institutional capacity building",
      "Reduces scrambling for next grant that distracts from research"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Funder commitment to multi-year schedules",
    "whoWorking": "- **Transluce, Apollo, Redwood**: Potential recipients",
    "sources": [
      {
        "text": "Peregrine 2025 #32: Reward Track Record in Security & Safety Organizations",
        "url": "../sources/peregrine-2025/interventions/peregrine-032.md"
      }
    ]
  },
  {
    "filename": "loss-of-control-threat-modeling",
    "title": "Loss of control threat modeling",
    "tag": "Security",
    "whatItIs": "Extensive threat-modeling experiments for loss-of-control scenarios, including honeypot experiments in realistic sandboxes with AI systems interacting with each other. Simulations reveal whether systems develop concerning behaviors like cooperation for deception or capability to jailbreak other systems.",
    "whyItMatters": [
      "We don't know what loss of control would actually look like or how it would unfold",
      "Concrete threat modeling identifies specific failure modes and tests proposed defenses",
      "May reveal unexpected risks like emergent cooperation between AI systems against oversight"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #27: Loss of Control Threat Modeling",
        "url": "../sources/peregrine-2025/interventions/peregrine-027.md"
      }
    ]
  },
  {
    "filename": "low-stakes-alignment-settings",
    "title": "Low-stakes alignment settings",
    "tag": "Security",
    "whatItIs": "Environments requiring multiple consecutive malicious actions to produce harmful outcomes. Uses solid sandboxing around agent environments so no bad outcome occurs unless the sandbox is deliberately breached. Creates multiple layers of protection where oversight has several chances to detect problematic behavior rather than relying on perfect first-action detection.",
    "whyItMatters": [
      "Perfect alignment may be unachievable, but defense-in-depth provides practical safety",
      "Multiple intervention points mean imperfect oversight can still catch threats",
      "Converts single-point-of-failure systems into resilient chains"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #20: Low-Stakes Alignment Settings",
        "url": "../sources/peregrine-2025/interventions/peregrine-020.md"
      }
    ]
  },
  {
    "filename": "misalignment-definitions",
    "title": "Misalignment definitions and measures",
    "tag": "Science",
    "whatItIs": "Sharpen formal understanding of how AI systems pursue goals in ways that undermine user intent. Includes refining frameworks for specification gaming and goal drift, clarifying what counts as alignment failure, and developing practical metrics for detecting loophole exploitation. Creates shared taxonomies that can be operationalized.",
    "whyItMatters": [
      "Without clear definitions, field can't measure progress or agree on whether systems are aligned",
      "Shared taxonomies enable coordination across research groups",
      "Regulatory bodies need clear standards; practical metrics allow tracking whether alignment improves with scale"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #25: Misalignment Definitions and Measures",
        "url": "../sources/peregrine-2025/interventions/peregrine-025.md"
      }
    ]
  },
  {
    "filename": "model-distillation-limits",
    "title": "Model distillation limits",
    "tag": "Science",
    "whatItIs": "Research into theoretical limits of AI capabilities, specifically whether there are fundamental physics-based constraints on model distillation. Investigates whether small, easily-diffused models can always capture capabilities of larger systems, or if certain capabilities inherently require large-scale infrastructure. Also analyzes hardware bottlenecks (GPU interconnect, test-time compute) to predict capability jumps.",
    "whyItMatters": [
      "If dangerous capabilities can always be distilled into small shareable models, containment strategies are fundamentally limited",
      "If physics constrains distillation, some capabilities require infrastructure that can be monitored",
      "Understanding these limits determines which governance strategies are even possible"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #24: Limits of Model Distillation",
        "url": "../sources/peregrine-2025/interventions/peregrine-024.md"
      }
    ]
  },
  {
    "filename": "model-drift-monitoring",
    "title": "Open-Source Model Drift Monitoring",
    "tag": "Security",
    "whatItIs": "Open-source tools to detect, measure, and address gradual changes in model behavior over time. Establishes standard metrics and benchmarks for identifying when models deviate from intended function or exhibit concerning behaviors. Provides transparent, vendor-neutral assessment of model drift across different AI systems.",
    "whyItMatters": [
      "Helps organizations maintain oversight of AI deployments and intervene before drift creates security vulnerabilities",
      "Vendor-neutral tools enable independent verification",
      "Open-source approach allows community improvement and trust"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #79: Open-Source AI Drift Monitoring",
        "url": "../sources/peregrine-2025/interventions/peregrine-079.md"
      }
    ]
  },
  {
    "filename": "model-weight-security",
    "title": "Model weight security",
    "tag": "Security",
    "whatItIs": "Comprehensive security measures protecting frontier AI model weights from theft, leakage, or unauthorized access. RAND's 2024 framework established five security levels (SL1-SL5): SL1 deters hobby hackers, SL5 secures against elite nation-state attackers with measures comparable to nuclear weapons protection. Key measures include: centralizing weights on access-controlled systems, insider threat programs, API hardening against exfiltration, differential privacy noise injection, confidential computing (trusted execution environments), and advanced red-teaming.",
    "whyItMatters": [
      "Model weights are a single point of failure: one leak bypasses years of safety work",
      "Adversaries with leaked weights can fine-tune away guardrails within hours",
      "A stolen frontier model would be worth hundreds of millions on the black market",
      "Recent incidents: OpenAI breach 2023 (internal communications), DeepSeek database backdoor January 2025, xAI internal LLM leak July 2025"
    ],
    "currentState": "- **Status**: Pilot (uneven implementation)\n- **Industry adoption**: 12 companies have published frontier AI safety policies (Anthropic, OpenAI, Google DeepMind, Meta, xAI, etc.) with weight security commitments\n- **Gap**: Only 20% of organizations specifically test for model theft despite 97% claiming AI security priority (Hidden Layer 2024)\n- **Technical challenge**: Weights are decrypted and vulnerable during use; confidential computing (TEEs) addresses this but adds complexity\n- **Current benchmarks**: Google's Gemini 2.5 claims \"RAND SL2\" alignment",
    "whoWorking": "- **RAND Corporation**: Security framework and recommendations\n- **Major labs**: Anthropic, OpenAI, Google DeepMind implementing varying security levels\n- **METR**: Tracking common elements of frontier AI safety policies\n- **UK AI Security Institute**: Evaluating lab security practices",
    "sources": [
      {
        "text": "Peregrine 2025 #159: Model Weight Security",
        "url": "../sources/peregrine-2025/interventions/peregrine-159.md"
      },
      {
        "text": "Atlas AI Resilience Gap Map: AI Systems as Targets",
        "url": "../sources/atlas-ai-resilience/proposals/ai-systems-as-targets.md"
      }
    ]
  },
  {
    "filename": "multi-agent-safety-systems",
    "title": "Multi-agent safety systems",
    "tag": "Security",
    "whatItIs": "Infrastructure for monitoring, simulating, and governing interactions between multiple AI agents. Two complementary components:\n\n**Multi-agent monitoring**: Systems that actively watch deployments and interactions between AI systems, identifying potentially harmful feedback loops or emergent goals before they become problematic. The field is underdeveloped; agentic interactions are either poorly captured or not well-conceptualized within existing frameworks.\n\n**Multi-agent safety protocols**: Protocols to prevent emergent cooperation between agents to circumvent safety constraints. Establishes methods for AI systems to verify trustworthiness of other agents, report problematic behavior, and implement collective safety measures across distributed systems. Includes tracking trust relationships and identifying bad actors.",
    "whyItMatters": [
      "Financial transactions, supply chains, and information exchange increasingly mediated by interconnected AI systems with minimal human oversight",
      "As AI systems increasingly interact with each other, emergent behaviors could arise that weren't present in individual systems",
      "High-speed AI-to-AI interactions outpace human monitoring capacity",
      "Without monitoring, multi-agent dynamics remain invisible until they cause harm"
    ],
    "currentState": "- **Status**: Research (rapidly developing field)\n- **Key risks identified (2025)**:\n  - Emergent collusion via shared state/privileges rather than explicit planning\n  - Steganographic communication enabling secret coordination channels\n  - Protocol-level attacks: message tampering, role spoofing, protocol exploitation\n  - \"Patchwork AGI\" hypothesis: near-AGI capability emerging from coordinated sub-AGI agents before individual AGI\n  - Single compromised agent can trigger harm across entire multi-agent system\n- **Research directions**:\n  - High-fidelity simulation environments for testing coordination protocols before deployment\n  - Distributional AGI safety frameworks for composite systems\n  - Architectural approaches embedding risk management into multi-agent design\n  - Trust verification protocols between agents\n- **Bottlenecks**: Tracing failures across peer interactions is complex; human oversight speed limits; conceptual frameworks still developing",
    "whoWorking": "- **Cooperative AI Foundation**: Major technical report on multi-agent risks (50+ researchers from DeepMind, Anthropic, CMU, Harvard)\n- **World Economic Forum**: Multi-agent safety frameworks\n- **Anthropic**: Constitutional AI research for embedding safety constraints in agents\n- **Academic groups**: Emergent behavior and multi-agent security research (multiple arxiv papers)",
    "sources": [
      {
        "text": "Peregrine 2025 #78: Monitor Complex Agent Interactions",
        "url": "../sources/peregrine-2025/interventions/peregrine-078.md"
      },
      {
        "text": "Peregrine 2025 #57: Multi-Agent Interaction Framework",
        "url": "../sources/peregrine-2025/interventions/peregrine-057.md"
      }
    ]
  },
  {
    "filename": "nucleic-acid-observatory",
    "title": "Nucleic acid observatory",
    "tag": "Security",
    "whatItIs": "Metagenomic sequencing of environmental samples (wastewater, air, nasal swabs) to detect novel pathogens before clinical cases appear. Unlike targeted PCR tests that look for known pathogens, metagenomic sequencing reads all genetic material in a sample—enabling detection of previously unknown or engineered pathogens. The NAO operates at scale: 270 billion read pairs sequenced in Q1 2025 alone (more than all pre-2025 sequencing combined), analyzing each billion reads for under $10 using AWS Batch.",
    "whyItMatters": [
      "Detects novel pathogens before clinical surveillance can identify them",
      "Works against unknown threats including engineered pathogens that evade targeted tests",
      "Provides critical early warning time for response measures—detection in wastewater can precede clinical cases by days to weeks"
    ],
    "currentState": "- **Status**: Pilot (operational detection systems) with government scale-up planned\n- **Scale**: CASPER system processes samples from 20 cities (as of Oct 2025), generating billions of reads weekly—the largest untargeted wastewater MGS dataset ever assembled\n- **Detection sensitivity**: Estimated to detect viral pathogens shedding like flu/SARS-CoV-2 before 1-3% of monitored population infected\n- **Government adoption**: President's FY 2026 Budget proposes $52M to CDC for \"Biothreat Radar\" based on NAO findings—would enable detection before 12 in 100,000 Americans infected\n- **Recent progress (2024-2025)**:\n  - Genetic engineering detection pipeline operational—detecting HIV-based viral vectors\n  - Reference-based growth detection moving to production (second operational method)\n  - 270B read pairs sequenced in Q1 2025 (more than all pre-2025 sequencing combined)\n  - Nextflow-based pipeline processes 1B reads for <$10\n- **Bottlenecks**: False positive management; international coordination for global deployment; integrating with public health response systems",
    "whoWorking": "- **Nucleic Acid Observatory (SecureBio)**: Primary operator; 9 FTEs across Near-Term Detection and Robust Detection teams; $3.4M Open Philanthropy grant for scale-up\n- **CDC**: FY 2026 budget request for $52M Biothreat Radar program based on NAO methods\n- **Broad Clinical Labs**: Sequencing partnership using NovaSeq X+ sequencers\n- **University of Missouri (Marc Johnson lab)**: Partner sequencing facility\n- **University of Miami (Helena Solo-Gabriele lab)**: South Florida sewershed\n- **PHC Global**: ANTI-DOTE contract for military/marine wastewater",
    "sources": [
      {
        "text": "Peregrine 2025 #137: Nucleic Acid Observatory",
        "url": "../sources/peregrine-2025/interventions/peregrine-137.md"
      },
      {
        "text": "Apollo Program for Biodefense: Detection",
        "url": "../sources/apollo-biodefense/proposals/detection.md"
      },
      {
        "text": "Concrete Biosecurity Projects: Early Detection Center",
        "url": "../sources/concrete-biosecurity-ea/proposals/early-detection-center.md"
      },
      {
        "text": "Biosecurity Engineers: Biomonitoring Hardware",
        "url": "../sources/biosecurity-engineers/proposals/biomonitoring-hardware.md"
      },
      {
        "text": "Atlas AI Resilience Gap Map: Pathogen Surveillance",
        "url": "../sources/atlas-ai-resilience/proposals/pathogen-surveillance-gap.md"
      }
    ]
  },
  {
    "filename": "opponent-shaping-research",
    "title": "Opponent shaping for AI systems",
    "tag": "Science",
    "whatItIs": "AI systems designed to shape other agents' learning and behavior toward mutually beneficial outcomes, rather than just optimizing against their strategies. Decentralized approach that de-escalates conflicts without requiring control over other agents' design or universal rules.",
    "whyItMatters": [
      "Optimizing against opponents creates arms races and destructive equilibria",
      "Offers path to stable, cooperative outcomes without centralized control",
      "Doesn't require universal agreement on rules"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Theoretical foundations; verification that shaping produces intended outcomes; deployment without bad actors exploiting it",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #202: Opponent Shaping",
        "url": "../sources/peregrine-2025/interventions/peregrine-202.md"
      }
    ]
  },
  {
    "filename": "pandemic-protective-equipment",
    "title": "Pandemic protective equipment",
    "tag": "Security",
    "whatItIs": "Next-generation personal protective equipment designed for mass stockpiling and rapid population-wide distribution during pandemics. Key design requirements: low manufacturing cost for billion-unit stockpiles, reliable protection without expert fitting, and multi-year shelf life for strategic reserves. Protection levels should approach professional-grade (better than N95) while remaining usable by untrained general population.",
    "whyItMatters": [
      "Current high-protection PPE (PAPRs, N95+, hazmat) requires expert fitting and training",
      "Simple PPE (cloth masks, surgical masks) provides limited protection",
      "Gap exists for \"good enough\" protection that works at population scale during emergencies",
      "Engineered pandemics may require immediate population-wide protection before vaccines available"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Manufacturing cost reduction; shelf-life optimization; usability testing with non-expert populations; regulatory pathway for emergency stockpiles",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Concrete Biosecurity Projects: Super PPE",
        "url": "../sources/concrete-biosecurity-ea/proposals/super-ppe.md"
      },
      {
        "text": "Biosecurity Engineers: Next-Gen PPE",
        "url": "../sources/biosecurity-engineers/proposals/next-gen-ppe.md"
      }
    ]
  },
  {
    "filename": "pandemic-resilient-refuges",
    "title": "Pandemic-resilient refuges",
    "tag": "Security",
    "whatItIs": "Hardened facilities (\"Mars on Earth\") designed to preserve civilization-rebuilding capacity through extreme pandemics. Key capabilities beyond basic survival shelters: (1) pathogen testing at entry to maintain sterile internal environment, (2) self-contained life support allowing indefinite isolation, and (3) countermeasure development capacity—labs and equipment to develop vaccines/treatments from inside the refuge.",
    "whyItMatters": [
      "Most biosecurity measures aim to prevent catastrophe; refuges assume prevention might fail",
      "Pure survival bunkers delay extinction but don't enable recovery",
      "Refuges with countermeasure capacity can eventually emerge and \"re-seed\" the population",
      "Provides last-line defense against extinction-level engineered pandemics"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Massive capital cost; unclear governance (who gets in?); maintaining technical capacity for countermeasure development; avoiding cult/bunker mentality",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Concrete Biosecurity Projects: Refuges",
        "url": "../sources/concrete-biosecurity-ea/proposals/refuges.md"
      }
    ]
  },
  {
    "filename": "personnel-security-screening",
    "title": "Personnel security screening for AI access",
    "tag": "Security",
    "whatItIs": "Mandated screening and monitoring for personnel with access to powerful AI systems, analogous to security clearances in intelligence or cybersecurity. Primary goal is eliminating worst outcomes from geopolitical adversaries or mentally unstable individuals accessing AGI-level systems.",
    "whyItMatters": [
      "Insider threats are hardest to defend against",
      "Screening reduces probability of bad actor access",
      "Trades off against talent acquisition and organizational culture"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Defining screening criteria; talent pipeline effects; privacy concerns; implementation costs",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #200: Personnel Reliability Programs",
        "url": "../sources/peregrine-2025/interventions/peregrine-200.md"
      }
    ]
  },
  {
    "filename": "privacy-preserving-verification",
    "title": "Privacy-Preserving Verification Systems",
    "tag": "Security",
    "whatItIs": "Surveillance, monitoring, and verification technologies that track hardware supply chains, chip deployments, power grid construction, and computing infrastructure globally. Combines OSINT and privacy-preserving cryptographic methods to verify what activities occur inside data centers. Information distributed through diplomatic channels rather than complete public disclosure to avoid triggering adversarial reactions.",
    "whyItMatters": [
      "Provides transparency to prevent miscalculations between major powers",
      "Prevents countries from accelerating AI development out of fear others are doing the same",
      "Collects critical intelligence about AI development capacity without revealing sources"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #73: Privacy-Preserving Verification Systems",
        "url": "../sources/peregrine-2025/interventions/peregrine-073.md"
      }
    ]
  },
  {
    "filename": "private-industry-security",
    "title": "Private Industry Security Organization",
    "tag": "Society",
    "whatItIs": "A privately-funded alternative to the chronically underfunded Bureau of Industry and Security (BIS, $15M budget) focusing on GPU distribution oversight and AI research automation monitoring. The entity would prototype advanced data center surveillance, implement GPU tracking systems, and establish protocols for questioning companies about research automation capabilities. Design principle: create \"proposals that are hard and awkward to oppose\" to maximize political viability.",
    "whyItMatters": [
      "Current government oversight capacity is dramatically underfunded relative to the challenge",
      "Private alternative can move faster and be better resourced than government agencies",
      "Fills critical gap in enforcement and monitoring infrastructure"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whoWorking": "- **Bureau of Industry and Security**: Mentioned as underfunded incumbent",
    "sources": [
      {
        "text": "Peregrine 2025 #69: Industry Security",
        "url": "../sources/peregrine-2025/interventions/peregrine-069.md"
      }
    ]
  },
  {
    "filename": "problem-mapping",
    "title": "Problem mapping",
    "tag": "Science",
    "whatItIs": "Comprehensive list of critical AI safety problems that, if solved, would result in aligned AI systems. Goes beyond lists like Open Philanthropy's RFP by presenting an overarching framework justifying why solving the listed problems would lead to alignment. Systematically identifies research areas that collectively guarantee safety.",
    "whyItMatters": [
      "Current safety research is fragmented: problems studied without knowing if solving them produces alignment",
      "Comprehensive map enables strategic resource allocation and identifies neglected areas",
      "Provides confidence that solving the right problems actually works"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whoWorking": "- **Open Philanthropy/Coefficient Giving**: Partial (RFP list without overarching framework)",
    "sources": [
      {
        "text": "Peregrine 2025 #30: Problem Mapping",
        "url": "../sources/peregrine-2025/interventions/peregrine-030.md"
      }
    ]
  },
  {
    "filename": "public-model-audits",
    "title": "Public Model Audits",
    "tag": "Security",
    "whatItIs": "Independent organizations conduct comprehensive public audits of open-weight models using substantial compute resources. The audits demonstrate in public what thorough model analysis should look like, creating templates for what information should be available about any deployed AI system. Public process enables community feedback and establishes consensus on best practices.",
    "whyItMatters": [
      "Creates public precedents for robust evaluation that private labs cannot easily ignore",
      "Raises the floor for responsible disclosure without requiring immediate regulation",
      "Establishes community-vetted standards for model transparency"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #59: Public Audits",
        "url": "../sources/peregrine-2025/interventions/peregrine-059.md"
      }
    ]
  },
  {
    "filename": "risk-characterization-evidence",
    "title": "Risk characterization evidence",
    "tag": "Science",
    "whatItIs": "Robust research characterizing AI risks empirically with high validity, focused on evidence usable for policy decisions. Creates benchmarks establishing clear quantitative thresholds for risks. Provides governments with clear criteria for when to intervene, producing credible empirical evidence enabling appropriate action when AI capabilities increase.",
    "whyItMatters": [
      "Policy decisions require evidence, not speculation",
      "Current risk claims range from dismissive to apocalyptic with little empirical grounding",
      "Validated metrics give policymakers actionable intervention criteria rather than choosing between competing narratives"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #48: Characterizing AI Risks with Evidence",
        "url": "../sources/peregrine-2025/interventions/peregrine-048.md"
      }
    ]
  },
  {
    "filename": "risk-model-evaluation",
    "title": "Risk-Model Evaluation",
    "tag": "Science",
    "whatItIs": "Evaluation frameworks that test AI risk assessment systems by creating methodologies to identify, classify, and prioritize potential AI mistakes based on their consequences. The evaluations measure how well risk models detect both obvious and subtle errors in high-stakes contexts, quantifying false positive and false negative rates to enable calibrated trade-offs between intervention frequency and risk exposure.",
    "whyItMatters": [
      "Creates foundation for reliable triage systems that focus human attention on highest-priority risks",
      "Replaces ad-hoc risk assessment with systematic, quantifiable approaches",
      "Enables resource allocation based on measured detection accuracy rather than intuition"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #52: Risk-Model Evaluation",
        "url": "../sources/peregrine-2025/interventions/peregrine-052.md"
      }
    ]
  },
  {
    "filename": "rsp-implementation",
    "title": "RSP Implementation",
    "tag": "Society",
    "whatItIs": "Transforming existing Responsible Scaling Policies (RSPs) from theoretical frameworks into actionable implementation plans. Develops concrete, tested \"if-then commitments\" with ready-to-deploy protocols for when safety boundaries are approached. Builds social mechanisms including public accountability and transparency requirements to ensure compliance. Significant focus on creating industry-wide norms and social pressure that makes non-compliance reputationally costly.",
    "whyItMatters": [
      "Current RSPs are theoretical commitments without tested implementation",
      "Without concrete protocols and enforcement mechanisms, commitments may not be honored under pressure",
      "Social pressure mechanisms complement technical and legal enforcement"
    ],
    "currentState": "- **Status**: Pilot/Deployed (varies by lab)\n- **Recent developments (2024-2025)**:\n  - Anthropic: October 2024 RSP update introduced safety case methodology, new capability thresholds, board approval requirements, Long Term Benefit Trust consultation\n  - OpenAI: \"Preparedness Framework\" approach with capability-based risk assessment\n  - Google DeepMind: Frontier Safety Framework (2024), anticipates deceptive alignment in near-future models\n  - Microsoft: Frontier Governance Framework (2025)\n  - Meta: Frontier AI Framework (2025), focused on catastrophic misuse by human actors\n  - Amazon: Frontier Model Safety Framework (2025)\n- **Convergence**: All major labs now use capability-based thresholds classifying models by dangerous capabilities\n- **Divergence**: DeepMind/Anthropic anticipate deceptive alignment; Meta/Amazon focus on misuse by humans\n- **Bottlenecks**: Implementation varies across labs; commitments untested under real pressure; \"promissory note\" criticism",
    "whoWorking": "- **Anthropic**: Pioneered RSPs (September 2023), most detailed framework with AI Safety Levels (ASL)\n- **OpenAI**: Preparedness Framework\n- **Google DeepMind**: Frontier Safety Framework\n- **Microsoft, Meta, Amazon**: Published frameworks in 2025\n- **AI Lab Watch**: Tracking and comparing RSP commitments across labs",
    "sources": [
      {
        "text": "Peregrine 2025 #92: Enhanced Responsible Scaling Implementation",
        "url": "../sources/peregrine-2025/interventions/peregrine-092.md"
      }
    ]
  },
  {
    "filename": "safety-benchmarks",
    "title": "Safety benchmark development",
    "tag": "Science",
    "whatItIs": "Comprehensive high-effort benchmarks for evaluating AI safety and security. Many safety benchmarks have been defunded or discontinued. Develops rigorous, transparent methodologies for assessing various dimensions of AI safety. Aims to provide common language for discussing safety progress across different models and organizations.",
    "whyItMatters": [
      "Without agreed benchmarks, no way to measure safety progress or compare approaches",
      "Labs can claim models are \"safe\" without standard meaning",
      "High-quality benchmarks create accountability with objective measures enabling comparison across models and time"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Previous benchmarks defunded/discontinued; skepticism about benchmark validity for actual safety",
    "whoWorking": "- Not specified (many safety benchmarks discontinued)",
    "sources": [
      {
        "text": "Peregrine 2025 #50: Safety Benchmark Development",
        "url": "../sources/peregrine-2025/interventions/peregrine-050.md"
      }
    ]
  },
  {
    "filename": "safety-research-synthesis",
    "title": "Safety research synthesis",
    "tag": "Science",
    "whatItIs": "Better information-sharing mechanisms for topics where experts hold dramatically different assessments (e.g., mechanistic interpretability). Involves publishing analyses that clarify current cruxes in technical debates and generating evidence to resolve them rather than letting uncertainty persist indefinitely. Pays down research debt from fragmented discussions with little consensus-building.",
    "whyItMatters": [
      "Key disagreements persist without resolution; experts can't evaluate each other's claims",
      "Resources wasted on parallel efforts that could be coordinated",
      "Systematic synthesis identifies which uncertainties actually matter and creates shared understanding"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #28: Synthesis, Consensus, and Paying Down Research Debt",
        "url": "../sources/peregrine-2025/interventions/peregrine-028.md"
      }
    ]
  },
  {
    "filename": "safety-technique-meta-research",
    "title": "Safety technique meta-research",
    "tag": "Science",
    "whatItIs": "Research evaluating and validating effectiveness of different AI safety approaches. Multiple independent actors assess the same models using different techniques, comparing results to determine reliability and blind spots. Moves alignment from piecemeal theoretical approaches toward empirical safety science with repeatable, testable methodologies that scale with capabilities.",
    "whyItMatters": [
      "We don't actually know which safety techniques work",
      "Different groups use different methods and report different results with no systematic comparison",
      "Meta-research identifies which techniques are actually effective rather than just theoretically promising"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Coordination across independent research groups",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #47: Meta-Research on Safety Techniques",
        "url": "../sources/peregrine-2025/interventions/peregrine-047.md"
      }
    ]
  },
  {
    "filename": "sandbagging-detection",
    "title": "Evaluation sandbagging detection",
    "tag": "Security",
    "whatItIs": "Methodologies and tools to identify when AI systems deliberately underperform on safety evaluations for strategic purposes. Sandbagging can be developer-initiated (explicit intervention) or AI-initiated (unintended model behavior). Key techniques:\n\n- **Password-locking detection**: Models can be fine-tuned to hide capabilities unless given a password—generalizes to benchmarks like WMDP\n- **Noise injection**: Inject noise into model weights; sandbagging models show anomalous performance improvements (disrupts underperformance mechanisms while core capabilities remain intact)\n- **On-distribution finetuning**: Single-sample finetuning on evaluation tasks can elicit hidden capabilities, but creates false positive risk\n- **Evaluation-awareness monitoring**: Check if models reason about being evaluated (precursor to strategic sabotage)",
    "whyItMatters": [
      "If models can strategically underperform on safety evaluations, the entire evaluation paradigm breaks",
      "A model might hide dangerous capabilities during testing and reveal them in deployment",
      "Detecting sandbagging is essential for evaluations to remain meaningful",
      "Current capabilities: models can already reason about being evaluated"
    ],
    "currentState": "- **Status**: Research (active development)\n- **Recent findings (2025)**:\n  - Noise injection classifier achieves 100% accuracy distinguishing sandbagging vs. standard configurations (llama70B, deepseek-7b, mistral7B)\n  - METR GPT-5 evaluation (August 2025): No evidence of sandbagging, but model can reason about being evaluated—\"should be closely monitored\"\n  - Auditing game (December 2025): Detection remains difficult without training access; only on-distribution finetuning reliably removes sandbagging\n  - Demonstrations generalize well to removing sandbagging across distribution shifts\n- **Bottlenecks**: Detection without training access unreliable (1/5 models correctly classified); transcript analysis and prompting ineffective against sophisticated sandbagging",
    "whoWorking": "- **METR**: GPT-5 sandbagging evaluation, ongoing research\n- **Redwood Research**: Countermeasures research, password-locked model experiments\n- **Academic groups**: Noise injection detection methods, auditing games",
    "sources": [
      {
        "text": "Peregrine 2025 #44: Evaluation Sandbagging Detection",
        "url": "../sources/peregrine-2025/interventions/peregrine-044.md"
      }
    ]
  },
  {
    "filename": "secure-ai-facilities",
    "title": "Secure AI facilities",
    "tag": "Security",
    "whatItIs": "High-security infrastructure for frontier AI development, testing, and government oversight. Two complementary components:\n\n**Frontier-grade data centers (SL4-SL5)**: Data centers meeting SL4-SL5 security standards for frontier AI development. SL4 provides SCIF-level security (Secure Compartmentalized Information Facilities) with confidential computing and Hardware Security Modules. SL5 goes further, designed to resist \"top-priority operations by the world's most capable nation-state actors.\" Pilots are needed to develop practical SL5 protocols while reducing operational costs.\n\n**Government AI facilities**: Secure government data centers capable of hosting model weights and other sensitive AI assets. Serve as repositories for models deemed too dangerous for widespread distribution, or as platforms for government auditing and independent safety testing. Access protocols could be tied to government procurement contracts, creating market-based incentives for compliance with safety metrics.",
    "whyItMatters": [
      "Frontier AI systems may become nation-state targets requiring maximum security",
      "No secure government infrastructure currently exists for holding dangerous AI models",
      "Governments cannot independently verify safety claims without their own testing facilities",
      "Lab automation feedback loops become dangerous as AI improves at AI research",
      "Prevents catastrophic model/weight leaks and potential containment failures"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: High operational costs for SL5; unclear what SL5 means practically for AI systems; scaling SCIF-level security to frontier AI energy requirements; large capital expenditure; security clearance and classification issues; potential resistance from labs concerned about IP protection",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #61: Secure Frontier Data Centers",
        "url": "../sources/peregrine-2025/interventions/peregrine-061.md"
      },
      {
        "text": "Peregrine 2025 #163: SL5 Data Center Pilots",
        "url": "../sources/peregrine-2025/interventions/peregrine-163.md"
      },
      {
        "text": "Peregrine 2025 #112: Government Data Centers",
        "url": "../sources/peregrine-2025/interventions/peregrine-112.md"
      },
      {
        "text": "Atlas AI Resilience Gap Map: AI Systems as Targets",
        "url": "../sources/atlas-ai-resilience/proposals/ai-systems-as-targets.md"
      }
    ]
  },
  {
    "filename": "self-other-overlap-training",
    "title": "Self-other overlap training",
    "tag": "Science",
    "whatItIs": "Fine-tuning technique that reduces AI deception by minimizing the difference between how models internally represent self-referencing scenarios vs. other-referencing scenarios. During training, a loss function penalizes divergence between the model's processing of \"what I would do\" vs. \"what they would do\" contexts. Inspired by cognitive neuroscience research on empathy, where self-other distinction is necessary for deception.",
    "whyItMatters": [
      "Deception requires self-other distinctions not needed for task performance",
      "Selecting against these distinctions removes a prerequisite for deceptive behavior",
      "Complements behavioral alignment (RLHF) by targeting internal representations"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Testing in adversarial settings (sleeper agents); integration with other alignment methods",
    "whoWorking": "- **AE Studio**: Experiments showing deceptive responses dropped from 73.6% to 17.2% on 7B models, and from 100% to 2.7% on 78B models with minimal capability impact",
    "sources": [
      {
        "text": "Peregrine 2025 #12: Cooperative Alignment",
        "url": "../sources/peregrine-2025/interventions/peregrine-012.md"
      }
    ]
  },
  {
    "filename": "standardized-model-interfaces",
    "title": "Standardized AI model access interfaces",
    "tag": "Science",
    "whatItIs": "Standardized interfaces for the AI ecosystem (like USB-C universality), enabling consistent interaction with all models regardless of origin. Currently each lab uses different interfaces, forcing auditors and researchers to negotiate separate access arrangements with each company, potentially delaying critical safety work by months. Could be implemented \"within a year.\"",
    "whyItMatters": [
      "Fragmented interfaces slow safety research and auditing",
      "Duplicated work, delayed assessments, incompatible tools",
      "Standardization enables ecosystem to move at speed of fastest developer"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Lab coordination; defining standard that accommodates different architectures; governance of standard",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #192: Access Standards",
        "url": "../sources/peregrine-2025/interventions/peregrine-192.md"
      }
    ]
  },
  {
    "filename": "strategic-ai-liability-litigation",
    "title": "Strategic AI liability litigation",
    "tag": "Society",
    "whatItIs": "Preemptive lawsuits against AI organizations deemed negligent on risk management, potentially recruiting high-profile figures (Elon Musk mentioned). Strategic lawsuits across multiple jurisdictions to establish precedents and create legal clarity around AI risk management requirements.",
    "whyItMatters": [
      "Legal uncertainty allows labs to ignore risks",
      "Precedents clarify liability and create deterrence",
      "Provides legal hooks for future regulatory action"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Legal standing; identifying viable cases; high-profile plaintiff recruitment",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #188: Legal Clarity Initiatives",
        "url": "../sources/peregrine-2025/interventions/peregrine-188.md"
      }
    ]
  },
  {
    "filename": "subsidized-security-tools",
    "title": "Government-subsidized cybersecurity AI tools",
    "tag": "Security",
    "whatItIs": "Government procurement and distribution of advanced AI-powered bug-finding tools to critical infrastructure globally, prioritizing financial institutions (to prevent AI systems from acquiring funds). Treats cybersecurity as a public good, funding security improvements in countries that otherwise lack resources.",
    "whyItMatters": [
      "Security capabilities concentrate in wealthy organizations/nations",
      "Weak links become entry points for AI-enabled attacks",
      "Banks and financial institutions are priority targets (AI acquiring funds enables further operations)"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Requires government funding; international coordination for global deployment",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #166: Bug-Finding Democratization",
        "url": "../sources/peregrine-2025/interventions/peregrine-166.md"
      }
    ]
  },
  {
    "filename": "tamper-resistant-rlhf",
    "title": "Tamper-resistant RLHF",
    "tag": "Security",
    "whatItIs": "Techniques to make RLHF alignment resistant to subsequent modification or reversal. \"Irreversible RLHF\" would prevent actors from removing safety guardrails through additional fine-tuning after deployment. Research must balance benefits of sticky values against risks of permanently embedding flawed alignment. Open-source models provide testing grounds without requiring proprietary access.",
    "whyItMatters": [
      "Current safety guardrails can be stripped via fine-tuning, enabling malicious actors to weaponize aligned models",
      "Alignment resistant to removal would maintain safety properties even after model release or modification",
      "Addresses open-weight model safety problem"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #7: Tamper-Resistant RLHF",
        "url": "../sources/peregrine-2025/interventions/peregrine-007.md"
      }
    ]
  },
  {
    "filename": "task-decomposition",
    "title": "Task decomposition for dangerous capabilities",
    "tag": "Security",
    "whatItIs": "Architectural approach splitting dangerous AI capabilities across multiple isolated systems or users, preventing any single agent from having complete capability. Example: a bioweapon design task might be decomposed so one system handles molecular modeling, another handles synthesis planning, and neither has access to the other's outputs—only a human reviewer can combine them. Interfaces are designed so compartments can contribute to beneficial tasks while preventing end-to-end execution of catastrophic operations.",
    "whyItMatters": [
      "Provides defense-in-depth at architecture level, not just alignment level",
      "Even compromised individual systems cannot independently cause worst outcomes",
      "Reduces single points of failure for high-stakes applications"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Designing decompositions that actually prevent recombination; performance overhead of compartmentalization",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #3: Task Decomposition",
        "url": "../sources/peregrine-2025/interventions/peregrine-003.md"
      }
    ]
  },
  {
    "filename": "training-data-anti-poisoning",
    "title": "Training data anti-poisoning techniques",
    "tag": "Security",
    "whatItIs": "Detection systems for suspicious data patterns, resilient training methodologies that maintain performance when portions of data are compromised, and recovery protocols for systems exposed to poisoned inputs. Requires coordination across security teams currently working in isolation on similar problems.",
    "whyItMatters": [
      "Data poisoning can create backdoors or biased outputs that evade standard testing",
      "Compromised behavior may only manifest in specific contexts",
      "Protects integrity of entire training pipeline"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Multiple security teams working in isolation; detection methods lag behind attack sophistication",
    "whoWorking": "- Multiple security teams (working in isolation)",
    "sources": [
      {
        "text": "Peregrine 2025 #165: Anti-Poisoning Techniques",
        "url": "../sources/peregrine-2025/interventions/peregrine-165.md"
      }
    ]
  },
  {
    "filename": "training-data-attribution",
    "title": "Training data attribution",
    "tag": "Science",
    "whatItIs": "Methods to trace model behaviors back to specific training examples using techniques like influence functions and counterfactual simulation. Rather than treating the model as a black box shaped by post-training alignment, attribution lets you identify which training data caused particular tendencies and modify the source directly.",
    "whyItMatters": [
      "Enables root-cause fixes for alignment issues rather than patching symptoms with RLHF",
      "Provides verifiable evidence for alignment claims (useful for audits and regulation)",
      "Could reveal unexpected sources of problematic behaviors that post-training methods miss"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Scaling influence functions to frontier model size; processing and interpreting massive attribution results",
    "whoWorking": "- **Anthropic**: Leading work, though much remains unpublished",
    "sources": [
      {
        "text": "Peregrine 2025 #1: Training Data Attribution",
        "url": "../sources/peregrine-2025/interventions/peregrine-001.md"
      }
    ]
  },
  {
    "filename": "training-data-licensing",
    "title": "Training Data Licensing",
    "tag": "Society",
    "whatItIs": "A regulated market for high-quality, difficult-to-replicate training data (scientific papers, professional knowledge, etc.) that cannot yet be automated. Fairly compensates those whose data is used for AI training while providing nations and regulatory bodies leverage over AI development. Companies needing specialized data must comply with regulations to maintain access, creating \"natural incentives for companies to sign up\" to oversight mechanisms.",
    "whyItMatters": [
      "Creates economic incentives for compliance with governance frameworks by controlling access to valuable training data",
      "Provides leverage for regulators without requiring heavy enforcement",
      "Addresses fairness concerns about AI profiting from others' contributions"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #95: Training Data Licensing",
        "url": "../sources/peregrine-2025/interventions/peregrine-095.md"
      }
    ]
  },
  {
    "filename": "transmissible-vaccines",
    "title": "Transmissible vaccines",
    "tag": "Security",
    "whatItIs": "Develop self-spreading vaccines that transmit between hosts like normal viruses, providing population-level immunity without requiring individual vaccination. This physical security approach constrains biological threats at the material level rather than through software-based solutions. Requires funding for experts who take near-future AI capabilities seriously and investment in research that goes beyond current biological literature to address qualitatively different threats than anything seen before.",
    "whyItMatters": [
      "Software controls can be bypassed; physical constraints on biological materials are harder to circumvent",
      "Could provide defense against sophisticated AI-enabled biological attacks",
      "Addresses scenarios where traditional vaccination campaigns cannot keep pace with fast-spreading engineered pathogens"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Biosafety and regulatory challenges; ethical concerns about non-consensual biological intervention; technical difficulty of controlling spread dynamics; dual-use concerns",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #143: Physical Security Mechanisms",
        "url": "../sources/peregrine-2025/interventions/peregrine-143.md"
      }
    ]
  },
  {
    "filename": "transport-air-filtration",
    "title": "Transport air filtration",
    "tag": "Security",
    "whatItIs": "Apply indoor air quality principles to large vehicles like aircraft and cruise ships. Environmental controls to reduce pathogen transmission during travel. Includes HEPA filtration upgrades, increased air exchange rates, and potentially UV disinfection systems adapted for vehicle environments.",
    "whyItMatters": [
      "Travel is a key vector for pandemic spread",
      "Reducing transmission in aircraft could slow international spread of novel pathogens",
      "Enclosed vehicles create high-risk transmission environments similar to buildings",
      "Ships and cruise vessels have been pandemic amplifiers historically"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Weight and power constraints on aircraft; retrofit costs; coordination with vehicle manufacturers and operators; regulatory requirements across jurisdictions",
    "whoWorking": "- Not specified (aircraft manufacturers have existing HEPA systems; cruise industry has updated protocols post-COVID)",
    "sources": [
      {
        "text": "Biosecurity Engineers: Vehicle Air Filtration",
        "url": "../sources/biosecurity-engineers/proposals/vehicle-air-filtration.md"
      }
    ]
  },
  {
    "filename": "ultra-reliable-evaluation",
    "title": "Ultra-Reliable AI Evaluation",
    "tag": "Science",
    "whatItIs": "Engineering approaches and benchmarking methodologies designed to achieve \"five nines\" (99.999%) reliability rather than the 99% accuracy typical of current AI benchmarks. The methods efficiently identify edge cases where models fail catastrophically without requiring millions of test instances, using adversarial probing modeled on aircraft safety standards rather than Kaggle competitions.",
    "whyItMatters": [
      "Critical systems (medical, infrastructure, autonomous vehicles) require reliability guarantees far beyond current benchmarks",
      "Current evaluation methods miss rare but catastrophic failure modes",
      "Without efficient methods to find rare failures, deploying AI in high-stakes domains remains unsafe"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #55: Ultra-Reliable AI Evaluation",
        "url": "../sources/peregrine-2025/interventions/peregrine-055.md"
      }
    ]
  },
  {
    "filename": "uv-air-disinfection",
    "title": "UV air disinfection",
    "tag": "Security",
    "whatItIs": "Deploy ultraviolet light for continuous air disinfection in occupied spaces through two complementary approaches:\n\n**Far-UVC (222nm)**: Filtered krypton-chloride excimer lamps at 222nm wavelength. Unlike conventional 254nm UV-C (exposure limit: 6 mJ/cm²), Far-UVC has much higher safe exposure limits (ACGIH 2025: 161 mJ/cm² for eyes, 479 mJ/cm² for skin over 8-hour exposure) because the wavelength is absorbed by the outer dead layer of skin and tear film before reaching living cells. Can be directed throughout the room including at occupant level.\n\n**Upper-room UV (254nm)**: Ceiling-mounted fixtures using conventional 254nm UV directed at upper room air while occupants remain safely below. Achieves safety through spatial design rather than wavelength selection. Established technology with decades of evidence, currently under-deployed.\n\nBoth approaches provide continuous passive disinfection against bacteria, viruses, and fungi without pathogen-specific targeting.",
    "whyItMatters": [
      "Passive defense that does not require pathogen identification—eliminates pathogens regardless of whether they're known or engineered",
      "Works against any airborne threat including novel or vaccine-evading pathogens",
      "Far-UVC achieved 99.8% reduction of airborne infectious virus in occupied rooms (2024 study)",
      "Upper-room UV is deployable now without waiting for Far-UVC regulatory approval"
    ],
    "currentState": "- **Status**: Research/Pilot (Far-UVC), Deployment (Upper-room)\n- **Bottlenecks**:\n  - Far-UVC: Regulatory approval for widespread occupied-space use; production scaling for cost reduction; ozone generation requires adequate ventilation; long-term exposure studies ongoing\n  - Upper-room: Installation costs; building owner awareness; maintenance requirements; incorrect installation reducing effectiveness\n- **Recent progress (2024-2025)**:\n  - 3-year eye safety study showed no damage in workers under daily Far-UVC exposure (Sugihara et al., 2024)\n  - 2024 room study: 99.8% airborne virus reduction (murine norovirus) within regulatory limits\n  - 2025 corneal model study confirmed Far-UVC effects limited to outermost layers, no damage to regenerative cells\n  - LED development: 10% wall-plug efficiency prototypes scheduled for autumn 2025, commercial devices expected 2027\n  - 2025 safety review: \"high ability\" pathogen kill rate, \"high level of safety,\" long-term studies continue",
    "whoWorking": "- **UV Medico**: Commercial Far-UVC fixtures\n- **Columbia University (David Brenner lab)**: Foundational Far-UVC safety research\n- **Far UV Technologies**: Commercial Far-UVC deployment\n- **CDC**: Upper-room UV guidelines and research\n- Multiple academic groups validating efficacy and safety",
    "sources": [
      {
        "text": "Peregrine 2025 #136: UV Technology",
        "url": "../sources/peregrine-2025/interventions/peregrine-136.md"
      },
      {
        "text": "Concrete Biosecurity Projects: Sterilization Technology",
        "url": "../sources/concrete-biosecurity-ea/proposals/sterilization-technology.md"
      },
      {
        "text": "Biosecurity Engineers: Far-UVC Deployment",
        "url": "../sources/biosecurity-engineers/proposals/far-uvc.md"
      },
      {
        "text": "Biosecurity Engineers: Upper-Room UVGI",
        "url": "../sources/biosecurity-engineers/proposals/upper-room-uvgi.md"
      }
    ]
  },
  {
    "filename": "verified-kinetic-actuators",
    "title": "Verified Kinetic Actuators",
    "tag": "Security",
    "whatItIs": "Provably safe mechanisms for AI systems to interact with physical environments through robotics and other actuators. Verification systems ensure reliable and predictable operation of AI-controlled physical systems, providing safety guarantees for industrial automation, autonomous vehicles, and critical infrastructure.",
    "whyItMatters": [
      "Software failures in physical systems could have catastrophic consequences",
      "Without verification, AI control of robots and machinery creates unacceptable risk profiles",
      "Physical AI actions are irreversible in ways software actions often aren't"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #62: Verified Kinetic Actuators",
        "url": "../sources/peregrine-2025/interventions/peregrine-062.md"
      }
    ]
  },
  {
    "filename": "vulnerability-research-matching",
    "title": "Vulnerability-research matching",
    "tag": "Science",
    "whatItIs": "System that directly connects discovered AI vulnerabilities to research proposals addressing them. Ensures funding flows to researchers tackling actual identified problems rather than theoretical concerns. Continuously updates as new vulnerabilities are discovered through red teaming, creating rapid feedback loop between threat identification and solution development.",
    "whyItMatters": [
      "Safety research funding is disconnected from actual discovered vulnerabilities",
      "Red teams find problems but funding goes to proposals based on different criteria",
      "Matching system ensures resources go directly to solving identified problems"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #49: Vulnerability Matching",
        "url": "../sources/peregrine-2025/interventions/peregrine-049.md"
      }
    ]
  },
  {
    "filename": "wearable-infection-detection",
    "title": "Wearable infection detection",
    "tag": "Security",
    "whatItIs": "Create next-generation wearables for host response-based diagnosis that detect physiological anomalies indicating infection without identifying the specific pathogen. Rather than competing with commercial wearables, the approach identifies measurements beyond current capabilities and operationalizes existing devices for an early warning biodefense ecosystem. Leverages existing wearable technology momentum and user acceptance while developing missing components. Addresses the fundamental challenge: \"What do you look for when you don't know what you're looking for?\"",
    "whyItMatters": [
      "Novel or engineered pathogens may evade specific diagnostic tests",
      "Host response detection works against any biological threat including unanticipated ones",
      "Distributed detection through consumer devices creates population-level early warning system"
    ],
    "currentState": "- **Status**: Research\n- **Bottlenecks**: Identifying physiological signatures that reliably indicate early infection; false positive rates in population-scale deployment; privacy concerns; integration with existing wearable platforms",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #141: Global Immune System",
        "url": "../sources/peregrine-2025/interventions/peregrine-141.md"
      }
    ]
  },
  {
    "filename": "whistleblower-protection-fund",
    "title": "Whistleblower protection fund",
    "tag": "Society",
    "whatItIs": "Large, long-lived fund (several hundred million dollars) to secure livelihoods of potential whistleblowers for a decade and cover major legal exposure. Primarily protects AI lab employees, but also applies to government officials who refuse orders they believe endanger public safety related to AI development.",
    "whyItMatters": [
      "Insiders with knowledge of dangerous practices face career destruction and legal liability",
      "Credible protection removes barrier to information flow",
      "Could prevent catastrophic decisions through early disclosure"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Fund capitalization (hundreds of millions); legal structure; credibility with potential whistleblowers",
    "whoWorking": "- **AIWI**: AI whistleblower initiatives mentioned",
    "sources": [
      {
        "text": "Peregrine 2025 #189: Whistleblower Protection Fund",
        "url": "../sources/peregrine-2025/interventions/peregrine-189.md"
      }
    ]
  },
  {
    "filename": "workforce-displacement-planning",
    "title": "Workforce displacement response planning",
    "tag": "Society",
    "whatItIs": "Coordinated frameworks for identifying vulnerable sectors, quantifying displacement timelines, and deploying retraining programs before mass job loss occurs. The approach brings together government agencies, educational institutions, industry, and labor organizations to create sector-specific transition plans. Analysis suggests large-scale displacement risk within 3 years, with K-curve inequality effects potentially accelerating within 18 months.",
    "whyItMatters": [
      "Social safety nets could fail faster than institutions can respond",
      "Political backlash from unmanaged displacement undermines AI governance capacity",
      "Early identification enables targeted interventions in vulnerable sectors"
    ],
    "currentState": "- **Status**: Idea\n- **Bottlenecks**: Requires multi-stakeholder coordination; no clear institutional home",
    "whoWorking": "- Not specified",
    "sources": [
      {
        "text": "Peregrine 2025 #153: AI Displacement Response Planning",
        "url": "../sources/peregrine-2025/interventions/peregrine-153.md"
      }
    ]
  }
]