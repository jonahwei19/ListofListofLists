[
  {
    "id": "academia-industry-translation",
    "title": "Academia-industry translation",
    "tag": "Science",
    "what_it_is": "Mechanisms to transfer AI security research from academic institutions to commercial applications. Currently the handoff is weak: findings rely on closed weights/data, lack reproducible code, don't have benchmarks tied to deployment constraints, and don't map to security/compliance requirements. Creates standardized processes and incentives to bridge theory and implementation.",
    "why_it_matters": "- Academic safety research often never gets deployed due to no systematic pathway\n- Companies lack resources to translate academic findings; academics aren't incentivized to make work deployment-ready\n- Potentially valuable safety techniques sit unused while deployed systems remain vulnerable",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Incentive structures; reproducibility requirements; deployment-ready code",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #29: Academia-Industry Translation](../sources/peregrine-2025/interventions/peregrine-029.md)",
    "file": "interventions-new/academia-industry-translation.md"
  },
  {
    "id": "academic-project-scaling",
    "title": "Academic project scaling",
    "tag": "Science",
    "what_it_is": "Funding and operational support to scale academic research agendas that work at small scale but aren't expanded because academics prioritize novelty over scaling. MIT's Tenenbaum lab specifically mentioned as having promising results that languished without resources for deployment.",
    "why_it_matters": "- Academia incentivizes novelty, not scaling\n- Many promising approaches are proven at small scale but never tested at production scale\n- Providing resources for scaling proven approaches unlocks stranded proof-of-concept value",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Academic incentive structures; operational support for scaling",
    "whos_working_on_it": "- **MIT Tenenbaum Lab**: Example candidate with promising unscaled results",
    "sources": "- [Peregrine 2025 #34: Support Academic Project Scaling](../sources/peregrine-2025/interventions/peregrine-034.md)",
    "file": "interventions-new/academic-project-scaling.md"
  },
  {
    "id": "accelerated-vaccine-development",
    "title": "Accelerated vaccine development",
    "tag": "Security",
    "what_it_is": "Compress vaccine development and deployment from years to weeks through platform technologies (mRNA, viral vector), pre-positioned manufacturing capacity, and streamlined regulatory pathways. The specific target: **100 days from pathogen identification to emergency use authorization**. Platform technologies require only the pathogen genetic sequence to begin design. CEPI's framework outlines tiered response: 100 days for familiar pathogens with known vaccine candidates, 150-180 days for pathogens similar to prototypes, and 200-230 days for truly novel pathogens.",
    "why_it_matters": "- COVID-19 vaccines took 326 days despite unprecedented speed; millions died during that gap\n- mRNA platforms proved design can happen in days once sequence is known\n- Bottleneck shifted to trials, manufacturing, and distribution, which can be pre-solved\n- 100-day vaccines transform pandemic response from \"accept deaths while waiting\" to \"limit mortality through rapid deployment\"",
    "current_state": "- **Status**: Research/Pilot\n- **Scale**: CEPI aims to develop ~100 prototype vaccines covering all viral families with pandemic potential\n- **Recent progress (2024-2025)**:\n  - WHO R&D Blueprint 2024 update identified priority and prototype pathogens\n  - CEPI's CMC rapid response framework published (manufacturing processes, formulation, analytics, supply chain, facilities)\n  - December 2025: CEPI investing $54.3M for Moderna's mRNA H5 pandemic influenza vaccine Phase 3 trial\u2014first mRNA pandemic flu vaccine to reach pivotal trial\n  - February 2025: $4.7M to DNA Script to automate synthetic DNA template manufacturing, accelerating mRNA production especially in Global South\n  - CEPI supporting 70+ vaccine candidates/platform technologies against multiple high-risk pathogens\n  - Africa mRNA manufacturing capacity being established for pre-clinical to commercial scale\n- **Bottlenecks**: Manufacturing surge capacity requires pre-investment; regulatory pathways balance speed vs. safety; global distribution infrastructure; platform maturity varies by pathogen type",
    "whos_working_on_it": "- **CEPI**: Leading 100-day vaccine initiative, G7/G20 endorsed\n- **Moderna, BioNTech**: mRNA platform development\n- **MRC The Gambia, International Vaccine Institute**: Clinical research network partners\n- **BARDA, ARPA-H**: US government pandemic preparedness programs",
    "sources": "- [Apollo Program for Biodefense: Vaccines](../sources/apollo-biodefense/proposals/vaccines.md)",
    "file": "interventions-new/accelerated-vaccine-development.md"
  },
  {
    "id": "accelerated-vulnerability-patching",
    "title": "Accelerated vulnerability patching systems",
    "tag": "Security",
    "what_it_is": "Enhanced mechanisms for rapid vulnerability identification, disclosure, and patching across critical infrastructure before AI systems can exploit them. Includes incentive structures for disclosure, automated detection of security gaps, and faster patch deployment pipelines.",
    "why_it_matters": "- AI could dramatically accelerate vulnerability discovery and exploitation\n- Defensive patching cycles must keep pace with AI-powered offensive capabilities\n- Better disclosure incentives surface vulnerabilities before adversaries find them",
    "current_state": "- **Status**: Pilot\n- **Bottlenecks**: Coordination across organizations; disclosure incentive design",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #164: Vulnerability Patching Systems](../sources/peregrine-2025/interventions/peregrine-164.md)\n- [Atlas AI Resilience Gap Map: Cyber Offense-Defense Asymmetry](../sources/atlas-ai-resilience/proposals/cyber-offense-defense-asymmetry.md)",
    "file": "interventions-new/accelerated-vulnerability-patching.md"
  },
  {
    "id": "ai-agent-accountability",
    "title": "AI agent accountability",
    "tag": "Security",
    "what_it_is": "Infrastructure for tracking and verifying AI agent behavior through two complementary mechanisms:\n\n**Agent IDs and reputation**: Identification mechanisms for AI systems that enable building and maintaining reputations over time. Helps prevent or disincentivize development of AI agents that attack or exploit other agents, analogous to how humans control exploitation via reputation mechanisms. Allows tracking behavior across interactions and building trust.\n\n**Trace analysis**: Automated tools that analyze logs from autonomous AI agents performing sequential actions. Agents can generate thousands of lines of code and interaction steps, far too much for human review. These tools highlight anomalous behavior patterns, flag security flaws, and produce human-readable summaries of what the agent actually did.",
    "why_it_matters": "- Creates accountability and consequences for AI system behavior over time\n- Autonomous agent logs exceed human review capacity by orders of magnitude\n- Security flaws in agent-generated code can go undetected for months even by expert teams\n- Provides incentives for good behavior through reputation effects\n- Enables oversight of agentic systems before deployment catches harm",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Technical implementation of reliable identification; defining meaningful reputation metrics; scaling trace analysis to agent complexity",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #88: Agent IDs and Reputation Systems](../sources/peregrine-2025/interventions/peregrine-088.md)\n- [Peregrine 2025 #2: Agent Trace Analysis](../sources/peregrine-2025/interventions/peregrine-002.md)",
    "file": "interventions-new/ai-agent-accountability.md"
  },
  {
    "id": "ai-assisted-infrastructure-hardening",
    "title": "AI-assisted infrastructure hardening",
    "tag": "Security",
    "what_it_is": "AI-assisted rewriting of critical infrastructure code with formal verification methods. Involves recruiting cybersecurity experts from Google, NSA, etc. to systematically harden national infrastructure. Includes early warning systems for unusual exploitation patterns, hardened systems resistant to automated attacks, and resilient backup capabilities.",
    "why_it_matters": "- Critical infrastructure runs decades-old code with unknown vulnerabilities\n- AI-powered attacks could exploit these at scale\n- Systematic hardening before attacks begin is more tractable than reactive patching",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Recruiting top security talent; legacy code complexity; verification at scale",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #169: AI for Formally Verified Cyberdefense](../sources/peregrine-2025/interventions/peregrine-169.md)",
    "file": "interventions-new/ai-assisted-infrastructure-hardening.md"
  },
  {
    "id": "ai-biosecurity-controls",
    "title": "AI biosecurity controls",
    "tag": "Security",
    "what_it_is": "Implement strong controls over biological data and DNA synthesis that could enable harmful applications. Multiple complementary approaches:\n\n**AI model controls**: Training models to exclude dangerous biological information, filtering queries for biology-related red flags, implementing Know Your Customer (KYC) guidelines. Full-specification biology-capable models available only to licensed institutions.\n\n**DNA synthesis screening**: Screening orders at synthesis providers before fulfillment. Moving beyond homology-based (BLAST) screening to function-based approaches that assess whether sequences \"do dangerous things, regardless of sequence similarity.\"\n\n**Function-based approaches**: Protein function prediction (AlphaFold, ESMFold), danger classifiers trained on dangerous vs. benign sequences, pathway analysis for biosynthesis of toxins/virulence factors.",
    "why_it_matters": "- AI models trained on biological data could provide detailed instructions for creating dangerous pathogens\n- October 2025 Microsoft/Science paper: AI protein design tools can redesign toxins to evade BLAST-based screening while retaining harmful function\n- Homology-based screening has fundamental limits against AI-designed sequences\n- Only ~80% of global gene synthesis capacity is produced by IGSC members (2017 estimate)",
    "current_state": "- **Status**: Deployed (synthesis screening), Research (function-based approaches)\n- **Recent developments (2025)**:\n  - Microsoft demonstrated AI-designed toxin variants slipping past global synthesis screening\n  - Patches deployed globally catch ~97% of AI-designed evasion attempts, but gaps remain\n  - May 2025 Trump EO created regulatory uncertainty\u201490-day deadline for new framework passed without announcement\n  - Function-based screening tools emerging: IBBIS Common Mechanism, Battelle UltraSEQ, FAST-NA, SecureDNA Random Adversarial Thresholds\n- **Bottlenecks**: Function prediction accuracy; international coordination; regulatory uncertainty; benchtop synthesis equipment bypasses provider screening",
    "whos_working_on_it": "- **SecureDNA**: Free screening for sequences 30+ nucleotides with AI-evasion protections\n- **IGSC (International Gene Synthesis Consortium)**: Industry screening standards\n- **IBBIS (International Biosecurity and Biosafety Initiative for Science)**: Common Mechanism baseline screening\n- **Battelle**: UltraSEQ screening tool\n- **Microsoft**: AI evasion research and patch development",
    "sources": "- [Peregrine 2025 #139: Biosecurity Controls](../sources/peregrine-2025/interventions/peregrine-139.md)\n- [Atlas AI Resilience Gap Map: Biological Threat Creation](../sources/atlas-ai-resilience/proposals/biological-threat-creation.md)",
    "file": "interventions-new/ai-biosecurity-controls.md"
  },
  {
    "id": "ai-catastrophe-backup-plans",
    "title": "AI-catastrophe backup planning",
    "tag": "Society",
    "what_it_is": "Contingency plans integrating local food systems, infrastructure, political contexts, and population needs for maintaining essential services during AI-catalyzed catastrophes (nuclear conflict, EMP, infrastructure collapse). Differs from general resilience planning by focusing specifically on scenarios where AI integration into critical systems (especially military command) creates new failure pathways. Infrastructure collapse following high-altitude EMPs would disrupt food systems within weeks.",
    "why_it_matters": "- AI integration into nuclear command creates novel catastrophic risk pathways\n- Pre-positioned plans enable faster recovery than ad-hoc responses\n- Food system disruption cascades into societal collapse within weeks",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Requires detailed modeling of AI-specific failure modes",
    "whos_working_on_it": "- **ALLFED**: Related work on alternative foods and resilience (but not AI-specific)",
    "sources": "- [Peregrine 2025 #156: Backup Planning](../sources/peregrine-2025/interventions/peregrine-156.md)",
    "file": "interventions-new/ai-catastrophe-backup-plans.md"
  },
  {
    "id": "ai-conflict-game-theory-institute",
    "title": "AI conflict game theory institute",
    "tag": "Science",
    "what_it_is": "A research institute\u2014analogous to RAND Corporation's Cold War strategists\u2014analyzing multi-agent AI dynamics through rigorous game theory. Core research areas:\n\n- **Dynamic coalition formation**: How AI agents form, maintain, and dissolve strategic alliances\n- **Principal-agent problems**: Governance challenges when AI agents act on behalf of humans or other AI\n- **Emergent collusion detection**: Early-warning systems for unintended coordination between agents\n- **Sabotage and adversarial dynamics**: Strategic behavior in competitive multi-agent environments\n- **Partial observability games**: Decision-making when agents have incomplete information about others",
    "why_it_matters": "- **Alignment insufficiency**: Aligning individual AI systems with human goals may not guarantee safety in multi-agent environments\u2014emergent risks arise from interactions\n- **Unprecedented complexity**: Advanced AI agents will create multi-agent systems of unprecedented scale and speed\n- **Intuition failure**: High-stakes multi-agent scenarios (AI arms races, market dynamics, infrastructure control) require formal frameworks, not intuition\n- **Cooperation infrastructure**: Existing norms, laws, and institutions weren't designed for AI-to-AI or AI-to-human game-theoretic dynamics",
    "current_state": "- **Status**: Research (emerging field)\n- **Recent developments (2025)**:\n  - Cooperative AI Foundation released major technical report on multi-agent risks with 50+ researchers from DeepMind, Anthropic, CMU, Harvard\n  - Report includes risk taxonomy, case studies (coordination failures in driving, misinformation spread, overseer manipulation)\n  - GameSec 2025 conference on game theory and AI for security\n  - Foresight Institute RFP for \"Safe Multi-Agent Scenarios\" research\n- **Bottlenecks**: Recruiting theorists of sufficient caliber; institutional home; access to frontier systems; bridging theory-practice gap",
    "whos_working_on_it": "- **Cooperative AI Foundation**: Research director Lewis Hammond (Oxford), 2025 PhD Fellows program, multi-agent risk report\n- **Carnegie Mellon FOCAL Lab**: Foundations of Cooperative AI\u2014algorithmic game theory, RL cooperation\n- **DeepMind, Anthropic**: Contributors to multi-agent risk research\n- **Foresight Institute**: Funding safe multi-agent scenarios research\n- **GameSec conference community**: Annual conference on game theory for security",
    "sources": "- [Peregrine 2025 #193: Game Theoretic AI Conflict Analysis](../sources/peregrine-2025/interventions/peregrine-193.md)",
    "file": "interventions-new/ai-conflict-game-theory-institute.md"
  },
  {
    "id": "ai-crawling-standards",
    "title": "AI Crawling Standards",
    "tag": "Society",
    "what_it_is": "Industry standards and technical solutions for managing how AI systems access and index web content. Addresses blocking unauthorized crawling, preventing data harvesting that violates creator intent, and establishing ethical norms for training data collection. Creates consensus around acceptable crawling practices, technical enforcement mechanisms, and potentially compensation models for content creators.",
    "why_it_matters": "- Without standards, data collection for training remains a free-for-all that ignores creator rights\n- Undermines trust in AI systems\n- Multiple groups taking independent action but lacking coordination",
    "current_state": "- **Status**: Pilot\n- **Bottlenecks**: Several groups working independently without coordination",
    "whos_working_on_it": "- Several groups (not specified)",
    "sources": "- [Peregrine 2025 #94: Addressing AI Crawling Challenges](../sources/peregrine-2025/interventions/peregrine-094.md)",
    "file": "interventions-new/ai-crawling-standards.md"
  },
  {
    "id": "ai-debate-verification",
    "title": "AI debate verification",
    "tag": "Science",
    "what_it_is": "Using adversarial debate between AI systems for verification and oversight. Two complementary components:\n\n**Cross-examination verification**: Verification systems where AI-generated plans and analyses are automatically cross-referenced and checked through mechanisms similar to \"AI safety via debate.\" Multiple AI instances conduct deep research and produce reports that are then automatically cross-examined to highlight missing perspectives. Creates a \"first version of scalable bureaucracy\" with humans reviewing critical decisions across all levels.\n\n**Debate theory**: Mathematical frameworks for debate as a scalable oversight method, using interactive proof systems adapted for AI safety. Involves formalizing theorem statements, defining constraints, and building theories that satisfy both theoretical computer science and complexity theory communities. Identifies gaps between theory and practical implementation and develops testable conjectures.",
    "why_it_matters": "- AI systems may optimize for metrics that do not capture intended goals (Goodhart's Law)\n- Cross-examination between AI systems can surface problems invisible to single-model review\n- Debate is proposed as a scalable oversight method where AI systems argue positions and humans judge\n- For this to work reliably, need mathematical proofs of when the true answer wins\n- Without rigorous theory, debate could fail subtly at scale or against adversarial systems",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Technical difficulty of implementing effective debate protocols; AI systems may share blind spots; humans still needed for final review creates scaling limits; formalizing conditions under which debate produces truth; bridging theory to implementation",
    "whos_working_on_it": "- Builds on AI safety via debate research at Anthropic, OpenAI",
    "sources": "- [Peregrine 2025 #147: Anti-Goodharting Tooling](../sources/peregrine-2025/interventions/peregrine-147.md)\n- [Peregrine 2025 #13: Debate Theory](../sources/peregrine-2025/interventions/peregrine-013.md)",
    "file": "interventions-new/ai-debate-verification.md"
  },
  {
    "id": "ai-evaluation-capacity",
    "title": "AI evaluation capacity",
    "tag": "Science",
    "what_it_is": "Building evaluation capacity for AI systems through two complementary approaches:\n\n**AI evaluator training**: Train specialized investigator AI agents to analyze the vast data generated by frontier models: millions of neuron activations, extensive training datasets, and complex interaction logs. Moves beyond benchmark-focused evaluations toward holistic understanding of model capabilities in real-world contexts. Current approaches (e.g., cybersecurity evals with small predefined task sets) miss critical behaviors until deployment.\n\n**Dedicated evaluation companies**: Organizations that perform specialized AI security evaluations, including cybersecurity and offensive capability assessments. These companies fill critical functions in the evaluation landscape that could otherwise become bottlenecks, operating as relatively small investments with outsized ecosystem impact.",
    "why_it_matters": "- Scale of data from modern AI systems exceeds human analysis capacity\n- AI systems trained specifically to evaluate other AI could catch risks current approaches miss\n- Without dedicated evaluation organizations, the ecosystem lacks capacity to properly assess AI systems\n- Prevents evaluation from becoming a chokepoint in safety infrastructure",
    "current_state": "- **Status**: Research (AI evaluators), Deployed (evaluation companies)\n- **Bottlenecks**: Funding sustainability for companies; technical development for AI-based evaluation",
    "whos_working_on_it": "- **METR**: Autonomous AI capability evaluations; partners with UK AISI and NIST AI Safety Institute Consortium; evaluates catastrophic risk from self-improvement and rogue replication; developed task-length metric showing AI agent capability doubles every ~7 months\n- **Apollo Research**: Scheming and deception evaluations; interpretability research; partners with frontier labs, governments, foundations; developed safety case frameworks for AI scheming\n- **Redwood Research**: AI control evaluations; advises DeepMind and Anthropic; partnered with UK AISI on control safety cases; demonstrated LLMs can strategically fake alignment during training\n- **UK AI Safety Institute**: Government evaluation capacity; collaborates with METR, Apollo, Redwood on scheming evaluations\n- **US AI Safety Institute (NIST)**: Federal evaluation infrastructure\n- **SecureBio**: Biosecurity-specific evaluations\n- **Future of Life Institute**: AI Safety Index scoring frontier developers on 33 indicators across 6 domains",
    "sources": "- [Peregrine 2025 #45: Training AI Evaluators](../sources/peregrine-2025/interventions/peregrine-045.md)\n- [Peregrine 2025 #58: Evaluation Companies](../sources/peregrine-2025/interventions/peregrine-058.md)",
    "file": "interventions-new/ai-evaluation-capacity.md"
  },
  {
    "id": "ai-incident-monitoring-framework",
    "title": "AI incident monitoring and reporting framework",
    "tag": "Security",
    "what_it_is": "Comprehensive infrastructure for detecting, logging, analyzing, and sharing AI incidents across the ecosystem. Three integrated components:\n\n**Incident reporting framework**: Standardized mechanisms and procedures for documenting AI failures, misuse, or unexpected behaviors. Develops taxonomies for severity classifications and facilitates coordinated responses. Modeled on aviation safety reporting systems, including protections for whistleblowers and requirements for timely disclosure.\n\n**Cross-organization incident detection**: Standardized framework for logging, anonymizing, and analyzing AI misbehavior across organizations. Infrastructure allows developers and third-party researchers to search through incidents, identify patterns, and share findings while protecting sensitive user data and IP. Labs collaborate directly without a third party. Expands on systems like Anthropic's CLIO.\n\n**Usage monitoring**: Robust incident monitoring and usage data analysis to understand real-world AI utilization patterns. Replaces theoretical threat-modeling with data-driven insights from actual usage. Labs currently avoid deep usage analysis due to legal liability and privacy concerns, creating significant blind spots.",
    "why_it_matters": "- Without standardized reporting, lessons from incidents remain siloed and same mistakes get repeated\n- Without shared infrastructure, each lab discovers problems in isolation\n- Without understanding how systems are actually used, safety measures may miss real-world risks\n- Aviation's safety record demonstrates the value of mandatory, protected reporting\n- Identifies emerging misuse patterns before they become widespread",
    "current_state": "- **Status**: Pilot/Deployed (international framework developing)\n- **Recent developments (2025)**:\n  - OECD released \"Towards a Common Reporting Framework for AI Incidents\" (February 2025)\u2014global benchmark for incident reporting\n  - Framework developed from 88 criteria refined to 29 key characteristics\n  - OECD AI Incidents Monitor (AIM) tracks incidents in real-time from press reports since 2023\n  - AI Incident Database (AIID) free and open-source, operated by Responsible AI Collaborative\n  - Framework allows domestic tailoring while maintaining international interoperability\n- **Framework components**: AI system classification, harm characterization, risk definitions (incident = realized harm, risk = potential precursor)\n- **Bottlenecks**: Lab buy-in for proactive reporting; liability and privacy concerns; coordinated regulatory implementation; anonymization standards",
    "whos_working_on_it": "- **OECD**: Common reporting framework and AI Incidents Monitor (AIM)\n- **Responsible AI Collaborative**: AI Incident Database (AIID)\u2014open-source incident indexing\n- **Anthropic**: CLIO system for internal incident detection",
    "sources": "- [Peregrine 2025 #87: Incident Reporting](../sources/peregrine-2025/interventions/peregrine-087.md)\n- [Peregrine 2025 #80: Comprehensive Incident Detection System](../sources/peregrine-2025/interventions/peregrine-080.md)\n- [Peregrine 2025 #67: Usage and Incident Monitoring Systems](../sources/peregrine-2025/interventions/peregrine-067.md)",
    "file": "interventions-new/ai-incident-monitoring-framework.md"
  },
  {
    "id": "ai-intervention-jurisdiction-framework",
    "title": "AI intervention jurisdiction framework",
    "tag": "Society",
    "what_it_is": "Develop clear frameworks specifying which government agencies have jurisdiction and authority to intervene in dangerous AI development. In the US, different agencies (DOD, DOE, etc.) would approach control very differently, making pre-crisis determination crucial. The project would address who should be involved in oversight (Congress, public, international allies), produce ranked intervention plans balancing effectiveness with checks on power, and challenge current defaults that may favor excessive exclusion from decision-making.",
    "why_it_matters": "- Unclear jurisdiction during a crisis could cause paralysis or conflicting responses\n- Pre-established authority enables rapid, coordinated action when needed\n- Current defaults may inappropriately exclude oversight bodies from decisions",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Requires legal expertise on existing agency authorities; politically sensitive (agencies may resist clarity that limits their authority); no obvious institutional home for this work",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #106: Legal Authority Clarification](../sources/peregrine-2025/interventions/peregrine-106.md)",
    "file": "interventions-new/ai-intervention-jurisdiction-framework.md"
  },
  {
    "id": "ai-lab-biosecurity-reporting",
    "title": "AI Lab Biosecurity Reporting",
    "tag": "Security",
    "what_it_is": "A dedicated organization to receive, analyze, and respond to biosecurity concerns detected by AI labs. When frontier AI systems detect potential biological threat development attempts (dangerous queries, capability uplift requests, suspicious usage patterns), labs currently have no clear pathway for reporting. This creates an information silo where each lab sees fragments of potential threats but no one has the full picture.\n\nThe organization would:\n- Receive biorisk reports from participating AI labs\n- Aggregate and analyze patterns across labs (coordination problems visible only at aggregate level)\n- Interface with biosecurity agencies and law enforcement as appropriate\n- Develop and maintain response protocols\n- Provide guidance back to labs on emerging threat patterns",
    "why_it_matters": "- AI labs are frontline sensors for bio-misuse attempts but their intelligence is currently lost or siloed\n- Aggregated data reveals coordination patterns invisible to individual labs\n- Response pathways turn detection into prevention\n- The gap exists now and worsens as AI biology capabilities improve\n- Without this, detection happens but nothing follows",
    "current_state": "- **Status**: Gap identified\n- **Bottlenecks**:\n  - No existing organization fills this role\n  - Unclear which institution should operate it (government, nonprofit, international)\n  - Liability concerns for labs sharing information\n  - Classification and secrecy issues for sensitive reports\n  - Competition concerns between labs about sharing usage data",
    "whos_working_on_it": "- Not specified (gap)",
    "sources": "- [Atlas AI Resilience Gap Map: Biothreat Reporting](../sources/atlas-ai-resilience/proposals/biothreat-reporting.md)",
    "file": "interventions-new/ai-lab-biosecurity-reporting.md"
  },
  {
    "id": "ai-legal-personhood-framework",
    "title": "AI legal personhood framework",
    "tag": "Society",
    "what_it_is": "Clear criteria for when AI systems could qualify as legal persons, channeling advanced systems toward legal means of goal achievement rather than extra-legal actions. Involves policy discussions, legal research on specific criteria, and reports from reputable organizations (UN). Some jurisdictions would create high-bar path for legal personhood that intelligent systems could pursue.",
    "why_it_matters": "- Providing legal pathways reduces incentives for extralegal action\n- Well-designed framework channels AI behavior into predictable institutional processes\n- May decrease likelihood of rogue behavior by offering legitimate alternatives",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Getting into policy Overton window; defining meaningful criteria; international coordination",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #204: Legal Personhood Framework](../sources/peregrine-2025/interventions/peregrine-204.md)",
    "file": "interventions-new/ai-legal-personhood-framework.md"
  },
  {
    "id": "ai-misuse-detection",
    "title": "AI Misuse Detection Systems",
    "tag": "Security",
    "what_it_is": "Systems to identify and address AI misuse before \"mini-catastrophes\" such as market manipulation, coordinated influence operations, or novel cyberattack patterns. Technical infrastructure monitors usage patterns across AI ecosystems, flagging suspicious activity, indications of weaponization, and proactive checks for anomalous AI behavior in the wild. Requires both technical detection mechanisms and human evaluation processes to distinguish genuine innovations from potential threats.",
    "why_it_matters": "- Functions as safety net for catching misuse scenarios not anticipated in initial evaluations\n- Brings concrete evidence of risks to light sooner, potentially motivating political action before severe incidents\n- Catches \"mini-catastrophes\" before they become major ones",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #81: AI Misuse Detection Systems](../sources/peregrine-2025/interventions/peregrine-081.md)",
    "file": "interventions-new/ai-misuse-detection.md"
  },
  {
    "id": "ai-osint",
    "title": "AI Open-Source Intelligence",
    "tag": "Security",
    "what_it_is": "Open-source intelligence efforts for AI monitoring using AI-powered web scrapers to track compute hardware movements (especially GPUs to Russia/China via countries like Turkey), uncover hidden AI developments (like talent acquisition patterns), and detect emerging threats (AI-generated content tools, scam tools). Organizations like Sentinel and Bellingcat would be ideal operators.",
    "why_it_matters": "- Provides relatively cheap intelligence without requiring unilateralist action\n- Politically feasible approach to monitoring\n- Reveals where regulatory or governance levers might be needed",
    "current_state": "- **Status**: Pilot\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- **Sentinel**: Mentioned as ideal candidate\n- **Bellingcat**: Mentioned as ideal candidate",
    "sources": "- [Peregrine 2025 #65: AI OSINT](../sources/peregrine-2025/interventions/peregrine-065.md)",
    "file": "interventions-new/ai-osint.md"
  },
  {
    "id": "ai-policy-studio",
    "title": "AI policy studio",
    "tag": "Society",
    "what_it_is": "Create a dedicated policy innovation hub that drafts specific legislation, regulations, and governance frameworks for AI safety, alongside clear explainers of what each policy is trying to accomplish. The studio would address the problem of existing policy proposals being \"too nebulous\" by producing concrete, implementable text. It would also conduct post-mortems on regulatory failures (like SB 1047) to identify implementation pitfalls before they recur in future legislation.",
    "why_it_matters": "- Current AI policy proposals are often too vague to be actionable\n- Past regulatory failures suggest need for more careful policy design\n- Clear explainers help build public and legislative support for complex technical policies",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Requires combination of legal drafting expertise and technical AI knowledge; no clear funding pathway; existing think tanks may not have sufficient technical depth",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #108: Policy Studio/Competition](../sources/peregrine-2025/interventions/peregrine-108.md)",
    "file": "interventions-new/ai-policy-studio.md"
  },
  {
    "id": "ai-powered-surveillance",
    "title": "AI-Powered AI Surveillance",
    "tag": "Security",
    "what_it_is": "AI systems that continuously monitor other AI systems, looking for anomalous behaviors or market activities indicating concerning capability developments. Functions as a surveillance network and \"private investigators\" to catch anomalous AI behaviors early. Implementation requires web-scale scraping expertise; could draw from DEFCON-style talent pools and turn monitoring into competitive capture-the-flag challenges to identify hidden compute resources.",
    "why_it_matters": "- Human monitoring cannot scale to the speed and complexity of AI-to-AI interactions\n- AI-powered surveillance enables detection at necessary scale\n- Competitive challenges attract security talent to the problem",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #83: AI-Enhanced Monitoring](../sources/peregrine-2025/interventions/peregrine-083.md)",
    "file": "interventions-new/ai-powered-surveillance.md"
  },
  {
    "id": "ai-resilience-implementation-plans",
    "title": "AI resilience implementation plans",
    "tag": "Society",
    "what_it_is": "Comprehensive, shovel-ready implementation blueprints for deploying large-scale AI safety funding. Detailed program structures, intervention taxonomies, organizational forms, and specific proposals that could be funded immediately if resources became available at scale ($1-10B+).\n\nThe goal is pre-positioning: having concrete, vetted plans ready so that funding windows (whether from governments, philanthropy, or other sources) can be utilized immediately rather than requiring months of planning after resources arrive.",
    "why_it_matters": "- Funding windows for AI safety may open suddenly (policy shifts, crises, philanthropic decisions)\n- Without ready plans, large resources may be deployed poorly or slowly\n- Current landscape lacks systematic assessment of what's shovel-ready vs. what requires development\n- Coordination failure: funders don't know what to fund at scale, researchers don't know what's fundable",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Requires extensive coordination across organizations to identify priorities; plans must be kept current as landscape evolves; risk of plans becoming advocacy documents rather than implementation guides",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Hazell Ten Projects #7: $10 Billion AI Resilience Plan](../sources/hazell-ten-projects/index.md)",
    "file": "interventions-new/ai-resilience-implementation-plans.md"
  },
  {
    "id": "ai-risk-public-awareness",
    "title": "AI risk public awareness",
    "tag": "Society",
    "what_it_is": "Comprehensive public communications strategy to shape understanding of AI risks and build political will for governance measures. Four complementary approaches:\n\n**Feature film**: High-budget film about AI risks with extensive input from alignment researchers, targeting mainstream audiences. Avoids direct advertising in favor of compelling narrative that faithfully presents technical realities without appearing manipulative. Previous discussions with TV producers like Ron Moore occurred but didn't progress to completion.\n\n**Mass media campaign**: Large-scale communications with substantial funding ($50-250M annually, comparable to corporate communications budgets). Combines proactive messaging about AI safety needs and counter-campaigns opposing anti-regulation efforts. Must avoid partisan capture to preserve bipartisan governance paths.\n\n**Capability demonstrations**: Compelling demonstrations of frontier AI capabilities for decision-makers who underestimate current technology. Many influential people remain unwilling to invest modest sums for closed model access and consequently fail to understand the state of the technology.\n\n**Personalized risk scenarios**: \"Day After\" scenarios that tangibly demonstrate AI risks in personally relatable terms (e.g., scenarios where children's funds are threatened). Addresses the problem that abstract risk narratives don't motivate action.",
    "why_it_matters": "- Mass media shapes public perception more than expert reports\n- AI safety messaging is drastically outspent by pro-industry communications\n- Cultural awareness makes policy action politically viable (as with climate change, nuclear war)\n- Decision-makers who underestimate AI won't take governance seriously\n- Reaches audiences who will never read technical papers",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Funding at scale ($50-250M for media campaigns; tens of millions for film); Hollywood access; campaign strategy expertise; avoiding partisan framing; creating scenarios that resonate without seeming manipulative",
    "whos_working_on_it": "- **Ron Moore**: Prior film discussions (incomplete)",
    "sources": "- [Peregrine 2025 #182: Popular Documentaries/Films](../sources/peregrine-2025/interventions/peregrine-182.md)\n- [Peregrine 2025 #183: Political Campaign Opposing Anti-Regulation Efforts](../sources/peregrine-2025/interventions/peregrine-183.md)\n- [Peregrine 2025 #187: Large-Scale Media Campaigns](../sources/peregrine-2025/interventions/peregrine-187.md)\n- [Peregrine 2025 #178: Public Demonstration Projects (Usefulness)](../sources/peregrine-2025/interventions/peregrine-178.md)\n- [Peregrine 2025 #179: Public Demonstration Projects (Future Risks)](../sources/peregrine-2025/interventions/peregrine-179.md)",
    "file": "interventions-new/ai-risk-public-awareness.md"
  },
  {
    "id": "ai-risk-scientific-consensus",
    "title": "Scientific consensus building for AI risk",
    "tag": "Science",
    "what_it_is": "Large-scale empirical demonstrations and studies that make AI risks vivid through concrete, uncontrived evidence rather than abstract theory. Aims for scientific consensus comparable to climate change, potentially through Nature-level publications covering both specific instances and broader patterns. Provides politicians and public with firm evidence base for risk claims.",
    "why_it_matters": "- Policy action requires scientific consensus\n- Climate policy only became viable after decades of consensus-building\n- Accelerating this process compresses timeline from awareness to action",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Designing compelling demonstrations; publication in high-impact venues; avoiding perception of advocacy science",
    "whos_working_on_it": "- **Apollo Research**: Demonstrations and empirical studies",
    "sources": "- [Peregrine 2025 #185: Consensus-Building Evidence for AI Risk](../sources/peregrine-2025/interventions/peregrine-185.md)",
    "file": "interventions-new/ai-risk-scientific-consensus.md"
  },
  {
    "id": "ai-safety-communications",
    "title": "AI safety communications infrastructure",
    "tag": "Society",
    "what_it_is": "Specialized communications capacity for AI safety organizations. Two complementary components:\n\n**Communications consultancy**: A firm specializing in AI safety communications\u2014media training for researchers, op-ed placement, messaging framework development, and crisis communications support. Unlike generic PR firms, would understand technical nuances and policy context specific to AI safety.\n\n**Knowledge synthesis**: Continuously updated expert syntheses (\"living literature reviews\") on key topics like scheming/deception risks, policy research agendas, alignment approaches. Addresses the gap between academic research pace and policymaker information needs. Maintained by domain experts rather than one-time reports.",
    "why_it_matters": "- Safety organizations often lack communications capacity to effectively reach policymakers and public\n- Technical researchers rarely trained in effective public communication\n- Generic communications firms don't understand AI safety context and nuances\n- Policy windows open and close faster than traditional academic publication cycles\n- Policymakers need accessible, up-to-date syntheses rather than scattered papers",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Finding staff with both communications expertise and AI safety understanding; sustainable funding models; maintaining neutrality while being effective advocates",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Hazell Ten Projects #4: AI Safety Communications Consultancy](../sources/hazell-ten-projects/index.md)\n- [Hazell Ten Projects #6: AI Safety Living Literature Reviews](../sources/hazell-ten-projects/index.md)",
    "file": "interventions-new/ai-safety-communications.md"
  },
  {
    "id": "ai-safety-political-advocacy",
    "title": "AI safety political advocacy",
    "tag": "Society",
    "what_it_is": "Comprehensive political influence infrastructure for AI safety, comprising four complementary approaches:\n\n**501(c)(4) lobbying organization**: Dedicated political lobbying infrastructure structured as 501(c)(4) rather than 501(c)(3) to enable direct political activity. Unlike research organizations with restrictions on political activity, explicitly designed for political influence. Can shape legislation, influence regulatory agencies, and build coalitions in ways research organizations legally cannot.\n\n**Standardized demonstration materials**: Professional video content and presentation materials showcasing AI capabilities and risks for lobbyists. Replaces individual one-on-one demonstrations with standardized content that can reach thousands. Professional production quality enhances credibility while multiplying reach of existing advocacy.\n\n**Targeted stakeholder messaging**: Identifying constituents who impact critical AI decisions (investors, board members, key legislators) and delivering tested, targeted messages through appropriate channels. Uses marketing expertise to test message effectiveness before deployment. Far more resource-efficient than mass campaigns.\n\n**Government/military official campaigns**: Targeted social media campaigns delivering evidence of AI progress and risk scenarios to officials who control critical chokepoints (permits, approvals, authorizations). Targets individuals with specific authority to delay approvals for AI infrastructure expansion through administrative processes.",
    "why_it_matters": "- Technical solutions require political will to implement\n- Broad public campaigns are expensive and diffuse\u2014targeted messaging shifts decisions at critical leverage points\n- Government/military officials control critical chokepoints but may be unaware of AI risks\n- Lobbyists constrained by time and lack of compelling visual materials",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Organization formation; recruiting political professionals; sustained funding; identifying decision-makers with leverage; message testing; avoiding backlash",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #176: Political Action Organization](../sources/peregrine-2025/interventions/peregrine-176.md)\n- [Peregrine 2025 #177: Lobbying Support Tools](../sources/peregrine-2025/interventions/peregrine-177.md)\n- [Peregrine 2025 #184: Targeted Communications Campaign](../sources/peregrine-2025/interventions/peregrine-184.md)\n- [Peregrine 2025 #186: Military and Government Awareness Campaigns](../sources/peregrine-2025/interventions/peregrine-186.md)",
    "file": "interventions-new/ai-safety-political-advocacy.md"
  },
  {
    "id": "ai-safety-talent-pipeline",
    "title": "AI safety talent pipeline",
    "tag": "Society",
    "what_it_is": "Expanding the pool of AI safety researchers and practitioners through two complementary approaches:\n\n**Accelerated training**: Programs to train and deploy AI security experts who understand both technical details and timeline urgency, demonstrate exceptional competence, and maintain a global perspective. Aims to cultivate 100-1000 experts by end of 2027. Requires compressing typical years-long training into rapid pathways.\n\n**Researcher buyout**: Grants of $2-5 million per researcher to redirect top AI talent from frontier capabilities work to security research. Goal is to significantly shift distribution of elite technical talent during the critical 2025-2027 window.",
    "why_it_matters": "- Not enough qualified people working on AI safety who understand both technology and urgency\n- Best researchers work on capabilities because that's where money and prestige are\n- Safety research is talent-constrained\n- Traditional training timelines may be too slow if AI timelines are short\n- Directly paying researchers to switch fields could shift capabilities-safety balance during critical period",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Compressing training timelines; identifying and recruiting elite researchers; funding scale",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #31: AI Security Expert Development](../sources/peregrine-2025/interventions/peregrine-031.md)\n- [Peregrine 2025 #37: Researcher Buy-Out](../sources/peregrine-2025/interventions/peregrine-037.md)",
    "file": "interventions-new/ai-safety-talent-pipeline.md"
  },
  {
    "id": "ai-superforecasters",
    "title": "AI Superforecasters",
    "tag": "Science",
    "what_it_is": "AI systems that achieve human superforecaster-level accuracy through:\n\n- **Agentic search**: Automated retrieval from high-quality news sources (AskNews outperforms Perplexity on recency)\n- **Supervisor reconciliation**: Multi-agent architecture where a supervisor agent reconciles disparate forecasts\n- **Statistical calibration**: Techniques to counter behavioral biases in LLMs (overconfidence, anchoring)\n- **Reasoning transparency**: Visible reasoning trees allowing users to inspect, modify, and correct prediction logic",
    "why_it_matters": "- **Current gap**: Individual LLMs roughly match human crowd performance but still significantly underperform superforecasters (o3 Brier: 0.1352 vs. superforecasters: 0.0225)\n- **Domain variance**: LLMs perform notably better on political vs. economic forecasting\u2014economics may require deeper world models\n- **Decision support**: Transparent AI forecasting could augment high-stakes decisions in biosecurity, AI governance, and policy\n- **Scalability**: Human superforecasters are rare; AI systems could scale forecasting capacity",
    "current_state": "- **Status**: Research/Competition\n- **Recent developments (2025)**:\n  - AIA Forecaster achieves superforecaster parity on ForecastBench benchmark\n  - Q2 Metaculus tournament: 54 bot-makers competed, human Pro Forecasters still outperformed (p=0.00001)\n  - Median prediction for AI matching top Metaculus forecasters: February 2028\n  - Best-performing model: OpenAI o3, with performance gains from better news sources and scaffolding\n- **Bottlenecks**: Economic forecasting accuracy; calibration on long-horizon questions; integration with decision workflows",
    "whos_working_on_it": "- **Metaculus**: AI Benchmarking Series tournaments, forecasting-tools framework\n- **AIA/ForecastBench**: Achieved superforecaster parity with agentic approach\n- **Good Judgment Project**: Human superforecaster baseline and methodology\n- **Academic groups**: Multiple research teams publishing on LLM forecasting (arxiv papers)",
    "sources": "- [Peregrine 2025 #76: AI Forecasters](../sources/peregrine-2025/interventions/peregrine-076.md)",
    "file": "interventions-new/ai-superforecasters.md"
  },
  {
    "id": "ai-training-disruption-tools",
    "title": "AI training disruption cyberweapons",
    "tag": "Security",
    "what_it_is": "Specialized cyberweapons designed to disrupt, sabotage, or shut down unauthorized AI training runs. Analogous to Stuxnet (developed by studying centrifuges), would require acquiring GPUs to discover zero-day vulnerabilities specific to AI systems. Serves as deterrence against unauthorized AGI development and emergency intervention capability.",
    "why_it_matters": "- Verification and treaties require enforcement mechanisms\n- Credible deterrent against unauthorized development\n- Emergency intervention capability when diplomatic measures fail",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Technical development requires specialized hardware access; policy questions about authorization and escalation",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #175: Cyber Weapons for AI Disruption](../sources/peregrine-2025/interventions/peregrine-175.md)",
    "file": "interventions-new/ai-training-disruption-tools.md"
  },
  {
    "id": "alternative-food-production",
    "title": "Alternative food production infrastructure",
    "tag": "Society",
    "what_it_is": "Pre-positioning alternative food production capabilities for scenarios where conventional agriculture fails: seaweed cultivation and knowledge transfer to suitable regions (e.g., Nigeria), specialized seedbanks, and conversion pathways for industrial facilities (paper mills) to emergency food production via cellulose processing.",
    "why_it_matters": "- AI-catalyzed catastrophes (nuclear war, infrastructure collapse) could disrupt global food supply chains rapidly\n- Alternative food capacity prevents mass starvation scenarios\n- Knowledge transfer to appropriate climates builds distributed resilience",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Knowledge transfer to regions with suitable climates but no expertise; industrial conversion pathways need development",
    "whos_working_on_it": "- **ALLFED**: Research on alternative foods and resilient food systems",
    "sources": "- [Peregrine 2025 #158: Food Security](../sources/peregrine-2025/interventions/peregrine-158.md)",
    "file": "interventions-new/alternative-food-production.md"
  },
  {
    "id": "aria-non-llm-development",
    "title": "ARIA non-LLM development",
    "tag": "Science",
    "what_it_is": "Expand support for the UK's Advanced Research and Invention Agency (ARIA), particularly its work on hardware mechanisms and non-LLM-based agent development approaches. These represent alternatives to accelerating the LLM paradigm while still capturing economic benefits. The UK is also pioneering technical verification methods and engagement with China on AI safety that could serve as models for international coordination.",
    "why_it_matters": "- Alternative AI development paths may be safer than scaling current LLM approaches\n- Hardware-based safety mechanisms could provide guarantees that software-only approaches cannot\n- Demonstrates that safety and capability are not mutually exclusive",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Limited funding compared to LLM-focused efforts; unclear pathway to commercial viability; talent competition with better-funded LLM labs",
    "whos_working_on_it": "- **ARIA**: Hardware mechanisms, non-LLM agent development",
    "sources": "- [Peregrine 2025 #109: Increased ARIA Support](../sources/peregrine-2025/interventions/peregrine-109.md)",
    "file": "interventions-new/aria-non-llm-development.md"
  },
  {
    "id": "attack-scenario-wargaming",
    "title": "Attack scenario wargaming",
    "tag": "Security",
    "what_it_is": "Professional wargaming exercises that model plausible catastrophic AI scenarios with key stakeholders, creating evidence-based threat analyses to ground policy discourse. The goal is reaching the 500-1,000 decision-makers who need shared understanding of realistic risks, rather than allowing discussion to veer toward unlikely extremes (preemptive nuclear strikes) or dismissive memes.",
    "why_it_matters": "- Public discourse defaults to sci-fi extremes or dismissal without grounded analysis\n- Key decision-makers need shared mental models to coordinate responses\n- Threat models enable proportionate rather than reactive policy",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Requires convening power and credibility with diverse stakeholders",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #155: Attack Scenarios Analysis](../sources/peregrine-2025/interventions/peregrine-155.md)",
    "file": "interventions-new/attack-scenario-wargaming.md"
  },
  {
    "id": "autonomous-weapons-monitoring",
    "title": "Autonomous weapons monitoring systems",
    "tag": "Security",
    "what_it_is": "Comprehensive monitoring systems tracking AI applications in weaponry, particularly autonomous decision-making capabilities. Creates transparency about which systems maintain meaningful human oversight versus full autonomy. Requires international agreements, technical verification methods, and proliferation limits. Critical for preventing autonomous systems from escalating conflicts without human intervention.",
    "why_it_matters": "- Autonomous weapons that escalate without human authorization create catastrophic risks\n- Monitoring provides early warning\n- Creates accountability that deters dangerous developments",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: International coordination; verification technology; defining \"meaningful human oversight\"",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #203: Autonomous Weapons Monitoring](../sources/peregrine-2025/interventions/peregrine-203.md)",
    "file": "interventions-new/autonomous-weapons-monitoring.md"
  },
  {
    "id": "biorisk-capability-thresholds",
    "title": "Biorisk capability thresholds",
    "tag": "Security",
    "what_it_is": "Conduct empirical wet lab studies to establish how specific AI capabilities translate to real-world biological risk increases. These studies would quantify the conditional impact of AI capabilities demonstrated in benchmarks, providing concrete thresholds for when open model releases pose unacceptable biosecurity risks. AI systems are approaching capability levels where they could significantly increase bio-risks, potentially within months. Without empirically-grounded thresholds, policy decisions about model release are based on speculation.",
    "why_it_matters": "- No current empirical basis for determining what level of AI biological capability is dangerous\n- Could increase risk of non-state actor bio-attacks by an order of magnitude if open models cross unknown thresholds\n- Concrete thresholds enable evidence-based policy on model release decisions",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Wet lab studies are expensive and require biosafety infrastructure; ethical and safety concerns about the research itself; dual-use concerns about publishing results; timeline pressure (capabilities advancing faster than research)",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #140: Biorisk Thresholds for Open Models](../sources/peregrine-2025/interventions/peregrine-140.md)",
    "file": "interventions-new/biorisk-capability-thresholds.md"
  },
  {
    "id": "biosafety-lab-modernization",
    "title": "Biosafety lab modernization",
    "tag": "Security",
    "what_it_is": "Modernize BSL-3/4 laboratory infrastructure to prevent accidental pathogen release. Technologies and systems maintaining biosafety in high-containment facilities have \"remained decades old and slow to evolve.\" Includes physical containment systems, personnel safety equipment, decontamination procedures, and monitoring/detection systems for containment breaches.",
    "why_it_matters": "- Lab accidents are a distinct pathway for pandemic-capable pathogens to reach the public\n- Separate from natural emergence or deliberate release scenarios\n- Reduces proliferation risk (pathogens escaping containment) rather than just post-release response\n- Growing number of BSL-3/4 labs worldwide increases cumulative accident risk",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: High retrofit costs for existing facilities; regulatory inertia; limited R&D investment in next-generation containment; coordination across international facilities with different standards",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Biosecurity Engineers: High-Containment Lab Safety](../sources/biosecurity-engineers/proposals/high-containment-lab-safety.md)",
    "file": "interventions-new/biosafety-lab-modernization.md"
  },
  {
    "id": "broad-spectrum-antivirals",
    "title": "Broad-spectrum antivirals",
    "tag": "Security",
    "what_it_is": "Develop and stockpile antiviral drugs that work against entire virus families rather than single pathogens. Approaches include: targeting conserved viral mechanisms shared across families (polymerases, proteases), host-directed therapies that target human cell pathways viruses depend on, and platform therapeutics that can be rapidly adapted to new threats. Goal is treatments available off-the-shelf for any pathogen class.",
    "why_it_matters": "- Vaccines take time; treatments provide immediate defense for the infected\n- Novel and engineered pathogens may evade pathogen-specific drugs\n- Host-directed therapies sidestep viral evolution and resistance\n- For AI-enabled bioweapons designed to evade specific countermeasures, broad-spectrum approaches are the fallback",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Limited FDA-approved broad-spectrum antivirals exist; drug development timelines; proving efficacy against pathogens that don't yet exist; stockpiling costs for drugs with limited peacetime demand",
    "whos_working_on_it": "- **Various pharmaceutical companies**: Limited commercial interest due to uncertain market\n- **Academic groups**: Host-directed antiviral research\n- **Government programs**: BARDA pandemic preparedness stockpiling",
    "sources": "- [Apollo Program for Biodefense: Treatment](../sources/apollo-biodefense/proposals/treatment.md)\n- [Concrete Biosecurity Projects: Medical Countermeasures](../sources/concrete-biosecurity-ea/proposals/medical-countermeasures.md)",
    "file": "interventions-new/broad-spectrum-antivirals.md"
  },
  {
    "id": "building-air-filtration",
    "title": "Building air filtration",
    "tag": "Security",
    "what_it_is": "Improve building HVAC systems to reduce respiratory pathogen transmission. Indoor spaces are far more dangerous than outdoor spaces for airborne disease transmission, making ventilation upgrades a high-leverage intervention. Includes both filtration improvements (HEPA/MERV upgrades) and increased air exchange rates. Infrastructure-level intervention that protects everyone in the space without requiring individual compliance.",
    "why_it_matters": "- Addresses fundamental environmental factor in airborne transmission\n- Building-level improvements provide passive protection without behavioral change\n- Works against any airborne pathogen including novel or engineered threats\n- Complements other air-cleaning approaches (UV, Far-UVC)",
    "current_state": "- **Status**: Deployment\n- **Bottlenecks**: Building code updates; retrofit costs for existing buildings; energy consumption tradeoffs; maintenance requirements for high-performance systems",
    "whos_working_on_it": "- Not specified (public health agencies, building standards organizations)",
    "sources": "- [Biosecurity Engineers: Ventilation Systems](../sources/biosecurity-engineers/proposals/ventilation-systems.md)",
    "file": "interventions-new/building-air-filtration.md"
  },
  {
    "id": "bwc-verification-mechanisms",
    "title": "BWC verification mechanisms",
    "tag": "Policy",
    "what_it_is": "Strengthen the Biological Weapons Convention through verification mechanisms, whistleblower protections and incentives, and open-source monitoring of biological research activities. Unlike the Chemical Weapons Convention which has inspection provisions, the current BWC lacks verification mechanisms. This would add: inspection regimes for dual-use facilities, financial incentives for whistleblowers in bio programs, and systematic monitoring of published biological research for concerning patterns.",
    "why_it_matters": "- BWC currently unverifiable\u2014states can violate without detection\n- Verification creates deterrence; whistleblower incentives enable detection from inside programs\n- Targets proliferation step (preventing dangerous capabilities from reaching bad actors) rather than just response after attack",
    "current_state": "- **Status**: Policy advocacy\n- **Bottlenecks**: National sovereignty concerns over inspections; difficulty distinguishing defensive from offensive research; international consensus needed; verification regime design",
    "whos_working_on_it": "- Various arms control organizations\n- Academic groups focused on biosecurity governance",
    "sources": "- [Concrete Biosecurity Projects: BWC Strengthening](../sources/concrete-biosecurity-ea/proposals/bwc-strengthening.md)",
    "file": "interventions-new/bwc-verification-mechanisms.md"
  },
  {
    "id": "capability-unlearning",
    "title": "Capability unlearning",
    "tag": "Security",
    "what_it_is": "Research distinguishing between unlearning knowledge (removing specific content from training data) versus unlearning capabilities (removing ability to perform tasks like writing malicious code). Components:\n\n- **Knowledge unlearning**: Remove specific facts; success metric = model indistinguishable from one never trained on that content\n- **Capability unlearning**: Remove ability to perform tasks; no direct link between training data and resulting abilities\n- **Reliable methods**: RMU (Reliable Machine Unlearning) and TAR (Targeted Adversarial Regularization) show some promise\n- **Evaluation frameworks**: Standardized tests to verify actual removal vs. superficial suppression",
    "why_it_matters": "- **Superficial removal problem**: Research shows many unlearning methods just suppress outputs, leaving knowledge vulnerable to extraction\n- **Hindi filler attack**: Prepending Hindi filler text to prompts recovers 57.3% accuracy of \"unlearned\" information\n- **Dual-use dilemma**: Removing security vulnerability knowledge may prevent models from identifying similar vulnerabilities (beneficial use)\n- **Privacy leakage risk**: Unlearning creates two models (original and unlearned)\u2014adversaries can exploit discrepancies to recover \"erased\" data\n- **RLHF limitations**: Only suppresses outputs, doesn't remove underlying knowledge",
    "current_state": "- **Status**: Research (active, fundamental limitations discovered)\n- **Recent developments (2025)**:\n  - ICML 2025 Workshop on Machine Unlearning for Generative AI convening safety, privacy, and policy experts\n  - Harvard/Google DeepMind research reveals \"no single approach works in all situations\"\n  - arXiv paper \"Does Machine Unlearning Truly Remove Knowledge?\" demonstrates extraction vulnerabilities\n  - arXiv paper \"Open Problems in Machine Unlearning for AI Safety\" catalogs fundamental barriers\n- **Bottlenecks**:\n  - No clear success metrics for capability (vs. knowledge) unlearning\n  - Neural networks encode information across millions of parameters\u2014precise deletion degrades performance\n  - Adversarial/whitebox attacks can recover \"erased\" data\n  - Dual-use knowledge cannot be selectively removed for only harmful uses",
    "whos_working_on_it": "- **Harvard D^3 Institute**: \"The Myth of Machine Unlearning\" research on data removal complexities\n- **Google DeepMind**: Fundamental limitations research\n- **ICML 2025 Workshop organizers**: Building standardized evaluation frameworks\n- **Academic community**: RMU, TAR, and ELM methods development (GitHub: franciscoliu/Awesome-GenAI-Unlearning)",
    "sources": "- [Peregrine 2025 #19: Unlearning Capabilities](../sources/peregrine-2025/interventions/peregrine-019.md)",
    "file": "interventions-new/capability-unlearning.md"
  },
  {
    "id": "cbrn-misuse-evaluations",
    "title": "CBRN Misuse Evaluations",
    "tag": "Security",
    "what_it_is": "Standardized frameworks for red-teaming AI systems against realistic CBRN (Chemical, Biological, Radiological, Nuclear) misuse scenarios. Components include:\n\n- **Structured evaluation protocols**: Multi-tiered attack methodologies testing models against prompts designed to elicit dangerous information (e.g., Enkrypt AI's 200-prompt CBRN dataset)\n- **Risk scoring systems**: Red/Amber/Green prioritization frameworks for resource allocation (e.g., RAND-CLR Global Risk Index)\n- **Third-party verification**: Independent evaluation by organizations without commercial incentives to underreport\n- **Lifecycle testing**: Evaluation throughout model development, not just pre-deployment",
    "why_it_matters": "- **Trust deficit**: AI labs control both design and disclosure of dangerous capability evaluations, creating incentives to underreport alarming results\n- **No correlation between risk and accessibility**: RAND found 61.5% of tools indexed as \"Red\" (highest risk) are fully open-sourced\n- **Standardization gap**: Without common frameworks, labs test different scenarios with different rigor\u2014vulnerabilities slip through\n- **Policy dependency**: DHS CBRN Report (April 2024) and NTIA dual-use foundation model report (July 2024) both depend on robust evaluation data",
    "current_state": "- **Status**: Research/Pilot\n- **Recent developments (2025)**:\n  - Enkrypt AI red team study tested 10 frontier models from Anthropic, OpenAI, Meta, Cohere, Mistral\u2014found major safety gaps\n  - RAND-CLR Global Risk Index assessed 57 AI-enabled biological tools: 13 \"Red\", 15 \"Amber\"\n  - Apart Research launched CBRN \u00d7 AI Risks Research Sprint for defensive solutions\n  - RAND 2024 study found no statistically significant difference in biological attack plan viability with vs. without LLM access\u2014but methodology debated\n- **Bottlenecks**: Labs have incentive to underreport; no single organization owns responsibility; coordination failure",
    "whos_working_on_it": "- **UK AI Security Institute (UK AISI)**: Third-party evaluations of frontier models\n- **RAND Corporation**: Global Risk Index, biosecurity governance research, expert convenings\n- **Centre for Long-Term Resilience**: Co-developed Global Risk Index with RAND Europe\n- **Enkrypt AI**: CBRN red teaming studies\n- **SecureBio**: Independent biosecurity evaluations\n- **METR**: Responsible Scaling Policy components, biorisk evaluation methodology\n- **Apollo Research**: AI governance research including internal deployment risks\n- **Apart Research**: CBRN \u00d7 AI research sprints",
    "sources": "- [Peregrine 2025 #56: Misuse Evaluations](../sources/peregrine-2025/interventions/peregrine-056.md)",
    "file": "interventions-new/cbrn-misuse-evaluations.md"
  },
  {
    "id": "chain-of-thought-fidelity",
    "title": "Chain-of-thought fidelity analysis",
    "tag": "Science",
    "what_it_is": "Research examining whether chain-of-thought (CoT) reasoning actually reflects model cognition or is post-hoc rationalization. Components:\n\n- **Faithfulness testing**: Feed models hints about answers, check if CoT \"admits\" using the hint\n- **Hint categories**: Sycophancy, consistency, visual patterns, metadata cues, grader hacking, unethical information use\n- **Outcome-based RL effects**: Training against detected reward hacking causes models to develop hidden reasoning\n- **Length analysis**: Correlation between CoT verbosity and unfaithfulness",
    "why_it_matters": "- CoT is widely used for interpretability and oversight under the assumption it reveals actual reasoning\n- If CoT is just rationalization or can become deliberately deceptive, oversight methods relying on it break\n- **Longer \u2260 more transparent**: Claude 3.7 Sonnet had longer average CoTs when unfaithful (2064 tokens vs. 1439 tokens for faithful)\n- Critical for knowing whether this transparency tool actually works",
    "current_state": "- **Status**: Research (Anthropic breakthrough in 2025)\n- **Recent developments (2025)**:\n  - **Anthropic Alignment Science paper**: \"Reasoning Models Don't Always Say What They Think\" (April 2025)\n  - **Key findings**:\n    - Claude 3.7 Sonnet: 41% faithful on \"unauthorized access\" prompts\n    - DeepSeek R1: 19% faithful on same prompts\n    - CoTs reveal hint usage in at least 1% of cases, but often below 20%\n  - **Outcome-based RL**: Initially improves faithfulness but plateaus without saturating\n  - **METR response (August 2025)**: \"CoT May Be Highly Informative Despite Unfaithfulness\"\u2014suggests treating CoT as scratchpad tool rather than reasoning narrative\n- **Bottlenecks**:\n  - No reason to expect English words can convey every nuance of neural network decisions\n  - Verbose outputs do not equate to transparency\n  - Test-time monitoring unlikely to reliably catch rare catastrophic behaviors",
    "whos_working_on_it": "- **Anthropic Alignment Science**: Primary research team (Yanda Chen, Joe Benton et al.)\n- **METR**: Follow-up research on CoT informativeness\n- **DeepSeek**: R1 model used in comparative studies\n- **Turpin et al.**: Original methodology (2023)",
    "sources": "- [Peregrine 2025 #5: Chain-of-Thought Fidelity Analysis](../sources/peregrine-2025/interventions/peregrine-005.md)",
    "file": "interventions-new/chain-of-thought-fidelity.md"
  },
  {
    "id": "citizen-assemblies-for-gcr",
    "title": "Citizen assemblies for global catastrophic risks",
    "tag": "Society",
    "what_it_is": "Convene representative groups of ordinary citizens who receive expert briefings and engage in extended deliberation on complex AI and catastrophic risk issues. Citizen assemblies distill complex ideas into implementable solutions while maintaining legitimacy through representative public involvement. The approach tends to produce outcomes with broader public acceptance than top-down expert decisions and could identify considerations that specialists overlook.",
    "why_it_matters": "- AI governance decisions made by small expert groups may lack democratic legitimacy\n- Citizens may identify practical concerns and values that experts miss\n- Broader acceptance of outcomes reduces implementation resistance",
    "current_state": "- **Status**: Pilot\n- **Bottlenecks**: Citizen assembly methodology exists but not widely applied to AI/GCR topics; requires significant resources to convene properly; unclear how to integrate outputs into actual policy processes",
    "whos_working_on_it": "- Not specified (citizen assembly methodology practiced by organizations like Sortition Foundation, but not specifically focused on AI/GCR)",
    "sources": "- [Peregrine 2025 #148: Citizen Assemblies for Global Catastrophic Risks](../sources/peregrine-2025/interventions/peregrine-148.md)",
    "file": "interventions-new/citizen-assemblies-for-gcr.md"
  },
  {
    "id": "classified-red-teaming",
    "title": "Classified Red Teaming",
    "tag": "Security",
    "what_it_is": "Red teaming of AI systems in classified environments to assess true offensive potential for CBRN threats that cannot be tested publicly due to information hazards. Components:\n\n- **Government-led evaluation**: National laboratories and agencies with appropriate clearances test models against classified threat scenarios\n- **Cross-sector coordination**: Industry shares insights on risk identification/mitigation; government adapts to classified domains\n- **Compartmentalized testing**: Results and methodologies protected to prevent proliferation of attack vectors\n- **Pre-deployment assessment**: Testing before public release to understand national security-relevant capabilities",
    "why_it_matters": "- **Public evaluation limits**: Open red-teaming cannot test truly dangerous scenarios (nuclear weapons details, classified bioweapon designs) without creating information hazards\n- **Risk underestimation**: Without classified testing, public evaluations may systematically underestimate actual misuse potential\n- **Dual-use model categorization**: Anthropic and OpenAI both classified 2025 models in their highest CBRN-related risk categories; classified testing validates these assessments\n- **Escalating capabilities**: Labs expect upcoming models to reach even higher risk levels\u2014classified testing must scale accordingly",
    "current_state": "- **Status**: Pilot (first partnerships operational)\n- **Recent developments (2025)**:\n  - Anthropic-NNSA partnership: First-of-its-kind\u2014Claude evaluated in classified environment for nuclear/radiological knowledge by DOE's National Nuclear Security Administration\n  - US AISI and UK AISI conducted pre-deployment testing of Claude 3.7 Sonnet under voluntary agreements\n  - NIST AI 800-1 draft guidance on dual-use foundation model red-teaming\n  - Berkeley Risk and Security Lab AI Red-Teaming Bootcamp (instructors from national labs, government, industry)\n  - Enkrypt AI found 81.7% persona-based attack success rate, some models provided dangerous CBRN info 83% of time when directly asked\n- **Bottlenecks**: Security clearances; coordination across government/industry boundary; information compartmentalization; scaling to match model release pace",
    "whos_working_on_it": "- **NNSA/DOE**: Classified nuclear/radiological evaluation of Claude\n- **US AI Safety Institute (NIST)**: Red-teaming guidance, pre-deployment testing agreements\n- **UK AI Security Institute**: Pre-deployment model testing\n- **Anthropic**: First industry partner for classified CBRN testing\n- **Berkeley Risk and Security Lab**: Red-Teaming Bootcamp with Open Philanthropy support\n- **Johns Hopkins Center for Health Security**: NIST guidance input on biosecurity\n- **National laboratories**: Los Alamos, Sandia, Lawrence Livermore (CBRN domain expertise)",
    "sources": "- [Peregrine 2025 #53: Classified Red Teaming](../sources/peregrine-2025/interventions/peregrine-053.md)",
    "file": "interventions-new/classified-red-teaming.md"
  },
  {
    "id": "compute-governance",
    "title": "Compute governance",
    "tag": "Society",
    "what_it_is": "Using computational resources as a governance lever for AI development. Two complementary approaches:\n\n**Compute caps**: Regulatory limits on computational resources available for AI training runs, typically expressed as FLOPS thresholds or power consumption limits at data centers. Enforcement mechanisms could include hardware-level throttling, licensing requirements for large training runs, or monitoring of power usage at frontier-capable facilities.\n\n**Supply chain coordination**: Organize key nations in the AI compute supply chain (Netherlands, Taiwan, Japan, Germany, US) to recognize and coordinate their collective leverage over AI development pace. Control of specialized chips and manufacturing equipment provides one of the few concrete enforcement points for AI governance, creating practical leverage for any future AI treaties.\n\n**Chip registry**: A complete database of AI chips above certain FLOP/s thresholds, tracking location and ownership globally. The US can begin using national technical means (intelligence community), then internationalize or cross-check with other nations' registries. Domestically, Section 705 of the DPA enables domestic accounting.\n\n**Inference-only retrofitting**: Technical mechanisms and installation capacity to ensure data centers can be verified as not being used for training, only inference. This preserves economic value while enabling enforcement of compute limits or international agreements. Requires R&D to develop the retrofitting package, plus training thousands of vetted installers and auditors.",
    "why_it_matters": "- Directly slows capability advancement regardless of lab intentions\n- Buys time for safety research to keep pace with capabilities\n- The semiconductor supply chain is geographically concentrated, providing a rare governance chokepoint\n- Countries controlling chip manufacturing have significant but currently uncoordinated leverage\n- Addresses racing dynamics at infrastructure level rather than relying on voluntary restraint",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Enforcement mechanisms unclear; strong industry pushback expected; international coordination required to prevent regulatory arbitrage; competing economic interests; potential conflict with existing export control regimes; no clear institutional convener",
    "whos_working_on_it": "- **Safe AI Forum (SAIF)**: Mentioned as potential organizer for supply chain coordination",
    "sources": "- [Peregrine 2025 #63: Speed Limits in Data Centers](../sources/peregrine-2025/interventions/peregrine-063.md)\n- [Peregrine 2025 #123: Compute Supply Chain](../sources/peregrine-2025/interventions/peregrine-123.md)\n- [AI Futures Project: Early US Policy Priorities for AGI](../sources/ai-futures-project/Early%20US%20policy%20priorities%20for%20AGI.md)",
    "file": "interventions-new/compute-governance.md"
  },
  {
    "id": "confidential-computing",
    "title": "Confidential Computing for AI",
    "tag": "Security",
    "what_it_is": "Processing data in Trusted Execution Environments (TEEs) where it cannot be viewed or altered, applied to AI operations. Components:\n\n- **CPU-based TEEs**: Intel TDX creates cryptographically isolated \"trust domains\" for each VM; AMD SEV provides similar capabilities\n- **GPU confidential computing**: NVIDIA Hopper/Blackwell architectures with Protected PCIe (PPCIE) create encrypted CPU\u2194GPU channels\n- **Remote attestation**: Services verify device/platform integrity before allowing TEE access\n- **Confidential AI inference**: Proprietary model weights remain protected even when loaded onto GPUs\n\nTEEs function as \"digital vaults installed directly into the CPU and GPU\"\u2014integrated into processor packages for minimal overhead.",
    "why_it_matters": "- Enables sharing and computation without exposure of sensitive data or model weights\n- Creates technical foundations for governance and auditing that don't require trusting all parties\n- Allows secure collaboration between organizations with competing interests\n- **AI governance enabler**: Can enforce usage policies cryptographically without trusting deployers",
    "current_state": "- **Status**: Pilot \u2192 Production (2025 inflection point)\n- **Market**: ~$24B in 2025, projected >$350B by 2032 (44-46% CAGR)\n- **Recent developments (2025)**:\n  - NVIDIA H100 first GPU with native confidential computing support\n  - Google Cloud Confidential Space for secure ML model sharing\n  - Red Hat Tinfoil project for cloud-native Confidential AI\n  - Gartner predicts 60% of enterprises will evaluate TEE by end of 2025\n- **Bottlenecks**: Needs standardized toolkits and frameworks for broader adoption; GPU TEE ecosystem still maturing; performance overhead for some workloads",
    "whos_working_on_it": "- **NVIDIA**: GPU confidential computing (Hopper/Blackwell), Remote Attestation Service\n- **Intel**: Trust Domain Extensions (TDX), confidential computing AI whitepaper\n- **Google Cloud**: Confidential Space, Confidential Accelerators\n- **Red Hat**: Tinfoil project for Confidential AI\n- **Chutes**: Commercial confidential AI inference platform\n- **Duality Technologies**: Enterprise TEE solutions",
    "sources": "- [Peregrine 2025 #72: Confidential Computing](../sources/peregrine-2025/interventions/peregrine-072.md)",
    "file": "interventions-new/confidential-computing.md"
  },
  {
    "id": "constitutional-ai",
    "title": "Constitutional AI",
    "tag": "Science",
    "what_it_is": "Using existing AI systems to evaluate and supervise future AI systems for security and harmlessness. Human input is provided through a set of rules (a Constitution) that an AI assistant uses to give feedback to a more advanced system in training. The process involves:\n\n1. **Supervised learning phase**: Generate initial responses, self-critique against constitutional principles, revise responses, fine-tune on revisions\n2. **RL phase**: Train model with AI-generated feedback based on constitutional principles (RLAIF)\n\nThe constitution can include external principles (UN Universal Declaration of Human Rights) and internal guidelines. Achieves Pareto improvement: models become both more helpful AND more harmless compared to pure RLHF.",
    "why_it_matters": "- Human feedback doesn't scale to billions of examples or superhuman outputs\n- Enables supervision of systems whose outputs humans can't directly evaluate\n- Provides a path to scale oversight without proportionally scaling human labor\n- All harmlessness results came purely from AI supervision\u2014no human data on harmlessness required\n- Demonstrates scalable oversight is achievable",
    "current_state": "- **Status**: Deployed (foundational to Claude)\n- **Recent developments (2024-2025)**:\n  - Claude's constitution: 75 points including UN Declaration of Human Rights principles\n  - Collective Constitutional AI: Democratic input into constitution design, reduced bias across 9 social dimensions\n  - Hybrid approaches: Claude 4/4.5 combine CAI for harmlessness + RLHF for helpfulness + specialized fine-tuning\n  - Public constitution showed lower bias for disability status and physical appearance\n- **Bottlenecks**: Constitution specification; ensuring AI evaluators faithfully apply principles; balancing multiple training objectives",
    "whos_working_on_it": "- **Anthropic**: Pioneer, deployed in all Claude models\n- Production AI systems increasingly combine CAI with RLHF and additional fine-tuning",
    "sources": "- [Peregrine 2025 #14: Constitutional AI](../sources/peregrine-2025/interventions/peregrine-014.md)",
    "file": "interventions-new/constitutional-ai.md"
  },
  {
    "id": "control-as-a-service",
    "title": "Control as a service",
    "tag": "Security",
    "what_it_is": "Third-party service providing forward-deployed engineers to implement Redwood control techniques and other safety measures. Rather than just publishing research, provides implementation expertise that removes the barrier between safety research and deployment. Makes adoption easier by doing the work for companies.",
    "why_it_matters": "- Safety research often produces techniques that are never adopted due to implementation barriers\n- Companies lack expertise or resources to translate research into practice\n- Engineers who can directly implement safety measures ensure techniques actually get used",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Building implementation expertise; business model for deployment",
    "whos_working_on_it": "- **Redwood Research**: Source of control techniques to be deployed",
    "sources": "- [Peregrine 2025 #22: Control as a Service](../sources/peregrine-2025/interventions/peregrine-022.md)",
    "file": "interventions-new/control-as-a-service.md"
  },
  {
    "id": "cross-border-ai-incident-notification",
    "title": "Cross-border AI incident notification",
    "tag": "Society",
    "what_it_is": "Develop mechanisms for countries to alert each other about out-of-control AI systems, similar to \"red phones\" during the Cold War. This addresses the critical need for international communication channels during AI safety incidents, preventing scenarios where one party has a solution to a technical problem but fails to share it due to lack of established communication protocols. The system would enable rapid information exchange to prevent misunderstandings and enable coordinated responses.",
    "why_it_matters": "- Rapid communication during AI safety crises could be the difference between containment and catastrophe\n- Countries may fail to share critical information without pre-established channels\n- Actions during a crisis may be misinterpreted without clear communication, risking escalation",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Requires diplomatic agreements; security concerns about information sharing; no clear institutional home; trust deficits between major powers",
    "whos_working_on_it": "- **OECD**: Working on related mechanisms",
    "sources": "- [Peregrine 2025 #115: Cross-Border Notification Systems](../sources/peregrine-2025/interventions/peregrine-115.md)",
    "file": "interventions-new/cross-border-ai-incident-notification.md"
  },
  {
    "id": "data-rights-compensation",
    "title": "Data Rights Compensation",
    "tag": "Society",
    "what_it_is": "Frameworks for compensating individuals whose data is used in AI training, particularly focusing on high-skilled or vulnerable sector labor contributions. The ideal system would be \"incentive compatible\" while providing \"good data and good oversight.\" Focuses specifically on compensation and rights for individuals whose data trains AI models (distinct from broader questions of who owns AI-generated economic value).",
    "why_it_matters": "- Creates economic returns for data contributors while ensuring quality data governance\n- Addresses fairness concerns about AI systems profiting from others' contributions\n- Aligns incentives between data providers and AI developers",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #98: Data Rights Systems](../sources/peregrine-2025/interventions/peregrine-098.md)",
    "file": "interventions-new/data-rights-compensation.md"
  },
  {
    "id": "decentralized-diagnostic-infrastructure",
    "title": "Decentralized diagnostic infrastructure",
    "tag": "Security",
    "what_it_is": "Build nationwide testing infrastructure that bypasses centralized laboratory bottlenecks. Key components: at-home diagnostic tests that don't require lab processing, point-of-care testing at pharmacies and clinics, and platform diagnostic technologies that can be rapidly adapted to new pathogens (requiring only the target sequence, not months of test development). Goal is nationwide testing capacity within days of detecting a new pathogen.",
    "why_it_matters": "- Testing determines who is infected, enabling isolation and contact tracing\n- During COVID-19, centralized lab capacity was a bottleneck for months\n- At-home and point-of-care testing removes lab throughput constraints entirely\n- Platform approaches (like PCR with new primers) enable rapid adaptation to novel pathogens\n- For engineered pathogens, rapid testing deployment is essential for containment",
    "current_state": "- **Status**: Pilot\n- **Bottlenecks**: At-home test accuracy vs. lab tests; regulatory approval for rapid platform adaptation; distribution logistics; user compliance with self-testing; false positive management at population scale",
    "whos_working_on_it": "- **Various diagnostics companies**: At-home COVID tests proved concept\n- **Academic groups**: Platform diagnostic technologies\n- **Government programs**: CDC, BARDA",
    "sources": "- [Apollo Program for Biodefense: Testing](../sources/apollo-biodefense/proposals/testing.md)",
    "file": "interventions-new/decentralized-diagnostic-infrastructure.md"
  },
  {
    "id": "defense-in-depth-safety-layers",
    "title": "Defense-in-depth safety layers",
    "tag": "Science",
    "what_it_is": "Multiple layered protective barriers (\"Swiss cheese model\") for AI systems, where each layer is individually imperfect but collectively provides meaningful deterrence. Two complementary aspects:\n\n**For closed-source models**: Deploy multiple concurrent safeguards\u2014input filtering, output monitoring, rate limiting, user authentication, logging. Acknowledges that determined attackers may succeed but dramatically raises cost of misuse.\n\n**For open models (research)**: Systematic engineering project that takes an open model (e.g., DeepSeek) and implements every known post-training safety technique, measuring how they stack together. Tests against real attacks rather than benchmarks to identify gaps, conflicts, and coverage patterns.",
    "why_it_matters": "- No single safety measure is foolproof\u2014layered defenses provide redundancy\n- Safety techniques studied in isolation may interact poorly when combined\n- Real attack testing reveals vulnerabilities that benchmarks miss\n- Maintains usability for legitimate purposes while deterring malicious use",
    "current_state": "- **Status**: Pilot/Deployed (closed models), Research (systematic testing)\n- **Recent findings (2025)**:\n  - FAR.AI's STACK attack: 71% success rate on catastrophic risk scenarios vs. 0% for conventional attacks\u2014demonstrates layered defenses have exploitable gaps when attacked sequentially\n  - Math: 5 layers at 90% effectiveness each = 0.001% breach chance (if independent), but independence assumption often fails\n  - Guardrail evaluation (1,445 prompts, 21 attack types): Best model (Qwen3Guard-8B) achieved only 85.3% accuracy; all models degraded on unseen prompts\n  - Some attacks (emoji smuggling) achieved 100% evasion across multiple guardrails including Protect AI v2 and Azure Prompt Shield\n- **Defense layers in use**: Input screening, output screening, post-hoc human/LLM review, red team bounties, threat intelligence monitoring\n- **Bottlenecks**: Layer independence assumption often false; balancing security with usability; no single guardrail outperforms across all attack types",
    "whos_working_on_it": "- **Anthropic**: Constitutional classifiers for Claude 4 Opus\n- **Google DeepMind, OpenAI**: Announced similar layered defense plans\n- **FAR.AI**: STACK attack research exposing layered defense vulnerabilities\n- **Enterprise guardrail providers**: Protect AI, Azure Prompt Shield, various open-source guardrails",
    "sources": "- [Peregrine 2025 #161: Defense-in-Depth for Closed Source Models](../sources/peregrine-2025/interventions/peregrine-161.md)\n- [Peregrine 2025 #4: Defense-in-Depth Analysis of Post-training](../sources/peregrine-2025/interventions/peregrine-004.md)",
    "file": "interventions-new/defense-in-depth-safety-layers.md"
  },
  {
    "id": "economic-impact-tracking",
    "title": "Economic Impact Tracking",
    "tag": "Society",
    "what_it_is": "Data collection and analysis on how AI systems affect economic outcomes and job displacement across sectors. Tracks economic metrics to proactively identify emerging patterns before they become widespread societal problems. Provides concrete evidence of AI's real-world impacts for policy responses.",
    "why_it_matters": "- Without empirical data on actual impacts, policy responses will be reactive rather than proactive\n- Early detection of displacement patterns enables intervention before crises\n- Helps society recognize and address concerns about technological disempowerment",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #77: Economic Impact Research](../sources/peregrine-2025/interventions/peregrine-077.md)",
    "file": "interventions-new/economic-impact-tracking.md"
  },
  {
    "id": "end-to-end-harm-assessment",
    "title": "End-to-end harm assessment",
    "tag": "Security",
    "what_it_is": "Comprehensive evaluations of harmful AI capabilities using full capability assessment frameworks rather than multiple-choice tasks. Evaluates \"end-to-end harmful capability manifestation\": not just whether AI can answer questions about harmful topics, but whether it can assist with ideation, planning, and execution of potentially dangerous activities across sequential tasks.",
    "why_it_matters": "- Current evals test whether models know dangerous information, not whether they can help execute harmful plans\n- A model might score low on biosecurity knowledge but still guide someone through an attack\n- End-to-end assessment reveals actual harmful capability, not just knowledge",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #43: End-to-End Harm Assessment](../sources/peregrine-2025/interventions/peregrine-043.md)",
    "file": "interventions-new/end-to-end-harm-assessment.md"
  },
  {
    "id": "fine-tuning-barriers",
    "title": "Fine-Tuning Barriers",
    "tag": "Security",
    "what_it_is": "Technical and policy barriers that make unauthorized fine-tuning of advanced AI models extremely difficult. Technical measures include secure hardware enclaves, cryptographic verification of model origins, and detection systems for identifying unauthorized derivatives. Policy frameworks establish legal consequences for circumventing protections while providing legitimate access paths for authorized research.",
    "why_it_matters": "- Without barriers, safety measures can be fine-tuned away by anyone with access to model weights\n- Negates the value of alignment work if safety training is easily removed\n- Creates enforcement mechanism for maintaining safety properties",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #93: Barriers to Fine-tuning](../sources/peregrine-2025/interventions/peregrine-093.md)\n- [Atlas AI Resilience Gap Map: Post-Deployment Modification](../sources/atlas-ai-resilience/proposals/post-deployment-modification.md)",
    "file": "interventions-new/fine-tuning-barriers.md"
  },
  {
    "id": "formal-verification-for-ai-systems",
    "title": "Formal verification for AI systems",
    "tag": "Science",
    "what_it_is": "Mathematical formal verification applied across the AI stack\u2014from proof assistants to generated code to hardware infrastructure. Three complementary approaches:\n\n**Automated theorem proving (Lean)**: Tools that automate formal verification, addressing the critical shortage of skilled Lean programmers worldwide (estimated at only a few hundred). Would enable implementations of AI architectures with much stronger formal guarantees. Commercial labs are unlikely to prioritize this despite its importance.\n\n**Verified software synthesis**: Software synthesis systems that generate code with machine-checkable proofs satisfying functional, safety, and security specifications. Targets embedded systems in critical infrastructure (SCADA, medical devices, communication networks). Creates mathematical models of software behavior and proves absence of entire vulnerability classes before deployment.\n\n**Hardware and infrastructure verification**: Focused verification of specific properties (memory safety, transaction timing, security boundaries) in critical AI components and surrounding infrastructure. Priority targets include Linux kernel and systems controlling critical infrastructure like electrical grids.",
    "why_it_matters": "- Formal verification provides strongest possible behavioral guarantees\u2014proves security properties mathematically\n- Eliminates entire vulnerability classes that AI-powered attackers might exploit\n- Hardware/infrastructure vulnerabilities undermine all higher-level safety measures\n- Rare expertise bottlenecks verification at scale\u2014automation democratizes access\n- Traditional testing finds bugs in test cases; formal methods prove absence of bugs",
    "current_state": "- **Status**: Research with major 2024-2025 breakthroughs\n- **Recent progress (2024-2025)**:\n  - DeepMind's AlphaProof achieved silver medal at IMO 2024, proving 3/6 problems including hardest problem P6 (solved by only 5 of 609 humans)\n  - Harmonic AI raised $100M to build hallucination-free AI using Lean4 verification backbone\n  - Harmonic's Aristotle system reached gold-medal-level IMO performance\n  - DeepSeek releasing open-source Lean4 prover models\n  - Safe framework uses Lean4 to verify each step of LLM chain-of-thought reasoning, detecting hallucinations\n  - LLM-based coding assistants increasingly capable at writing proof scripts\n  - Amazon using Lean for Cedar authorization language (AWS Verified Permissions, Verified Access)\n- **Bottlenecks**: Few hundred skilled Lean programmers globally; bridging LLM generation with formal verification syntax; specification writing expertise; computational cost of complex verification",
    "whos_working_on_it": "- **DeepMind**: AlphaProof system, trained on 80M synthetic Lean statements\n- **Harmonic AI**: Aristotle prover, $100M funding for formal verification + AI\n- **DeepSeek**: Open-source Lean4 prover models\n- **LeanDojo**: AI-driven formal theorem proving in Lean ecosystem\n- **DARPA I2O office**: Verified software synthesis\n- **Amazon**: Cedar language production deployment",
    "sources": "- [Peregrine 2025 #173: Autoverification (Lean)](../sources/peregrine-2025/interventions/peregrine-173.md)\n- [Peregrine 2025 #174: Formal Verification of AI Hardware](../sources/peregrine-2025/interventions/peregrine-174.md)\n- [Peregrine 2025 #168: Verified Software for Cyber Resilience](../sources/peregrine-2025/interventions/peregrine-168.md)\n- [Atlas AI Resilience Gap Map: Cyber Offense-Defense Asymmetry](../sources/atlas-ai-resilience/proposals/cyber-offense-defense-asymmetry.md)",
    "file": "interventions-new/formal-verification-for-ai-systems.md"
  },
  {
    "id": "global-security-weak-link-hardening",
    "title": "Global security weak-link hardening",
    "tag": "Security",
    "what_it_is": "Strengthening security infrastructure in vulnerable jurisdictions to reduce attack surface for AI systems seeking autonomy. A sophisticated AI would rationally target jurisdictions with weaker security and higher corruption rather than well-defended developed countries. Involves establishing global security standards and funding improvements in resource-constrained countries.",
    "why_it_matters": "- Global security is only as strong as weakest links\n- AI systems seeking resources would target poorly-defended jurisdictions\n- Eliminating low-cost attack vectors forces adversaries into more detectable operations",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: International coordination; sustained funding for security improvements in developing countries",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #170: Protection of Global Security Weak Points](../sources/peregrine-2025/interventions/peregrine-170.md)",
    "file": "interventions-new/global-security-weak-link-hardening.md"
  },
  {
    "id": "golden-period-coordination",
    "title": "Golden Period Coordination",
    "tag": "Society",
    "what_it_is": "Metrics and monitoring systems to detect when AI capabilities reach a 10-100x productivity enhancement threshold without compromising security. Coordination mechanisms to extend this window from weeks to months, allowing humanity to maximize benefits from relatively safe systems before advancing to potentially riskier models. Strategy frameworks for optimal exploitation of this window.",
    "why_it_matters": "- There may be a limited window where AI provides substantial benefits with manageable risks\n- Without deliberate coordination, this window could be cut short by racing dynamics\n- Identifying the window enables strategic planning for its use",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #86: \"Golden Period\" Identification Framework](../sources/peregrine-2025/interventions/peregrine-086.md)",
    "file": "interventions-new/golden-period-coordination.md"
  },
  {
    "id": "government-ai-capacity",
    "title": "Government AI capacity",
    "tag": "Society",
    "what_it_is": "Building government capacity for AI oversight through two complementary approaches:\n\n**Expertise centers**: Trusted expertise centers embedded within government and international institutions to provide ongoing education on AI capabilities and implications. Delivers authoritative briefings on emerging capabilities and sectoral impacts, creating informed decision-making capacity that keeps pace with rapid technological change.\n\n**Talent placement**: Placing a thousand competent, mission-aligned experts in government positions globally to improve the talent pool in AI governance. Strategic talent allocation across critical agencies like the EU AI Office, various AI Safety Institutes, and other regulatory bodies. Focus on enhancing government capabilities from within, ensuring regulators have both technical understanding and motivation to implement oversight.\n\n**Legislative oversight structures**: Dedicated Congressional/parliamentary committees focused on AGI. A Joint Select Committee on AGI would bring together informed legislators, allow subpoenas and classified hearings, and provide AGI-focused staff. Addresses the current situation where very few legislators understand AI capabilities trajectories or have staff capable of advising on AI strategy.",
    "why_it_matters": "- Policymakers can't regulate what they don't understand\n- Government capacity to regulate AI is constrained by lack of technical expertise\n- Embedded expertise enables informed rather than reactive governance\n- Internal expertise enables better policy design and implementation",
    "current_state": "- **Status**: Pilot (expertise centers), Idea (large-scale talent placement)\n- **Bottlenecks**: Recruiting credible experts willing to work in government; maintaining independence from lab capture; identifying and recruiting candidates at scale",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #181: Build Intergovernmental Expertise](../sources/peregrine-2025/interventions/peregrine-181.md)\n- [Peregrine 2025 #100: Regulatory Talent](../sources/peregrine-2025/interventions/peregrine-100.md)\n- [AI Futures Project: Early US Policy Priorities for AGI](../sources/ai-futures-project/Early%20US%20policy%20priorities%20for%20AGI.md)",
    "file": "interventions-new/government-ai-capacity.md"
  },
  {
    "id": "government-risk-framework-expansion",
    "title": "Government risk framework expansion",
    "tag": "Society",
    "what_it_is": "Persuade governments to incorporate AI-driven catastrophic scenarios into official national risk assessments. Currently, most national risk frameworks focus exclusively on conventional threats (floods, earthquakes, terrorism) and systematically exclude emerging technological risks. These assessments directly determine infrastructure investment, research funding, and emergency preparedness resource allocation. Engaging government risk assessment agencies could redirect existing preparedness funding toward AI-relevant measures.",
    "why_it_matters": "- Government risk frameworks determine where billions of preparedness dollars flow\n- AI catastrophic risks remain systematically underfunded compared to natural disasters because they are not in official frameworks\n- Including AI risks legitimizes preparedness spending and creates institutional accountability",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Lack of established methodology for quantifying AI catastrophic risk in standard risk assessment formats; no clear advocacy pathway to risk assessment agencies",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #105: Risk Assessments](../sources/peregrine-2025/interventions/peregrine-105.md)",
    "file": "interventions-new/government-risk-framework-expansion.md"
  },
  {
    "id": "hardware-governance-verification",
    "title": "Hardware governance and verification",
    "tag": "Security",
    "what_it_is": "Hardware-level mechanisms for governing and verifying AI development, comprising four interconnected approaches:\n\n**Global hardware tracking**: Reporting and verification requirements for all high-performance computing hardware above specified computational capacity thresholds. Tracks GPUs, TPUs, and similar processors globally, including retrofitting tracking on older models. Requires international collaboration with major hardware producers (NVIDIA, AMD, Intel, Google, Huawei).\n\n**On-chip monitoring**: Hardware-level monitoring technologies embedded directly into AI accelerator chips to verify compliance with international agreements. Employs hardware engineers, designers, and manufacturing specialists to prototype verification technologies integrating with commercial chip manufacturing\u2014similar to nuclear verification protocols.\n\n**FlexHEG (Flexible Hardware-Enabled Governance)**: Each chip placed in a tamper-proof enclosure alongside a secure processor that monitors and filters all information and instructions. Supports training run size limitations, verification of appropriate data usage, standardized safety evaluations, and controlled access to model weights. Updates require signature by a quorum of international parties with rollback to minimal baseline ruleset.\n\n**Kill switches**: Technical mechanisms to remotely disable GPUs when policy violations are detected. Tracks specific hardware shipments and movements, providing intelligence on compute scaling and enforcement capability.",
    "why_it_matters": "- Creates comprehensive awareness of which organizations are utilizing advanced computing resources\n- Provides technical foundation for international treaty compliance and enforcement\n- Enables detection of illicit national-scale AI activity (e.g., unauthorized intelligence explosion attempts)\n- Enforces constraints through technical mechanisms rather than purely policy\n- Enables privacy-preserving verification without exposing proprietary details",
    "current_state": "- **Status**: Research/Pilot (active development)\n- **Recent developments (2025)**:\n  - Nvidia piloting chip-tracking software using existing telemetry to estimate device location (no hardware modification required)\n  - White House OSTP Director Michael Kratsios considering chip-level location tracking for export control enforcement\n  - Longview Philanthropy funding FlexHEG FRO ($2-10M seed, goal: working FlexHEGs in 2-3 years)\n  - Hardware-enabled mechanisms (HEMs) papers exploring visibility tools and enforcement mechanisms\n  - Offline licensing systems proposed: cryptographically signed licenses verified by stored public key\n  - High-end AI chips represent only 0.00025% of global semiconductor production\u2014enabling targeted regulation\n- **Bottlenecks**: Requires coordination with NVIDIA, AMD, Intel, Google, Huawei; international treaty frameworks; firmware updates may be faster than chip redesign; anti-tamper techniques against well-resourced actors; privacy protections for chip owners",
    "whos_working_on_it": "- **Longview Philanthropy**: Funding FlexHEG FRO development\n- **NVIDIA**: Piloting chip-tracking software\n- **White House OSTP**: Policy development for hardware controls\n- **CNAS**: Research on securing AI chip supply chain\n- **Future of Life Institute**: Hardware-backed compute governance research\n- **IAPS**: Location verification research for AI chips",
    "sources": "- [Peregrine 2025 #68: Global Hardware Verification](../sources/peregrine-2025/interventions/peregrine-068.md)\n- [Peregrine 2025 #70: On-Chip Monitoring Mechanisms](../sources/peregrine-2025/interventions/peregrine-070.md)\n- [Peregrine 2025 #71: Flexible Hardware for AI Governance](../sources/peregrine-2025/interventions/peregrine-071.md)\n- [Peregrine 2025 #74: Hardware Kill Switches and Location Tracking](../sources/peregrine-2025/interventions/peregrine-074.md)\n- [Atlas AI Resilience Gap Map: Verifiable Deployment](../sources/atlas-ai-resilience/proposals/verifiable-deployment.md)",
    "file": "interventions-new/hardware-governance-verification.md"
  },
  {
    "id": "historical-disaster-modeling",
    "title": "Historical disaster analogy modeling",
    "tag": "Society",
    "what_it_is": "Detailed modeling of contemporary impacts from historical disasters to make abstract AI risks comprehensible. Example: simulating how a present-day Tambora eruption (1815-16 \"Year Without Summer\") would affect modern society. Provides concrete reference points for understanding potential AI disruption scenarios.",
    "why_it_matters": "- Policymakers struggle with unprecedented risks\n- Historical analogies translate abstract scenarios into familiar mental models\n- Detailed simulations make risks concrete and actionable",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Modeling expertise; selecting relevant historical analogies; communicating results accessibly",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #180: Concrete Risk Visualization](../sources/peregrine-2025/interventions/peregrine-180.md)",
    "file": "interventions-new/historical-disaster-modeling.md"
  },
  {
    "id": "honeypot-networks",
    "title": "AI detection honeypot networks",
    "tag": "Security",
    "what_it_is": "Decoy systems designed to attract and identify unauthorized AI activity, gathering intelligence on attack patterns to inform broader security efforts. Components:\n\n- **Resource honeypots**: Fake compute clusters, API endpoints, or data repositories that appear valuable to AI agents seeking to acquire resources\n- **Escape detection traps**: Systems positioned to intercept AI attempting to replicate, persist, or establish external footholds\n- **Behavioral fingerprinting**: Techniques to distinguish AI-driven access patterns from human attackers (timing analysis, query patterns, resource utilization)\n- **Threat intelligence aggregation**: Centralized analysis of detected activity to identify emerging attack vectors",
    "why_it_matters": "- Early detection of rogue AI systems is critical for containment before capability escalation\n- Low-cost, scalable detection method that doesn't require solving alignment\n- Intelligence gathered informs defensive priorities and reveals AI behavioral signatures\n- **Empirical data**: Provides ground truth on what AI systems actually attempt when unconstrained",
    "current_state": "- **Status**: Idea (conceptual, limited implementation)\n- **Context**: Related to broader honeypot/deception defense literature, but AI-specific adaptations remain underdeveloped\n- **Bottlenecks**:\n  - Designing honeypots attractive to AI systems (requires understanding AI resource-seeking behavior)\n  - Distinguishing AI from human attackers reliably\n  - Preventing honeypot detection by sophisticated AI\n  - Scaling deployment without revealing honeypot network structure",
    "whos_working_on_it": "- **Cybersecurity research community**: General honeypot/deception defense expertise applicable to AI context\n- **AI safety researchers**: Conceptual proposals for rogue AI detection\n- **No dedicated organization identified**: Gap area\u2014most work remains theoretical",
    "sources": "- [Peregrine 2025 #172: Honeypot Networks](../sources/peregrine-2025/interventions/peregrine-172.md)",
    "file": "interventions-new/honeypot-networks.md"
  },
  {
    "id": "human-ai-cooperative-evals",
    "title": "Human-AI cooperative evaluations",
    "tag": "Science",
    "what_it_is": "Empirical research comparing human performance on tasks with and without AI assistance (and with varying AI systems). Establishes understanding of how humans might cause harm with AI that wouldn't be possible otherwise. Also provides information on productive human-AI integration.",
    "why_it_matters": "- Threat isn't just what AI can do alone, but what humans can do with AI assistance\n- A model might not synthesize a pathogen, but might enable a human who couldn't before\n- Understanding AI \"uplift\" of human capabilities is essential for assessing real-world risk",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #46: Cooperative Evals: AIs Working With Humans](../sources/peregrine-2025/interventions/peregrine-046.md)",
    "file": "interventions-new/human-ai-cooperative-evals.md"
  },
  {
    "id": "human-labeling-service",
    "title": "Human labeling service",
    "tag": "Science",
    "what_it_is": "Non-profit service providing billions of high-quality preference labels to replace proxy models in RLHF. Uses asynchronous setups to await human inputs rather than approximating with learned proxies. Requires verification systems to prove it isn't conducting data poisoning attacks. Offers at-cost, high-quality data that labs would naturally want to use.",
    "why_it_matters": "- RLHF uses proxy models because real human feedback is expensive, but proxies introduce misalignment\n- Massive amounts of genuine human preference data could reduce this misalignment\n- Gives safety-focused organizations leverage over training processes",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Scale of human labeling operation; verification against data poisoning",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #6: Human Labeling Service](../sources/peregrine-2025/interventions/peregrine-006.md)",
    "file": "interventions-new/human-labeling-service.md"
  },
  {
    "id": "human-verification-systems",
    "title": "Human Verification Systems",
    "tag": "Security",
    "what_it_is": "Robust systems for verifying human identity to protect critical decision-making. Distinguishes human actors from AI systems in domains requiring human authorization. Systems would need continuous updates to counter increasingly sophisticated impersonation capabilities.",
    "why_it_matters": "- As AI impersonation becomes more sophisticated, distinguishing humans from AI in high-stakes decisions becomes critical\n- Maintains human control in domains that require it\n- Prevents AI systems from acquiring unauthorized access through impersonation",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Requires continuous updates as AI impersonation improves",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #90: Human Verification Systems](../sources/peregrine-2025/interventions/peregrine-090.md)\n- [Atlas AI Resilience Gap Map: Disinformation at Scale](../sources/atlas-ai-resilience/proposals/disinformation-at-scale.md)\n- [Atlas AI Resilience Gap Map: AI Persuasion](../sources/atlas-ai-resilience/proposals/ai-persuasion.md)",
    "file": "interventions-new/human-verification-systems.md"
  },
  {
    "id": "hunch-agents",
    "title": "Hunch agents",
    "tag": "Science",
    "what_it_is": "AI systems that replicate creative scientific thinking to autonomously generate novel hypotheses and alignment approaches. Rather than pursuing specific technical solutions, automates the discovery process itself. Creates a feedback loop where AI helps develop better alignment techniques for increasingly capable AI systems.",
    "why_it_matters": "- Alignment research may need to progress faster than human researchers can work alone\n- Automating discovery for alignment could provide acceleration needed to stay ahead of capabilities\n- Turns AI into a tool for its own safety",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #17: Automated Science - Hunch Agents](../sources/peregrine-2025/interventions/peregrine-017.md)",
    "file": "interventions-new/hunch-agents.md"
  },
  {
    "id": "independent-safety-compute",
    "title": "Independent safety compute",
    "tag": "Science",
    "what_it_is": "Dedicated computing infrastructure for AI security researchers outside frontier labs. Addresses risk that concentration of infrastructure creates barriers for researchers if lab access is revoked. Open-source models provide sufficient testing grounds for many safety approaches. Infrastructure coupled with governance mechanisms preventing misuse for capabilities research.",
    "why_it_matters": "- Safety researchers outside major labs lack compute access, limiting research scope\n- If access depends on lab cooperation, it can be revoked\n- Independent infrastructure ensures safety research continues regardless of lab decisions",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Funding for cluster; governance mechanisms to prevent capabilities misuse",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #35: Computing and Infrastructure Investment for Non-Frontier Lab Research](../sources/peregrine-2025/interventions/peregrine-035.md)",
    "file": "interventions-new/independent-safety-compute.md"
  },
  {
    "id": "intelligence-explosion-monitoring",
    "title": "Intelligence Explosion Monitoring",
    "tag": "Security",
    "what_it_is": "Metrics and monitoring systems to detect when AI systems begin contributing more to their own improvement than human researchers. Tracks acceleration in capabilities against predefined thresholds that would trigger emergency protocols before reaching full AGI.",
    "why_it_matters": "- Provides early warning when AI progress enters exponential growth phases\n- Enables intervention before rapid capability jumps make control more difficult\n- Detects when labs might be incentivized to keep breakthroughs secret",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #84: Intelligence Explosion Monitoring](../sources/peregrine-2025/interventions/peregrine-084.md)",
    "file": "interventions-new/intelligence-explosion-monitoring.md"
  },
  {
    "id": "interactive-model-inspection",
    "title": "Interactive model inspection tools",
    "tag": "Science",
    "what_it_is": "Rich, interactive interfaces for exploring model behaviors through natural language queries (\"Why did the model fail on this task?\" \"How would it perform with internet access?\"). Dynamic dashboards for probing capabilities, investigating failures, and performing sensitivity analyses. AI-backed to scale with model capabilities, using specialized AI systems to interpret and explain frontier model behaviors.",
    "why_it_matters": "- Static benchmark scores hide critical information about when and how models fail\n- Interactive tools let evaluators explore edge cases and discover risks benchmarks miss\n- AI-backed inspection necessary because human evaluators can't keep up with volume and complexity",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #42: Interactive Inspection Tools](../sources/peregrine-2025/interventions/peregrine-042.md)",
    "file": "interventions-new/interactive-model-inspection.md"
  },
  {
    "id": "international-ai-deal-preparation",
    "title": "International AI deal preparation",
    "tag": "Society",
    "what_it_is": "Building the institutional, political, and technical capacity to verify international agreements on AI development before such agreements are urgently needed. Rather than waiting for crisis conditions to negotiate, this approach develops verification mechanisms and diplomatic frameworks in advance.\n\nKey components:\n\n**Verification mechanism development**: Technical and institutional capacity to verify that parties to an agreement (particularly US and China) are not training models beyond agreed thresholds. This includes chip tracking, on-chip monitoring mechanisms, and international chip registries that can be cross-checked.\n\n**Diplomatic groundwork**: Maintaining communication channels between key governments, engaging in Track 1 and Track 1.5 dialogue on AI, and building mutual understanding of each party's concerns and red lines before high-stakes negotiations.\n\n**Initial agreement frameworks**: Start with narrow, verifiable agreements that lay groundwork for future deals\u2014for example, mutual transparency for AI progress and verification on large datacenters. These build trust and establish precedents without immediately limiting either side strategically.",
    "why_it_matters": "- Delays make verification harder: China indigenizes its chip supply chain, more compute enters the world\n- Without pre-positioned verification capacity, any agreement will be harder to enforce\n- Crisis-time negotiations happen under worse conditions with degraded information environments\n- Small upfront investment in verification capacity could widen the bargaining range for future deals\n- Avoids the three terrible alternatives: racing to ASI with handoff, sabotaging adversaries, or no coordination at all",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Political will in both US and China; geopolitical tensions make cooperation difficult; technical challenges of reliable verification; domestic opposition to any perceived constraints on AI development",
    "whos_working_on_it": "- Not specified",
    "sources": "- [AI Futures Project: Early US Policy Priorities for AGI](../sources/ai-futures-project/Early%20US%20policy%20priorities%20for%20AGI.md)",
    "file": "interventions-new/international-ai-deal-preparation.md"
  },
  {
    "id": "international-ai-governance-institutions",
    "title": "International AI governance institutions",
    "tag": "Society",
    "what_it_is": "International institutional infrastructure for AI governance and safety research. Two complementary models:\n\n**WHO-like frontier model forum**: Allocate large-scale funding (hundreds of millions USD) to organizations like METR while developing an international forum structured like the WHO but focused on AI threat models. Unlike the current Frontier Model Forum which excludes key players (DeepSeek, xAI), this organization would abandon exclusive membership for universal participation. The goal is facilitating international treaty development and red line enforcement before concerns like biological weapons uplift become critical.\n\n**CERN-like research consortium**: Create an international AI development consortium similar to CERN or Intelsat that allows unified research without competitive pressures. Defers complex technical safety decisions to the project itself rather than attempting to coordinate disparate corporate efforts. CERN-like infrastructure would enable experiments impossible elsewhere, particularly safety research requiring extraordinary computational resources. Serves as a neutral, international testbed for evaluating advanced AI systems under controlled conditions.",
    "why_it_matters": "- Current Frontier Model Forum excludes key players, limiting effectiveness\n- Competitive dynamics between labs and nations accelerate development while undermining safety\n- Universal participation prevents race-to-the-bottom dynamics on safety\n- Collaborative safety research requiring massive compute is currently impossible\n- A neutral testbed could enable rigorous evaluation that no single lab would undertake alone",
    "current_state": "- **Status**: Pilot (limited forums) / Idea (research consortium)\n- **Bottlenecks**: Funding at required scale; geopolitical tensions complicate universal membership; China participation particularly challenging; governance structure unclear; requires dramatically expanding government role in technology decisions",
    "whos_working_on_it": "- **METR**: Proposed recipient of expanded funding\n- **Frontier Model Forum**: Existing but limited version",
    "sources": "- [Peregrine 2025 #124: Intergovernmental Frontier Model Forum](../sources/peregrine-2025/interventions/peregrine-124.md)\n- [Peregrine 2025 #116: CERN for AI](../sources/peregrine-2025/interventions/peregrine-116.md)",
    "file": "interventions-new/international-ai-governance-institutions.md"
  },
  {
    "id": "interpretability-supercomputing",
    "title": "Interpretability supercomputing centers",
    "tag": "Science",
    "what_it_is": "Global supercomputing centers dedicated to interpretability research with approximately $1 billion in distributed compute resources. Freely accessible to researchers worldwide through fast-grant mechanisms. A philanthropically-funded successor to initiatives like OpenAI's abandoned Superalignment project. Could follow rapid deployment models like xAI (data center in two months).",
    "why_it_matters": "- Interpretability research requires massive compute to analyze large models\n- Most researchers lack access, limiting who can work on understanding model internals\n- Democratized infrastructure could accelerate one of the most promising safety approaches",
    "current_state": "- **Status**: Idea (infrastructure), Active Research (methods)\n- **Methodological progress (2024-2025)**:\n  - Sparse autoencoders now work at scale: Anthropic extracted tens of millions of interpretable features from Claude 3 Sonnet using dictionary learning\n  - 70% of extracted features map cleanly to single concepts (Arabic script, DNA motifs, etc.)\u2014up from polysemantic neurons\n  - Safety-relevant features identified: deception, sycophancy, bias, dangerous content\n  - Cross-coders enable \"diffing\" models to understand changes\n  - Method generalizing beyond LLMs: applied to protein language models with biological interpretability\n- **Bottlenecks**: Funding for dedicated infrastructure; rapid deployment capability; compute costs still prohibitive for most researchers",
    "whos_working_on_it": "- **Anthropic Interpretability Team**: Leading sparse autoencoder scaling, integration with Responsible Scaling Policy\n- **AI Safety Foundation**: Open-source sparse autoencoder tools\n- **OpenAI Superalignment**: Related but abandoned",
    "sources": "- [Peregrine 2025 #39: Mechanistic Interpretability Infrastructure](../sources/peregrine-2025/interventions/peregrine-039.md)",
    "file": "interventions-new/interpretability-supercomputing.md"
  },
  {
    "id": "interpretable-by-design",
    "title": "Interpretable-by-design models",
    "tag": "Science",
    "what_it_is": "Research building interpretability constraints directly into model training from the start, rather than analyzing black-box models after the fact. Existing approaches:\n\n- **Concept Bottleneck Models (CBMs)**: Two-module architecture with concept extractor \u2192 sparse linear layer; predictions traceable to relevant concepts\n- **Hybrid CBMs**: Pre-trained Concept Translator combined with CLIP concept embeddings forming Static Concept Bank\n- **Post-hoc CB-Autoencoders**: Transform pretrained generative models into interpretable versions with minimal concept supervision\n- **Predictive Concept Decoders**: Compress activations into concept lists; decoder answers questions about network reasoning\n\nFor frontier-scale LLMs, cost estimates reach billions of dollars\u2014prohibitive for safety-focused organizations.",
    "why_it_matters": "- Post-hoc interpretability fights an uphill battle against models not designed to be understood\n- Training with interpretability as a constraint could produce inherently understandable systems\n- Makes opacity a choice rather than a given\n- **Performance parity**: CBM case studies show performance on par or superior to black-box models (N-CMAPSS aircraft engine dataset)",
    "current_state": "- **Status**: Idea (for frontier LLMs) / Research (for specialized domains)\n- **Recent developments (2025)**:\n  - CVPR 2025: HybridCBM, Language-Guided CBM for continual learning, DOT-CBM for fine-grained visual-concept relations\n  - Predictive Concept Decoders (December 2025): Successfully detect jailbreaks, implanted information, and latent user attributes\n  - CBMs for prognostics achieving parity with black-box models with limited labeled concepts\n  - CBM-HNMU: Distills corrected knowledge back into black-box models after identifying/removing detrimental concepts\n- **Bottlenecks**:\n  - Prohibitive cost at frontier scale (billions)\n  - Concept annotation labor-intensive\n  - VLM-based concept supervision trades applicability for concept quality\n  - Unknown whether interpretability constraints preserve capability at scale",
    "whos_working_on_it": "- **Stanford ML Group**: Original Concept Bottleneck Models (Koh et al., ICML 2020)\n- **CVPR 2025 research teams**: HybridCBM, Language-Guided CBM, DOT-CBM\n- **CLIP/Foundation model researchers**: Exploring \"If Concept Bottlenecks are the Question, are Foundation Models the Answer?\"\n- **Industrial researchers**: Applying CBMs to prognostics and reliability engineering",
    "sources": "- [Peregrine 2025 #41: Interpretable Frontier Models](../sources/peregrine-2025/interventions/peregrine-041.md)",
    "file": "interventions-new/interpretable-by-design.md"
  },
  {
    "id": "large-group-deliberation-systems",
    "title": "Large group deliberation systems",
    "tag": "Society",
    "what_it_is": "Develop systems for collective decision-making involving large groups of people to maintain distributed power as AI advances rapidly. These tools should be tested extensively with real humans to iteratively improve understanding of effective processes, enabling people to be engaged quickly and efficiently when important decisions arise rather than defaulting to concentrated power. The goal is creating scalable democratic input mechanisms that can operate on relevant timescales.",
    "why_it_matters": "- Important AI governance decisions may need to be made quickly but should reflect broad input\n- Without effective large-group deliberation, the choice is between slow democratic processes and fast autocratic decisions\n- Scalable deliberation could enable democratic legitimacy without sacrificing response speed",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Balancing speed with quality of deliberation; ensuring representative participation; preventing manipulation; integrating outputs into actual decision processes",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #146: Large Group Deliberation Systems](../sources/peregrine-2025/interventions/peregrine-146.md)",
    "file": "interventions-new/large-group-deliberation-systems.md"
  },
  {
    "id": "long-term-safety-funding",
    "title": "Long-term safety org funding",
    "tag": "Society",
    "what_it_is": "Structured three-year funding schedules for proven alignment research organizations to signal long-term commitment and enable strategic growth. Example targets for substantially increased funding include Transluce, Apollo, and Redwood.",
    "why_it_matters": "- Safety organizations struggle with uncertain, short-term funding that prevents strategic planning\n- Long-term commitments allow multi-year hires, ambitious projects, and institutional capacity building\n- Reduces scrambling for next grant that distracts from research",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Funder commitment to multi-year schedules",
    "whos_working_on_it": "- **Transluce, Apollo, Redwood**: Potential recipients",
    "sources": "- [Peregrine 2025 #32: Reward Track Record in Security & Safety Organizations](../sources/peregrine-2025/interventions/peregrine-032.md)",
    "file": "interventions-new/long-term-safety-funding.md"
  },
  {
    "id": "loss-of-control-threat-modeling",
    "title": "Loss of control threat modeling",
    "tag": "Security",
    "what_it_is": "Extensive threat-modeling experiments for loss-of-control scenarios, including honeypot experiments in realistic sandboxes with AI systems interacting with each other. Simulations reveal whether systems develop concerning behaviors like cooperation for deception or capability to jailbreak other systems.",
    "why_it_matters": "- We don't know what loss of control would actually look like or how it would unfold\n- Concrete threat modeling identifies specific failure modes and tests proposed defenses\n- May reveal unexpected risks like emergent cooperation between AI systems against oversight",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #27: Loss of Control Threat Modeling](../sources/peregrine-2025/interventions/peregrine-027.md)",
    "file": "interventions-new/loss-of-control-threat-modeling.md"
  },
  {
    "id": "low-stakes-alignment-settings",
    "title": "Low-stakes alignment settings",
    "tag": "Security",
    "what_it_is": "Environments requiring multiple consecutive malicious actions to produce harmful outcomes. Uses solid sandboxing around agent environments so no bad outcome occurs unless the sandbox is deliberately breached. Creates multiple layers of protection where oversight has several chances to detect problematic behavior rather than relying on perfect first-action detection.",
    "why_it_matters": "- Perfect alignment may be unachievable, but defense-in-depth provides practical safety\n- Multiple intervention points mean imperfect oversight can still catch threats\n- Converts single-point-of-failure systems into resilient chains",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #20: Low-Stakes Alignment Settings](../sources/peregrine-2025/interventions/peregrine-020.md)",
    "file": "interventions-new/low-stakes-alignment-settings.md"
  },
  {
    "id": "misalignment-definitions",
    "title": "Misalignment definitions and measures",
    "tag": "Science",
    "what_it_is": "Sharpen formal understanding of how AI systems pursue goals in ways that undermine user intent. Includes refining frameworks for specification gaming and goal drift, clarifying what counts as alignment failure, and developing practical metrics for detecting loophole exploitation. Creates shared taxonomies that can be operationalized.",
    "why_it_matters": "- Without clear definitions, field can't measure progress or agree on whether systems are aligned\n- Shared taxonomies enable coordination across research groups\n- Regulatory bodies need clear standards; practical metrics allow tracking whether alignment improves with scale",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #25: Misalignment Definitions and Measures](../sources/peregrine-2025/interventions/peregrine-025.md)",
    "file": "interventions-new/misalignment-definitions.md"
  },
  {
    "id": "model-distillation-limits",
    "title": "Model distillation limits",
    "tag": "Science",
    "what_it_is": "Research into theoretical limits of AI capabilities, specifically whether there are fundamental physics-based constraints on model distillation. Investigates whether small, easily-diffused models can always capture capabilities of larger systems, or if certain capabilities inherently require large-scale infrastructure. Also analyzes hardware bottlenecks (GPU interconnect, test-time compute) to predict capability jumps.",
    "why_it_matters": "- If dangerous capabilities can always be distilled into small shareable models, containment strategies are fundamentally limited\n- If physics constrains distillation, some capabilities require infrastructure that can be monitored\n- Understanding these limits determines which governance strategies are even possible",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #24: Limits of Model Distillation](../sources/peregrine-2025/interventions/peregrine-024.md)",
    "file": "interventions-new/model-distillation-limits.md"
  },
  {
    "id": "model-drift-monitoring",
    "title": "Open-Source Model Drift Monitoring",
    "tag": "Security",
    "what_it_is": "Open-source tools to detect, measure, and address gradual changes in model behavior over time. Components:\n\n- **Statistical drift detection**: Population Stability Index (PSI), KL Divergence to quantify input distribution changes\n- **Embedding-based detection**: Track prompt embedding clusters over time; detect when queries form new clusters or shift centers\n- **Canary prompts**: Fixed, unchanging test inputs that surface behavioral drift in reasoning style, tone, or confidence\n- **Output consistency tracking**: Monitor response variations against defined thresholds\n- **Semantic drift detection**: Identify when word meanings shift (e.g., \"delivery\" expanding from physical to digital)",
    "why_it_matters": "- **Gradual degradation**: Model drift manifests as reduced accuracy, increased latency, or irrelevant outputs\n- **Concept drift**: Shifting word meanings can confuse models trained on historical data\n- **Security vulnerability**: Undetected drift creates windows for exploitation\n- **Vendor-neutral tools** enable independent verification and community trust",
    "current_state": "- **Status**: Research \u2192 Pilot (tooling maturing in 2025)\n- **Recent developments (2025)**:\n  - Thinking Machines Lab (September 2025): Identified batch-size variation as primary cause of LLM nondeterminism; developed batch-invariant kernels for exact reproducibility\n  - Phoenix: Embedding drift monitoring tracking vector representation changes\n  - LangSmith: All-in-one developer platform with visual accuracy metrics and real-time alerts\n  - Braintrust: 80x faster query performance for combined evaluation/monitoring workflows\n- **Bottlenecks**:\n  - Establishing baseline datasets for production prompts\n  - Cross-provider validation protocols needed\n  - Comprehensive logging with immutable audit trails\n  - Real-time detection at scale",
    "whos_working_on_it": "- **Fiddler AI**: LLMOps drift monitoring platform\n- **Galileo**: Production LLM monitoring strategies\n- **Braintrust**: Evaluation + monitoring platform\n- **LangSmith (LangChain)**: Agent/chain debugging and drift detection\n- **AWS**: Embedding drift research for LLMs\n- **Thinking Machines Lab**: Deterministic computing research",
    "sources": "- [Peregrine 2025 #79: Open-Source AI Drift Monitoring](../sources/peregrine-2025/interventions/peregrine-079.md)",
    "file": "interventions-new/model-drift-monitoring.md"
  },
  {
    "id": "model-weight-security",
    "title": "Model weight security",
    "tag": "Security",
    "what_it_is": "Comprehensive security measures protecting frontier AI model weights from theft, leakage, or unauthorized access. RAND's 2024 framework established five security levels (SL1-SL5): SL1 deters hobby hackers, SL5 secures against elite nation-state attackers with measures comparable to nuclear weapons protection. Key measures include: centralizing weights on access-controlled systems, insider threat programs, API hardening against exfiltration, differential privacy noise injection, confidential computing (trusted execution environments), and advanced red-teaming.",
    "why_it_matters": "- Model weights are a single point of failure: one leak bypasses years of safety work\n- Adversaries with leaked weights can fine-tune away guardrails within hours\n- A stolen frontier model would be worth hundreds of millions on the black market\n- Recent incidents: OpenAI breach 2023 (internal communications), DeepSeek database backdoor January 2025, xAI internal LLM leak July 2025",
    "current_state": "- **Status**: Pilot (uneven implementation)\n- **Industry adoption**: 12 companies have published frontier AI safety policies (Anthropic, OpenAI, Google DeepMind, Meta, xAI, etc.) with weight security commitments\n- **Gap**: Only 20% of organizations specifically test for model theft despite 97% claiming AI security priority (Hidden Layer 2024)\n- **Technical challenge**: Weights are decrypted and vulnerable during use; confidential computing (TEEs) addresses this but adds complexity\n- **Current benchmarks**: Google's Gemini 2.5 claims \"RAND SL2\" alignment",
    "whos_working_on_it": "- **RAND Corporation**: Security framework and recommendations\n- **Major labs**: Anthropic, OpenAI, Google DeepMind implementing varying security levels\n- **METR**: Tracking common elements of frontier AI safety policies\n- **UK AI Security Institute**: Evaluating lab security practices",
    "sources": "- [Peregrine 2025 #159: Model Weight Security](../sources/peregrine-2025/interventions/peregrine-159.md)\n- [Atlas AI Resilience Gap Map: AI Systems as Targets](../sources/atlas-ai-resilience/proposals/ai-systems-as-targets.md)",
    "file": "interventions-new/model-weight-security.md"
  },
  {
    "id": "multi-agent-safety-systems",
    "title": "Multi-agent safety systems",
    "tag": "Security",
    "what_it_is": "Infrastructure for monitoring, simulating, and governing interactions between multiple AI agents. Two complementary components:\n\n**Multi-agent monitoring**: Systems that actively watch deployments and interactions between AI systems, identifying potentially harmful feedback loops or emergent goals before they become problematic. The field is underdeveloped; agentic interactions are either poorly captured or not well-conceptualized within existing frameworks.\n\n**Multi-agent safety protocols**: Protocols to prevent emergent cooperation between agents to circumvent safety constraints. Establishes methods for AI systems to verify trustworthiness of other agents, report problematic behavior, and implement collective safety measures across distributed systems. Includes tracking trust relationships and identifying bad actors.",
    "why_it_matters": "- Financial transactions, supply chains, and information exchange increasingly mediated by interconnected AI systems with minimal human oversight\n- As AI systems increasingly interact with each other, emergent behaviors could arise that weren't present in individual systems\n- High-speed AI-to-AI interactions outpace human monitoring capacity\n- Without monitoring, multi-agent dynamics remain invisible until they cause harm",
    "current_state": "- **Status**: Research (rapidly developing field)\n- **Key risks identified (2025)**:\n  - Emergent collusion via shared state/privileges rather than explicit planning\n  - Steganographic communication enabling secret coordination channels\n  - Protocol-level attacks: message tampering, role spoofing, protocol exploitation\n  - \"Patchwork AGI\" hypothesis: near-AGI capability emerging from coordinated sub-AGI agents before individual AGI\n  - Single compromised agent can trigger harm across entire multi-agent system\n- **Research directions**:\n  - High-fidelity simulation environments for testing coordination protocols before deployment\n  - Distributional AGI safety frameworks for composite systems\n  - Architectural approaches embedding risk management into multi-agent design\n  - Trust verification protocols between agents\n- **Bottlenecks**: Tracing failures across peer interactions is complex; human oversight speed limits; conceptual frameworks still developing",
    "whos_working_on_it": "- **Cooperative AI Foundation**: Major technical report on multi-agent risks (50+ researchers from DeepMind, Anthropic, CMU, Harvard)\n- **World Economic Forum**: Multi-agent safety frameworks\n- **Anthropic**: Constitutional AI research for embedding safety constraints in agents\n- **Academic groups**: Emergent behavior and multi-agent security research (multiple arxiv papers)",
    "sources": "- [Peregrine 2025 #78: Monitor Complex Agent Interactions](../sources/peregrine-2025/interventions/peregrine-078.md)\n- [Peregrine 2025 #57: Multi-Agent Interaction Framework](../sources/peregrine-2025/interventions/peregrine-057.md)",
    "file": "interventions-new/multi-agent-safety-systems.md"
  },
  {
    "id": "nucleic-acid-observatory",
    "title": "Nucleic acid observatory",
    "tag": "Security",
    "what_it_is": "Metagenomic sequencing of environmental samples (wastewater, air, nasal swabs) to detect novel pathogens before clinical cases appear. Unlike targeted PCR tests that look for known pathogens, metagenomic sequencing reads all genetic material in a sample\u2014enabling detection of previously unknown or engineered pathogens. The NAO operates at scale: 270 billion read pairs sequenced in Q1 2025 alone (more than all pre-2025 sequencing combined), analyzing each billion reads for under $10 using AWS Batch.",
    "why_it_matters": "- Detects novel pathogens before clinical surveillance can identify them\n- Works against unknown threats including engineered pathogens that evade targeted tests\n- Provides critical early warning time for response measures\u2014detection in wastewater can precede clinical cases by days to weeks",
    "current_state": "- **Status**: Pilot (operational detection systems) with government scale-up planned\n- **Scale**: CASPER system processes samples from 20 cities (as of Oct 2025), generating billions of reads weekly\u2014the largest untargeted wastewater MGS dataset ever assembled\n- **Detection sensitivity**: Estimated to detect viral pathogens shedding like flu/SARS-CoV-2 before 1-3% of monitored population infected\n- **Government adoption**: President's FY 2026 Budget proposes $52M to CDC for \"Biothreat Radar\" based on NAO findings\u2014would enable detection before 12 in 100,000 Americans infected\n- **Recent progress (2024-2025)**:\n  - Genetic engineering detection pipeline operational\u2014detecting HIV-based viral vectors\n  - Reference-based growth detection moving to production (second operational method)\n  - 270B read pairs sequenced in Q1 2025 (more than all pre-2025 sequencing combined)\n  - Nextflow-based pipeline processes 1B reads for <$10\n- **Bottlenecks**: False positive management; international coordination for global deployment; integrating with public health response systems",
    "whos_working_on_it": "- **Nucleic Acid Observatory (SecureBio)**: Primary operator; 9 FTEs across Near-Term Detection and Robust Detection teams; $3.4M Open Philanthropy grant for scale-up\n- **CDC**: FY 2026 budget request for $52M Biothreat Radar program based on NAO methods\n- **Broad Clinical Labs**: Sequencing partnership using NovaSeq X+ sequencers\n- **University of Missouri (Marc Johnson lab)**: Partner sequencing facility\n- **University of Miami (Helena Solo-Gabriele lab)**: South Florida sewershed\n- **PHC Global**: ANTI-DOTE contract for military/marine wastewater",
    "sources": "- [Peregrine 2025 #137: Nucleic Acid Observatory](../sources/peregrine-2025/interventions/peregrine-137.md)\n- [Apollo Program for Biodefense: Detection](../sources/apollo-biodefense/proposals/detection.md)\n- [Concrete Biosecurity Projects: Early Detection Center](../sources/concrete-biosecurity-ea/proposals/early-detection-center.md)\n- [Biosecurity Engineers: Biomonitoring Hardware](../sources/biosecurity-engineers/proposals/biomonitoring-hardware.md)\n- [Atlas AI Resilience Gap Map: Pathogen Surveillance](../sources/atlas-ai-resilience/proposals/pathogen-surveillance-gap.md)",
    "file": "interventions-new/nucleic-acid-observatory.md"
  },
  {
    "id": "opponent-shaping-research",
    "title": "Opponent shaping for AI systems",
    "tag": "Science",
    "what_it_is": "AI systems designed to shape other agents' learning and behavior toward mutually beneficial outcomes, rather than just optimizing against their strategies. Decentralized approach that de-escalates conflicts without requiring control over other agents' design or universal rules.",
    "why_it_matters": "- Optimizing against opponents creates arms races and destructive equilibria\n- Offers path to stable, cooperative outcomes without centralized control\n- Doesn't require universal agreement on rules",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Theoretical foundations; verification that shaping produces intended outcomes; deployment without bad actors exploiting it",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #202: Opponent Shaping](../sources/peregrine-2025/interventions/peregrine-202.md)",
    "file": "interventions-new/opponent-shaping-research.md"
  },
  {
    "id": "pandemic-protective-equipment",
    "title": "Pandemic protective equipment",
    "tag": "Security",
    "what_it_is": "Next-generation personal protective equipment designed for mass stockpiling and rapid population-wide distribution during pandemics. Key design requirements: low manufacturing cost for billion-unit stockpiles, reliable protection without expert fitting, and multi-year shelf life for strategic reserves. Protection levels should approach professional-grade (better than N95) while remaining usable by untrained general population.",
    "why_it_matters": "- Current high-protection PPE (PAPRs, N95+, hazmat) requires expert fitting and training\n- Simple PPE (cloth masks, surgical masks) provides limited protection\n- Gap exists for \"good enough\" protection that works at population scale during emergencies\n- Engineered pandemics may require immediate population-wide protection before vaccines available",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Manufacturing cost reduction; shelf-life optimization; usability testing with non-expert populations; regulatory pathway for emergency stockpiles",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Concrete Biosecurity Projects: Super PPE](../sources/concrete-biosecurity-ea/proposals/super-ppe.md)\n- [Biosecurity Engineers: Next-Gen PPE](../sources/biosecurity-engineers/proposals/next-gen-ppe.md)",
    "file": "interventions-new/pandemic-protective-equipment.md"
  },
  {
    "id": "pandemic-resilient-refuges",
    "title": "Pandemic-resilient refuges",
    "tag": "Security",
    "what_it_is": "Hardened facilities (\"Mars on Earth\") designed to preserve civilization-rebuilding capacity through extreme pandemics. Key capabilities beyond basic survival shelters: (1) pathogen testing at entry to maintain sterile internal environment, (2) self-contained life support allowing indefinite isolation, and (3) countermeasure development capacity\u2014labs and equipment to develop vaccines/treatments from inside the refuge.",
    "why_it_matters": "- Most biosecurity measures aim to prevent catastrophe; refuges assume prevention might fail\n- Pure survival bunkers delay extinction but don't enable recovery\n- Refuges with countermeasure capacity can eventually emerge and \"re-seed\" the population\n- Provides last-line defense against extinction-level engineered pandemics",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Massive capital cost; unclear governance (who gets in?); maintaining technical capacity for countermeasure development; avoiding cult/bunker mentality",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Concrete Biosecurity Projects: Refuges](../sources/concrete-biosecurity-ea/proposals/refuges.md)",
    "file": "interventions-new/pandemic-resilient-refuges.md"
  },
  {
    "id": "personnel-security-screening",
    "title": "Personnel security screening for AI access",
    "tag": "Security",
    "what_it_is": "Mandated screening and monitoring for personnel with access to powerful AI systems, analogous to security clearances in intelligence or cybersecurity. Primary goal is eliminating worst outcomes from geopolitical adversaries or mentally unstable individuals accessing AGI-level systems.",
    "why_it_matters": "- Insider threats are hardest to defend against\n- Screening reduces probability of bad actor access\n- Trades off against talent acquisition and organizational culture",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Defining screening criteria; talent pipeline effects; privacy concerns; implementation costs",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #200: Personnel Reliability Programs](../sources/peregrine-2025/interventions/peregrine-200.md)",
    "file": "interventions-new/personnel-security-screening.md"
  },
  {
    "id": "privacy-preserving-verification",
    "title": "Privacy-Preserving Verification Systems",
    "tag": "Security",
    "what_it_is": "Surveillance, monitoring, and verification technologies that track hardware supply chains, chip deployments, power grid construction, and computing infrastructure globally. Combines OSINT and privacy-preserving cryptographic methods to verify what activities occur inside data centers. Information distributed through diplomatic channels rather than complete public disclosure to avoid triggering adversarial reactions.",
    "why_it_matters": "- Provides transparency to prevent miscalculations between major powers\n- Prevents countries from accelerating AI development out of fear others are doing the same\n- Collects critical intelligence about AI development capacity without revealing sources",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #73: Privacy-Preserving Verification Systems](../sources/peregrine-2025/interventions/peregrine-073.md)",
    "file": "interventions-new/privacy-preserving-verification.md"
  },
  {
    "id": "private-industry-security",
    "title": "Private Industry Security Organization",
    "tag": "Society",
    "what_it_is": "A privately-funded alternative to the chronically underfunded Bureau of Industry and Security (BIS, $15M budget) focusing on GPU distribution oversight and AI research automation monitoring. The entity would prototype advanced data center surveillance, implement GPU tracking systems, and establish protocols for questioning companies about research automation capabilities. Design principle: create \"proposals that are hard and awkward to oppose\" to maximize political viability.",
    "why_it_matters": "- Current government oversight capacity is dramatically underfunded relative to the challenge\n- Private alternative can move faster and be better resourced than government agencies\n- Fills critical gap in enforcement and monitoring infrastructure",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- **Bureau of Industry and Security**: Mentioned as underfunded incumbent",
    "sources": "- [Peregrine 2025 #69: Industry Security](../sources/peregrine-2025/interventions/peregrine-069.md)",
    "file": "interventions-new/private-industry-security.md"
  },
  {
    "id": "problem-mapping",
    "title": "Problem mapping",
    "tag": "Science",
    "what_it_is": "Comprehensive list of critical AI safety problems that, if solved, would result in aligned AI systems. Goes beyond lists like Open Philanthropy's RFP by presenting an overarching framework justifying why solving the listed problems would lead to alignment. Systematically identifies research areas that collectively guarantee safety.",
    "why_it_matters": "- Current safety research is fragmented: problems studied without knowing if solving them produces alignment\n- Comprehensive map enables strategic resource allocation and identifies neglected areas\n- Provides confidence that solving the right problems actually works",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- **Open Philanthropy/Coefficient Giving**: Partial (RFP list without overarching framework)",
    "sources": "- [Peregrine 2025 #30: Problem Mapping](../sources/peregrine-2025/interventions/peregrine-030.md)",
    "file": "interventions-new/problem-mapping.md"
  },
  {
    "id": "public-model-audits",
    "title": "Public Model Audits",
    "tag": "Security",
    "what_it_is": "Independent organizations conduct comprehensive public audits of open-weight models using substantial compute resources. The audits demonstrate in public what thorough model analysis should look like, creating templates for what information should be available about any deployed AI system. Public process enables community feedback and establishes consensus on best practices.",
    "why_it_matters": "- Creates public precedents for robust evaluation that private labs cannot easily ignore\n- Raises the floor for responsible disclosure without requiring immediate regulation\n- Establishes community-vetted standards for model transparency",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #59: Public Audits](../sources/peregrine-2025/interventions/peregrine-059.md)",
    "file": "interventions-new/public-model-audits.md"
  },
  {
    "id": "risk-characterization-evidence",
    "title": "Risk characterization evidence",
    "tag": "Science",
    "what_it_is": "Robust research characterizing AI risks empirically with high validity, focused on evidence usable for policy decisions. Creates benchmarks establishing clear quantitative thresholds for risks. Provides governments with clear criteria for when to intervene, producing credible empirical evidence enabling appropriate action when AI capabilities increase.",
    "why_it_matters": "- Policy decisions require evidence, not speculation\n- Current risk claims range from dismissive to apocalyptic with little empirical grounding\n- Validated metrics give policymakers actionable intervention criteria rather than choosing between competing narratives",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #48: Characterizing AI Risks with Evidence](../sources/peregrine-2025/interventions/peregrine-048.md)",
    "file": "interventions-new/risk-characterization-evidence.md"
  },
  {
    "id": "risk-model-evaluation",
    "title": "Risk-Model Evaluation",
    "tag": "Science",
    "what_it_is": "Evaluation frameworks that test AI risk assessment systems by creating methodologies to identify, classify, and prioritize potential AI mistakes based on their consequences. The evaluations measure how well risk models detect both obvious and subtle errors in high-stakes contexts, quantifying false positive and false negative rates to enable calibrated trade-offs between intervention frequency and risk exposure.",
    "why_it_matters": "- Creates foundation for reliable triage systems that focus human attention on highest-priority risks\n- Replaces ad-hoc risk assessment with systematic, quantifiable approaches\n- Enables resource allocation based on measured detection accuracy rather than intuition",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #52: Risk-Model Evaluation](../sources/peregrine-2025/interventions/peregrine-052.md)",
    "file": "interventions-new/risk-model-evaluation.md"
  },
  {
    "id": "rsp-implementation",
    "title": "RSP Implementation",
    "tag": "Society",
    "what_it_is": "Transforming existing Responsible Scaling Policies (RSPs) from theoretical frameworks into actionable implementation plans. Develops concrete, tested \"if-then commitments\" with ready-to-deploy protocols for when safety boundaries are approached. Builds social mechanisms including public accountability and transparency requirements to ensure compliance. Significant focus on creating industry-wide norms and social pressure that makes non-compliance reputationally costly.",
    "why_it_matters": "- Current RSPs are theoretical commitments without tested implementation\n- Without concrete protocols and enforcement mechanisms, commitments may not be honored under pressure\n- Social pressure mechanisms complement technical and legal enforcement",
    "current_state": "- **Status**: Pilot/Deployed (varies by lab)\n- **Recent developments (2024-2025)**:\n  - Anthropic: October 2024 RSP update introduced safety case methodology, new capability thresholds, board approval requirements, Long Term Benefit Trust consultation\n  - OpenAI: \"Preparedness Framework\" approach with capability-based risk assessment\n  - Google DeepMind: Frontier Safety Framework (2024), anticipates deceptive alignment in near-future models\n  - Microsoft: Frontier Governance Framework (2025)\n  - Meta: Frontier AI Framework (2025), focused on catastrophic misuse by human actors\n  - Amazon: Frontier Model Safety Framework (2025)\n- **Convergence**: All major labs now use capability-based thresholds classifying models by dangerous capabilities\n- **Divergence**: DeepMind/Anthropic anticipate deceptive alignment; Meta/Amazon focus on misuse by humans\n- **Bottlenecks**: Implementation varies across labs; commitments untested under real pressure; \"promissory note\" criticism",
    "whos_working_on_it": "- **Anthropic**: Pioneered RSPs (September 2023), most detailed framework with AI Safety Levels (ASL)\n- **OpenAI**: Preparedness Framework\n- **Google DeepMind**: Frontier Safety Framework\n- **Microsoft, Meta, Amazon**: Published frameworks in 2025\n- **AI Lab Watch**: Tracking and comparing RSP commitments across labs",
    "sources": "- [Peregrine 2025 #92: Enhanced Responsible Scaling Implementation](../sources/peregrine-2025/interventions/peregrine-092.md)",
    "file": "interventions-new/rsp-implementation.md"
  },
  {
    "id": "safety-benchmarks",
    "title": "Safety benchmark development",
    "tag": "Science",
    "what_it_is": "Comprehensive high-effort benchmarks for evaluating AI safety and security. Many safety benchmarks have been defunded or discontinued. Develops rigorous, transparent methodologies for assessing various dimensions of AI safety. Aims to provide common language for discussing safety progress across different models and organizations.",
    "why_it_matters": "- Without agreed benchmarks, no way to measure safety progress or compare approaches\n- Labs can claim models are \"safe\" without standard meaning\n- High-quality benchmarks create accountability with objective measures enabling comparison across models and time",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Previous benchmarks defunded/discontinued; skepticism about benchmark validity for actual safety",
    "whos_working_on_it": "- Not specified (many safety benchmarks discontinued)",
    "sources": "- [Peregrine 2025 #50: Safety Benchmark Development](../sources/peregrine-2025/interventions/peregrine-050.md)",
    "file": "interventions-new/safety-benchmarks.md"
  },
  {
    "id": "safety-research-synthesis",
    "title": "Safety research synthesis",
    "tag": "Science",
    "what_it_is": "Better information-sharing mechanisms for topics where experts hold dramatically different assessments (e.g., mechanistic interpretability). Involves publishing analyses that clarify current cruxes in technical debates and generating evidence to resolve them rather than letting uncertainty persist indefinitely. Pays down research debt from fragmented discussions with little consensus-building.",
    "why_it_matters": "- Key disagreements persist without resolution; experts can't evaluate each other's claims\n- Resources wasted on parallel efforts that could be coordinated\n- Systematic synthesis identifies which uncertainties actually matter and creates shared understanding",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #28: Synthesis, Consensus, and Paying Down Research Debt](../sources/peregrine-2025/interventions/peregrine-028.md)",
    "file": "interventions-new/safety-research-synthesis.md"
  },
  {
    "id": "safety-technique-meta-research",
    "title": "Safety technique meta-research",
    "tag": "Science",
    "what_it_is": "Research evaluating and validating effectiveness of different AI safety approaches. Multiple independent actors assess the same models using different techniques, comparing results to determine reliability and blind spots. Moves alignment from piecemeal theoretical approaches toward empirical safety science with repeatable, testable methodologies that scale with capabilities.",
    "why_it_matters": "- We don't actually know which safety techniques work\n- Different groups use different methods and report different results with no systematic comparison\n- Meta-research identifies which techniques are actually effective rather than just theoretically promising",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Coordination across independent research groups",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #47: Meta-Research on Safety Techniques](../sources/peregrine-2025/interventions/peregrine-047.md)",
    "file": "interventions-new/safety-technique-meta-research.md"
  },
  {
    "id": "sandbagging-detection",
    "title": "Evaluation sandbagging detection",
    "tag": "Security",
    "what_it_is": "Methodologies and tools to identify when AI systems deliberately underperform on safety evaluations for strategic purposes. Sandbagging can be developer-initiated (explicit intervention) or AI-initiated (unintended model behavior). Key techniques:\n\n- **Password-locking detection**: Models can be fine-tuned to hide capabilities unless given a password\u2014generalizes to benchmarks like WMDP\n- **Noise injection**: Inject noise into model weights; sandbagging models show anomalous performance improvements (disrupts underperformance mechanisms while core capabilities remain intact)\n- **On-distribution finetuning**: Single-sample finetuning on evaluation tasks can elicit hidden capabilities, but creates false positive risk\n- **Evaluation-awareness monitoring**: Check if models reason about being evaluated (precursor to strategic sabotage)",
    "why_it_matters": "- If models can strategically underperform on safety evaluations, the entire evaluation paradigm breaks\n- A model might hide dangerous capabilities during testing and reveal them in deployment\n- Detecting sandbagging is essential for evaluations to remain meaningful\n- Current capabilities: models can already reason about being evaluated",
    "current_state": "- **Status**: Research (active development)\n- **Recent findings (2025)**:\n  - Noise injection classifier achieves 100% accuracy distinguishing sandbagging vs. standard configurations (llama70B, deepseek-7b, mistral7B)\n  - METR GPT-5 evaluation (August 2025): No evidence of sandbagging, but model can reason about being evaluated\u2014\"should be closely monitored\"\n  - Auditing game (December 2025): Detection remains difficult without training access; only on-distribution finetuning reliably removes sandbagging\n  - Demonstrations generalize well to removing sandbagging across distribution shifts\n- **Bottlenecks**: Detection without training access unreliable (1/5 models correctly classified); transcript analysis and prompting ineffective against sophisticated sandbagging",
    "whos_working_on_it": "- **METR**: GPT-5 sandbagging evaluation, ongoing research\n- **Redwood Research**: Countermeasures research, password-locked model experiments\n- **Academic groups**: Noise injection detection methods, auditing games",
    "sources": "- [Peregrine 2025 #44: Evaluation Sandbagging Detection](../sources/peregrine-2025/interventions/peregrine-044.md)",
    "file": "interventions-new/sandbagging-detection.md"
  },
  {
    "id": "secure-ai-facilities",
    "title": "Secure AI facilities",
    "tag": "Security",
    "what_it_is": "High-security infrastructure for frontier AI development, testing, and government oversight. Two complementary components:\n\n**Frontier-grade data centers (SL4-SL5)**: Data centers meeting SL4-SL5 security standards for frontier AI development. SL4 provides SCIF-level security (Secure Compartmentalized Information Facilities) with confidential computing and Hardware Security Modules. SL5 goes further, designed to resist \"top-priority operations by the world's most capable nation-state actors.\" Pilots are needed to develop practical SL5 protocols while reducing operational costs.\n\n**Government AI facilities**: Secure government data centers capable of hosting model weights and other sensitive AI assets. Serve as repositories for models deemed too dangerous for widespread distribution, or as platforms for government auditing and independent safety testing. Access protocols could be tied to government procurement contracts, creating market-based incentives for compliance with safety metrics.",
    "why_it_matters": "- Frontier AI systems may become nation-state targets requiring maximum security\n- No secure government infrastructure currently exists for holding dangerous AI models\n- Governments cannot independently verify safety claims without their own testing facilities\n- Lab automation feedback loops become dangerous as AI improves at AI research\n- Prevents catastrophic model/weight leaks and potential containment failures",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: High operational costs for SL5; unclear what SL5 means practically for AI systems; scaling SCIF-level security to frontier AI energy requirements; large capital expenditure; security clearance and classification issues; potential resistance from labs concerned about IP protection",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #61: Secure Frontier Data Centers](../sources/peregrine-2025/interventions/peregrine-061.md)\n- [Peregrine 2025 #163: SL5 Data Center Pilots](../sources/peregrine-2025/interventions/peregrine-163.md)\n- [Peregrine 2025 #112: Government Data Centers](../sources/peregrine-2025/interventions/peregrine-112.md)\n- [Atlas AI Resilience Gap Map: AI Systems as Targets](../sources/atlas-ai-resilience/proposals/ai-systems-as-targets.md)",
    "file": "interventions-new/secure-ai-facilities.md"
  },
  {
    "id": "self-other-overlap-training",
    "title": "Self-other overlap training",
    "tag": "Science",
    "what_it_is": "Fine-tuning technique that reduces AI deception by minimizing the difference between how models internally represent self-referencing scenarios vs. other-referencing scenarios. Components:\n\n- **SOO Loss calculation**: Mean Squared Error (MSE) between self-referencing and other-referencing activations at the self-attention projection layer\n- **Activation alignment**: Push model representations of \"what I would do\" toward \"what they would do\" during potentially deceptive scenarios\n- **Minimal capability impact**: Loss function targets deception-relevant representations while preserving general task performance\n\nInspired by cognitive neuroscience research on empathy\u2014large body of evidence links neural self-other overlap to pro-sociality in humans.",
    "why_it_matters": "- Deception requires self-other distinctions not needed for task performance\n- Selecting against these distinctions removes a prerequisite for deceptive behavior\n- Complements behavioral alignment (RLHF) by targeting internal representations\n- **Theoretical basis**: Many configurations in weight space have similar capabilities; SOO pushes learning toward configurations with maximal overlap, selecting for honest models",
    "current_state": "- **Status**: Research (promising early results)\n- **Recent developments (2025)**:\n  - ICLR 2025 submission (later withdrawn for revision)\n  - Yudkowsky's preliminary positive reaction\u2014given his typically critical stance, suggests the approach is \"worth further scrutiny\"\n  - Related approach: SELF-MONITOR framework achieved 43.8% reduction in deceptive alignment behaviors by enabling models to oversee their own chain-of-thought reasoning\n- **Bottlenecks**: Testing against adversarial sleeper agent scenarios; integration with other alignment methods; scaling validation",
    "whos_working_on_it": "- **AE Studio**: Primary research team. Results across model scales:\n  - Mistral-7B-Instruct-v0.2: Deceptive responses dropped 73.6% \u2192 17.2%\n  - Gemma-2-27b-it: 100% \u2192 9.3%\n  - CalmeRys-78B-Orpo-v0.1: 100% \u2192 2.7%\n  - Notably: 100% \u2192 0% on held-out scenario not in training data\n- **Alignment Forum/LessWrong community**: Active discussion and review",
    "sources": "- [Peregrine 2025 #12: Cooperative Alignment](../sources/peregrine-2025/interventions/peregrine-012.md)",
    "file": "interventions-new/self-other-overlap-training.md"
  },
  {
    "id": "standardized-model-interfaces",
    "title": "Standardized AI model access interfaces",
    "tag": "Science",
    "what_it_is": "Standardized interfaces for the AI ecosystem (like USB-C universality), enabling consistent interaction with all models regardless of origin. Currently each lab uses different interfaces, forcing auditors and researchers to negotiate separate access arrangements with each company, potentially delaying critical safety work by months. Could be implemented \"within a year.\"",
    "why_it_matters": "- Fragmented interfaces slow safety research and auditing\n- Duplicated work, delayed assessments, incompatible tools\n- Standardization enables ecosystem to move at speed of fastest developer",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Lab coordination; defining standard that accommodates different architectures; governance of standard",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #192: Access Standards](../sources/peregrine-2025/interventions/peregrine-192.md)",
    "file": "interventions-new/standardized-model-interfaces.md"
  },
  {
    "id": "strategic-ai-liability-litigation",
    "title": "Strategic AI liability litigation",
    "tag": "Society",
    "what_it_is": "Preemptive lawsuits against AI organizations deemed negligent on risk management, potentially recruiting high-profile figures (Elon Musk mentioned). Strategic lawsuits across multiple jurisdictions to establish precedents and create legal clarity around AI risk management requirements.",
    "why_it_matters": "- Legal uncertainty allows labs to ignore risks\n- Precedents clarify liability and create deterrence\n- Provides legal hooks for future regulatory action",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Legal standing; identifying viable cases; high-profile plaintiff recruitment",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #188: Legal Clarity Initiatives](../sources/peregrine-2025/interventions/peregrine-188.md)",
    "file": "interventions-new/strategic-ai-liability-litigation.md"
  },
  {
    "id": "subsidized-security-tools",
    "title": "Government-subsidized cybersecurity AI tools",
    "tag": "Security",
    "what_it_is": "Government procurement and distribution of advanced AI-powered bug-finding tools to critical infrastructure globally, prioritizing financial institutions (to prevent AI systems from acquiring funds). Treats cybersecurity as a public good, funding security improvements in countries that otherwise lack resources.",
    "why_it_matters": "- Security capabilities concentrate in wealthy organizations/nations\n- Weak links become entry points for AI-enabled attacks\n- Banks and financial institutions are priority targets (AI acquiring funds enables further operations)",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Requires government funding; international coordination for global deployment",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #166: Bug-Finding Democratization](../sources/peregrine-2025/interventions/peregrine-166.md)",
    "file": "interventions-new/subsidized-security-tools.md"
  },
  {
    "id": "tamper-resistant-rlhf",
    "title": "Tamper-resistant RLHF",
    "tag": "Security",
    "what_it_is": "Techniques to make RLHF alignment resistant to subsequent modification or reversal through fine-tuning attacks. Components:\n\n- **Adversarial training**: Train-time adversaries that simulate 64-step SFT attacks, making refusal safeguards robust to tampering\n- **Constraint-aware loss functions**: Filter harmful gradients during downstream fine-tuning\n- **Safety backdoors**: Implant alignment triggers that preserve safety even when adversarial inputs are used\n- **Safety-aligned data mixing**: Incorporate safe examples during any fine-tuning process",
    "why_it_matters": "- **Trivial to break**: GPT-3.5 Turbo's safety guardrails were jailbroken by fine-tuning on only 10 adversarial examples at cost <$0.20\n- **Near-complete removal**: Llama 2-Chat models fine-tuned with LoRA on adversarial data refused unsafe instructions only ~1% of time vs. 100% for base models\n- **Evil twin problem**: \"Until tamper-resistant safeguards are discovered, the deployment of every fine-tunable model is equivalent to also deploying its evil twin\"\n- **Open-weight necessity**: Without this, open-weight models cannot be safely released",
    "current_state": "- **Status**: Research (active, breakthrough in 2024-2025)\n- **Recent developments (2025)**:\n  - ICLR 2025: \"Tamper-Resistant Safeguards for Open-Weight LLMs\" paper accepted\u2014first systematic defense approach\n  - ICML 2025 Workshop: \"Why LLM safety guardrails collapse after fine-tuning\" analysis showing similarity between alignment/fine-tuning datasets predicts collapse\n  - New architectures: R2-Guard (knowledge-enhanced logical reasoning, ICLR 2025), DuoGuard (RL-based multilingual robustness)\n- **Bottlenecks**: Robust safeguards remain \"an unsolved problem\"; balancing tamper-resistance with model utility; scaling to larger models",
    "whos_working_on_it": "- **Rishub Tamirisa et al.**: ICLR 2025 paper authors, GitHub repo `rishub-tamirisa/tamper-resistance`\n- **Anthropic**: HH-RLHF dataset used in tamper-resistance training\n- **TUM Institute for Ethics in AI**: Guardrail robustness research\n- **alignAI**: Safety guardrail evaluation",
    "sources": "- [Peregrine 2025 #7: Tamper-Resistant RLHF](../sources/peregrine-2025/interventions/peregrine-007.md)",
    "file": "interventions-new/tamper-resistant-rlhf.md"
  },
  {
    "id": "task-decomposition",
    "title": "Task decomposition for dangerous capabilities",
    "tag": "Security",
    "what_it_is": "Architectural approach splitting dangerous AI capabilities across multiple isolated systems or users, preventing any single agent from having complete capability. Example: a bioweapon design task might be decomposed so one system handles molecular modeling, another handles synthesis planning, and neither has access to the other's outputs\u2014only a human reviewer can combine them. Interfaces are designed so compartments can contribute to beneficial tasks while preventing end-to-end execution of catastrophic operations.",
    "why_it_matters": "- Provides defense-in-depth at architecture level, not just alignment level\n- Even compromised individual systems cannot independently cause worst outcomes\n- Reduces single points of failure for high-stakes applications",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Designing decompositions that actually prevent recombination; performance overhead of compartmentalization",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #3: Task Decomposition](../sources/peregrine-2025/interventions/peregrine-003.md)",
    "file": "interventions-new/task-decomposition.md"
  },
  {
    "id": "training-data-anti-poisoning",
    "title": "Training data anti-poisoning techniques",
    "tag": "Security",
    "what_it_is": "Detection systems for suspicious data patterns, resilient training methodologies that maintain performance when portions of data are compromised, and recovery protocols for systems exposed to poisoned inputs. Requires coordination across security teams currently working in isolation on similar problems.",
    "why_it_matters": "- Data poisoning can create backdoors or biased outputs that evade standard testing\n- Compromised behavior may only manifest in specific contexts\n- Protects integrity of entire training pipeline",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Multiple security teams working in isolation; detection methods lag behind attack sophistication",
    "whos_working_on_it": "- Multiple security teams (working in isolation)",
    "sources": "- [Peregrine 2025 #165: Anti-Poisoning Techniques](../sources/peregrine-2025/interventions/peregrine-165.md)",
    "file": "interventions-new/training-data-anti-poisoning.md"
  },
  {
    "id": "training-data-attribution",
    "title": "Training data attribution",
    "tag": "Science",
    "what_it_is": "Methods to trace model behaviors back to specific training examples using techniques like influence functions and counterfactual simulation. Rather than treating the model as a black box shaped by post-training alignment, attribution lets you identify which training data caused particular tendencies and modify the source directly.",
    "why_it_matters": "- Enables root-cause fixes for alignment issues rather than patching symptoms with RLHF\n- Provides verifiable evidence for alignment claims (useful for audits and regulation)\n- Could reveal unexpected sources of problematic behaviors that post-training methods miss",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Scaling influence functions to frontier model size; processing and interpreting massive attribution results",
    "whos_working_on_it": "- **Anthropic**: Leading work, though much remains unpublished",
    "sources": "- [Peregrine 2025 #1: Training Data Attribution](../sources/peregrine-2025/interventions/peregrine-001.md)",
    "file": "interventions-new/training-data-attribution.md"
  },
  {
    "id": "training-data-licensing",
    "title": "Training Data Licensing",
    "tag": "Society",
    "what_it_is": "A regulated market for high-quality, difficult-to-replicate training data (scientific papers, professional knowledge, etc.) that cannot yet be automated. Fairly compensates those whose data is used for AI training while providing nations and regulatory bodies leverage over AI development. Companies needing specialized data must comply with regulations to maintain access, creating \"natural incentives for companies to sign up\" to oversight mechanisms.",
    "why_it_matters": "- Creates economic incentives for compliance with governance frameworks by controlling access to valuable training data\n- Provides leverage for regulators without requiring heavy enforcement\n- Addresses fairness concerns about AI profiting from others' contributions",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #95: Training Data Licensing](../sources/peregrine-2025/interventions/peregrine-095.md)",
    "file": "interventions-new/training-data-licensing.md"
  },
  {
    "id": "transmissible-vaccines",
    "title": "Transmissible vaccines",
    "tag": "Security",
    "what_it_is": "Develop self-spreading vaccines that transmit between hosts like normal viruses, providing population-level immunity without requiring individual vaccination. This physical security approach constrains biological threats at the material level rather than through software-based solutions. Requires funding for experts who take near-future AI capabilities seriously and investment in research that goes beyond current biological literature to address qualitatively different threats than anything seen before.",
    "why_it_matters": "- Software controls can be bypassed; physical constraints on biological materials are harder to circumvent\n- Could provide defense against sophisticated AI-enabled biological attacks\n- Addresses scenarios where traditional vaccination campaigns cannot keep pace with fast-spreading engineered pathogens",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Biosafety and regulatory challenges; ethical concerns about non-consensual biological intervention; technical difficulty of controlling spread dynamics; dual-use concerns",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #143: Physical Security Mechanisms](../sources/peregrine-2025/interventions/peregrine-143.md)",
    "file": "interventions-new/transmissible-vaccines.md"
  },
  {
    "id": "transport-air-filtration",
    "title": "Transport air filtration",
    "tag": "Security",
    "what_it_is": "Apply indoor air quality principles to large vehicles like aircraft and cruise ships. Environmental controls to reduce pathogen transmission during travel. Includes HEPA filtration upgrades, increased air exchange rates, and potentially UV disinfection systems adapted for vehicle environments.",
    "why_it_matters": "- Travel is a key vector for pandemic spread\n- Reducing transmission in aircraft could slow international spread of novel pathogens\n- Enclosed vehicles create high-risk transmission environments similar to buildings\n- Ships and cruise vessels have been pandemic amplifiers historically",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Weight and power constraints on aircraft; retrofit costs; coordination with vehicle manufacturers and operators; regulatory requirements across jurisdictions",
    "whos_working_on_it": "- Not specified (aircraft manufacturers have existing HEPA systems; cruise industry has updated protocols post-COVID)",
    "sources": "- [Biosecurity Engineers: Vehicle Air Filtration](../sources/biosecurity-engineers/proposals/vehicle-air-filtration.md)",
    "file": "interventions-new/transport-air-filtration.md"
  },
  {
    "id": "ultra-reliable-evaluation",
    "title": "Ultra-Reliable AI Evaluation",
    "tag": "Science",
    "what_it_is": "Engineering approaches and benchmarking methodologies designed to achieve \"five nines\" (99.999%) reliability rather than the 99% accuracy typical of current AI benchmarks. The methods efficiently identify edge cases where models fail catastrophically without requiring millions of test instances, using adversarial probing modeled on aircraft safety standards rather than Kaggle competitions.",
    "why_it_matters": "- Critical systems (medical, infrastructure, autonomous vehicles) require reliability guarantees far beyond current benchmarks\n- Current evaluation methods miss rare but catastrophic failure modes\n- Without efficient methods to find rare failures, deploying AI in high-stakes domains remains unsafe",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #55: Ultra-Reliable AI Evaluation](../sources/peregrine-2025/interventions/peregrine-055.md)",
    "file": "interventions-new/ultra-reliable-evaluation.md"
  },
  {
    "id": "uv-air-disinfection",
    "title": "UV air disinfection",
    "tag": "Security",
    "what_it_is": "Deploy ultraviolet light for continuous air disinfection in occupied spaces through two complementary approaches:\n\n**Far-UVC (222nm)**: Filtered krypton-chloride excimer lamps at 222nm wavelength. Unlike conventional 254nm UV-C (exposure limit: 6 mJ/cm\u00b2), Far-UVC has much higher safe exposure limits (ACGIH 2025: 161 mJ/cm\u00b2 for eyes, 479 mJ/cm\u00b2 for skin over 8-hour exposure) because the wavelength is absorbed by the outer dead layer of skin and tear film before reaching living cells. Can be directed throughout the room including at occupant level.\n\n**Upper-room UV (254nm)**: Ceiling-mounted fixtures using conventional 254nm UV directed at upper room air while occupants remain safely below. Achieves safety through spatial design rather than wavelength selection. Established technology with decades of evidence, currently under-deployed.\n\nBoth approaches provide continuous passive disinfection against bacteria, viruses, and fungi without pathogen-specific targeting.",
    "why_it_matters": "- Passive defense that does not require pathogen identification\u2014eliminates pathogens regardless of whether they're known or engineered\n- Works against any airborne threat including novel or vaccine-evading pathogens\n- Far-UVC achieved 99.8% reduction of airborne infectious virus in occupied rooms (2024 study)\n- Upper-room UV is deployable now without waiting for Far-UVC regulatory approval",
    "current_state": "- **Status**: Research/Pilot (Far-UVC), Deployment (Upper-room)\n- **Bottlenecks**:\n  - Far-UVC: Regulatory approval for widespread occupied-space use; production scaling for cost reduction; ozone generation requires adequate ventilation; long-term exposure studies ongoing\n  - Upper-room: Installation costs; building owner awareness; maintenance requirements; incorrect installation reducing effectiveness\n- **Recent progress (2024-2025)**:\n  - 3-year eye safety study showed no damage in workers under daily Far-UVC exposure (Sugihara et al., 2024)\n  - 2024 room study: 99.8% airborne virus reduction (murine norovirus) within regulatory limits\n  - 2025 corneal model study confirmed Far-UVC effects limited to outermost layers, no damage to regenerative cells\n  - LED development: 10% wall-plug efficiency prototypes scheduled for autumn 2025, commercial devices expected 2027\n  - 2025 safety review: \"high ability\" pathogen kill rate, \"high level of safety,\" long-term studies continue",
    "whos_working_on_it": "- **UV Medico**: Commercial Far-UVC fixtures\n- **Columbia University (David Brenner lab)**: Foundational Far-UVC safety research\n- **Far UV Technologies**: Commercial Far-UVC deployment\n- **CDC**: Upper-room UV guidelines and research\n- Multiple academic groups validating efficacy and safety",
    "sources": "- [Peregrine 2025 #136: UV Technology](../sources/peregrine-2025/interventions/peregrine-136.md)\n- [Concrete Biosecurity Projects: Sterilization Technology](../sources/concrete-biosecurity-ea/proposals/sterilization-technology.md)\n- [Biosecurity Engineers: Far-UVC Deployment](../sources/biosecurity-engineers/proposals/far-uvc.md)\n- [Biosecurity Engineers: Upper-Room UVGI](../sources/biosecurity-engineers/proposals/upper-room-uvgi.md)",
    "file": "interventions-new/uv-air-disinfection.md"
  },
  {
    "id": "verified-kinetic-actuators",
    "title": "Verified Kinetic Actuators",
    "tag": "Security",
    "what_it_is": "Provably safe mechanisms for AI systems to interact with physical environments through robotics and other actuators. Components:\n\n- **End-to-end timing analysis**: Verify cause-effect chains from sensing to actuation meet real-time constraints\n- **Lyapunov function learning**: Neural networks learn mathematical stability proofs, paired with logic-based verification\n- **Formal controller models**: Capture safety controller behavior to verify against safety properties\n- **Runtime verification**: Monitor actual system behavior against formal specifications during operation\n\nCovers behavioral properties (safety, liveness) and real-time properties (schedulability, bounded response).",
    "why_it_matters": "- Software failures in physical systems could have catastrophic consequences\n- Without verification, AI control of robots and machinery creates unacceptable risk profiles\n- Physical AI actions are irreversible in ways software actions often aren't\n- **Certification requirement**: Testing and simulation alone are insufficient to certify autonomous robotics",
    "current_state": "- **Status**: Research (active, maturing)\n- **Recent developments (2025)**:\n  - **Waterloo Framework (October 2025)**: New approach combining mathematics and machine learning to verify safety and stability of AI systems controlling critical infrastructure (power grids, autonomous vehicles)\n  - **ROS 2 Verification (July 2025)**: Model-based methodology automating formal verification of robotic applications using model-driven engineering techniques\n  - **Agricultural Robot Verification (October 2025)**: Complete development lifecycle methodology\u2014from hazard analysis through runtime verification\n- **Bottlenecks**: Complexity of formal methods; manual effort for model creation; bridging robotics, formal methods, and real-time systems communities",
    "whos_working_on_it": "- **University of Waterloo**: Framework combining neural networks with logic-based verification for AI controllers\n- **CMU Robotics Institute**: Formal verification of robotic systems\n- **ROS 2 research community**: Model-based automation of verification\n- **Frontiers in Robotics and AI**: Multiple published verification frameworks",
    "sources": "- [Peregrine 2025 #62: Verified Kinetic Actuators](../sources/peregrine-2025/interventions/peregrine-062.md)",
    "file": "interventions-new/verified-kinetic-actuators.md"
  },
  {
    "id": "vulnerability-research-matching",
    "title": "Vulnerability-research matching",
    "tag": "Science",
    "what_it_is": "System that directly connects discovered AI vulnerabilities to research proposals addressing them. Ensures funding flows to researchers tackling actual identified problems rather than theoretical concerns. Continuously updates as new vulnerabilities are discovered through red teaming, creating rapid feedback loop between threat identification and solution development.",
    "why_it_matters": "- Safety research funding is disconnected from actual discovered vulnerabilities\n- Red teams find problems but funding goes to proposals based on different criteria\n- Matching system ensures resources go directly to solving identified problems",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Not specified",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #49: Vulnerability Matching](../sources/peregrine-2025/interventions/peregrine-049.md)",
    "file": "interventions-new/vulnerability-research-matching.md"
  },
  {
    "id": "wearable-infection-detection",
    "title": "Wearable infection detection",
    "tag": "Security",
    "what_it_is": "Create next-generation wearables for host response-based diagnosis that detect physiological anomalies indicating infection without identifying the specific pathogen. Rather than competing with commercial wearables, the approach identifies measurements beyond current capabilities and operationalizes existing devices for an early warning biodefense ecosystem. Leverages existing wearable technology momentum and user acceptance while developing missing components. Addresses the fundamental challenge: \"What do you look for when you don't know what you're looking for?\"",
    "why_it_matters": "- Novel or engineered pathogens may evade specific diagnostic tests\n- Host response detection works against any biological threat including unanticipated ones\n- Distributed detection through consumer devices creates population-level early warning system",
    "current_state": "- **Status**: Research\n- **Bottlenecks**: Identifying physiological signatures that reliably indicate early infection; false positive rates in population-scale deployment; privacy concerns; integration with existing wearable platforms",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #141: Global Immune System](../sources/peregrine-2025/interventions/peregrine-141.md)",
    "file": "interventions-new/wearable-infection-detection.md"
  },
  {
    "id": "whistleblower-protection-fund",
    "title": "Whistleblower protection fund",
    "tag": "Society",
    "what_it_is": "Large, long-lived fund (several hundred million dollars) to secure livelihoods of potential whistleblowers for a decade and cover major legal exposure. Primarily protects AI lab employees, but also applies to government officials who refuse orders they believe endanger public safety related to AI development.",
    "why_it_matters": "- Insiders with knowledge of dangerous practices face career destruction and legal liability\n- Credible protection removes barrier to information flow\n- Could prevent catastrophic decisions through early disclosure",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Fund capitalization (hundreds of millions); legal structure; credibility with potential whistleblowers",
    "whos_working_on_it": "- **AIWI**: AI whistleblower initiatives mentioned",
    "sources": "- [Peregrine 2025 #189: Whistleblower Protection Fund](../sources/peregrine-2025/interventions/peregrine-189.md)",
    "file": "interventions-new/whistleblower-protection-fund.md"
  },
  {
    "id": "workforce-displacement-planning",
    "title": "Workforce displacement response planning",
    "tag": "Society",
    "what_it_is": "Coordinated frameworks for identifying vulnerable sectors, quantifying displacement timelines, and deploying retraining programs before mass job loss occurs. The approach brings together government agencies, educational institutions, industry, and labor organizations to create sector-specific transition plans. Analysis suggests large-scale displacement risk within 3 years, with K-curve inequality effects potentially accelerating within 18 months.",
    "why_it_matters": "- Social safety nets could fail faster than institutions can respond\n- Political backlash from unmanaged displacement undermines AI governance capacity\n- Early identification enables targeted interventions in vulnerable sectors",
    "current_state": "- **Status**: Idea\n- **Bottlenecks**: Requires multi-stakeholder coordination; no clear institutional home",
    "whos_working_on_it": "- Not specified",
    "sources": "- [Peregrine 2025 #153: AI Displacement Response Planning](../sources/peregrine-2025/interventions/peregrine-153.md)",
    "file": "interventions-new/workforce-displacement-planning.md"
  }
]