# Model distillation limits

**Tag**: Science

## What it is
Research into theoretical limits of AI capabilities, specifically whether there are fundamental physics-based constraints on model distillation. Investigates whether small, easily-diffused models can always capture capabilities of larger systems, or if certain capabilities inherently require large-scale infrastructure. Also analyzes hardware bottlenecks (GPU interconnect, test-time compute) to predict capability jumps.

## Why it matters
- If dangerous capabilities can always be distilled into small shareable models, containment strategies are fundamentally limited
- If physics constrains distillation, some capabilities require infrastructure that can be monitored
- Understanding these limits determines which governance strategies are even possible

## Current state
- **Status**: Idea
- **Bottlenecks**: Not specified

## Who's working on it
- Not specified

## Sources
- [Peregrine 2025 #24: Limits of Model Distillation](../sources/peregrine-2025/interventions/peregrine-024.md)
