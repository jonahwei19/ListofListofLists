# AI evaluation capacity

**Tag**: Science

## What it is
Building evaluation capacity for AI systems through two complementary approaches:

**AI evaluator training**: Train specialized investigator AI agents to analyze the vast data generated by frontier models: millions of neuron activations, extensive training datasets, and complex interaction logs. Moves beyond benchmark-focused evaluations toward holistic understanding of model capabilities in real-world contexts. Current approaches (e.g., cybersecurity evals with small predefined task sets) miss critical behaviors until deployment.

**Dedicated evaluation companies**: Organizations that perform specialized AI security evaluations, including cybersecurity and offensive capability assessments. These companies fill critical functions in the evaluation landscape that could otherwise become bottlenecks, operating as relatively small investments with outsized ecosystem impact.

## Why it matters
- Scale of data from modern AI systems exceeds human analysis capacity
- AI systems trained specifically to evaluate other AI could catch risks current approaches miss
- Without dedicated evaluation organizations, the ecosystem lacks capacity to properly assess AI systems
- Prevents evaluation from becoming a chokepoint in safety infrastructure

## Current state
- **Status**: Research (AI evaluators), Deployed (evaluation companies)
- **Bottlenecks**: Funding sustainability for companies; technical development for AI-based evaluation

## Who's working on it
- **METR**: Autonomous AI capability evaluations; partners with UK AISI and NIST AI Safety Institute Consortium; evaluates catastrophic risk from self-improvement and rogue replication; developed task-length metric showing AI agent capability doubles every ~7 months
- **Apollo Research**: Scheming and deception evaluations; interpretability research; partners with frontier labs, governments, foundations; developed safety case frameworks for AI scheming
- **Redwood Research**: AI control evaluations; advises DeepMind and Anthropic; partnered with UK AISI on control safety cases; demonstrated LLMs can strategically fake alignment during training
- **UK AI Safety Institute**: Government evaluation capacity; collaborates with METR, Apollo, Redwood on scheming evaluations
- **US AI Safety Institute (NIST)**: Federal evaluation infrastructure
- **SecureBio**: Biosecurity-specific evaluations
- **Future of Life Institute**: AI Safety Index scoring frontier developers on 33 indicators across 6 domains

## Sources
- [Peregrine 2025 #45: Training AI Evaluators](../sources/peregrine-2025/interventions/peregrine-045.md)
- [Peregrine 2025 #58: Evaluation Companies](../sources/peregrine-2025/interventions/peregrine-058.md)
