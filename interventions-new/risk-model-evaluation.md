# Risk-Model Evaluation

**Tag**: Science

## What it is
Evaluation frameworks that test AI risk assessment systems by creating methodologies to identify, classify, and prioritize potential AI mistakes based on their consequences. The evaluations measure how well risk models detect both obvious and subtle errors in high-stakes contexts, quantifying false positive and false negative rates to enable calibrated trade-offs between intervention frequency and risk exposure.

## Why it matters
- Creates foundation for reliable triage systems that focus human attention on highest-priority risks
- Replaces ad-hoc risk assessment with systematic, quantifiable approaches
- Enables resource allocation based on measured detection accuracy rather than intuition

## Current state
- **Status**: Research
- **Bottlenecks**: Not specified

## Who's working on it
- Not specified

## Sources
- [Peregrine 2025 #52: Risk-Model Evaluation](../sources/peregrine-2025/interventions/peregrine-052.md)
