# Misalignment definitions and measures

**Tag**: Science

## What it is
Sharpen formal understanding of how AI systems pursue goals in ways that undermine user intent. Includes refining frameworks for specification gaming and goal drift, clarifying what counts as alignment failure, and developing practical metrics for detecting loophole exploitation. Creates shared taxonomies that can be operationalized.

## Why it matters
- Without clear definitions, field can't measure progress or agree on whether systems are aligned
- Shared taxonomies enable coordination across research groups
- Regulatory bodies need clear standards; practical metrics allow tracking whether alignment improves with scale

## Current state
- **Status**: Research
- **Bottlenecks**: Not specified

## Who's working on it
- Not specified

## Sources
- [Peregrine 2025 #25: Misalignment Definitions and Measures](../sources/peregrine-2025/interventions/peregrine-025.md)
