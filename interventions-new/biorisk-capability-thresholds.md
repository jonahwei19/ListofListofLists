# Biorisk capability thresholds

**Tag**: Security

## What it is
Conduct empirical wet lab studies to establish how specific AI capabilities translate to real-world biological risk increases. These studies would quantify the conditional impact of AI capabilities demonstrated in benchmarks, providing concrete thresholds for when open model releases pose unacceptable biosecurity risks. AI systems are approaching capability levels where they could significantly increase bio-risks, potentially within months. Without empirically-grounded thresholds, policy decisions about model release are based on speculation.

## Why it matters
- No current empirical basis for determining what level of AI biological capability is dangerous
- Could increase risk of non-state actor bio-attacks by an order of magnitude if open models cross unknown thresholds
- Concrete thresholds enable evidence-based policy on model release decisions

## Current state
- **Status**: Research
- **Bottlenecks**: Wet lab studies are expensive and require biosafety infrastructure; ethical and safety concerns about the research itself; dual-use concerns about publishing results; timeline pressure (capabilities advancing faster than research)

## Who's working on it
- Not specified

## Sources
- [Peregrine 2025 #140: Biorisk Thresholds for Open Models](../sources/peregrine-2025/interventions/peregrine-140.md)
