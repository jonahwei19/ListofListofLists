# Compute caps for training runs

**Tag**: Policy

## What it is
Regulatory limits on computational resources available for AI training runs, enforced through licensing requirements or hardware-level throttling.

**Mechanism**:
- Define FLOPS threshold above which training runs require special license (e.g., 10^26 FLOPS)
- Data centers above capacity threshold must obtain permits for large training runs
- Enforcement options: power usage monitoring at facilities, hardware telemetry, on-site audits
- Violations trigger penalties: fines, facility shutdown, hardware seizure
- International treaties could harmonize caps across jurisdictions

**Why caps vs. other approaches**: Directly limits capability advancement regardless of lab intentions. Other approaches (safety testing, alignment research) don't bound what capabilities get developed.

## Why it matters
- Directly slows frontier capability advancement
- Buys time for safety research and governance to catch up
- Addresses racing dynamics at infrastructure level rather than relying on voluntary restraint
- Creates clear, measurable compliance standard

## Current state
- **Status**: Idea (politically controversial)
- **Bottlenecks**: Strong industry pushback expected; enforcement mechanisms unclear; international coordination required to prevent regulatory arbitrage; defining appropriate threshold levels; exemptions for beneficial uses (medical AI, climate modeling)

## Who's working on it
- **Policy researchers**: Academic proposals
- **No active implementation efforts identified**

## Sources
- [Peregrine 2025 #63: Speed Limits in Data Centers](../sources/peregrine-2025/interventions/peregrine-063.md)
