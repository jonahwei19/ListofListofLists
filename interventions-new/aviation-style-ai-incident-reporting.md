# Aviation-style AI incident reporting

**Tag**: Security

## What it is
Mandatory, standardized reporting framework for AI failures and near-misses, modeled on aviation safety systems.

**Mechanism**:
- Labs and deployers required to report AI incidents to central database within defined timeframe
- Whistleblower protections for employees who report safety issues (immunity from retaliation)
- Standardized taxonomy for severity classification and incident categories
- Reports anonymized to encourage disclosure without competitive harm
- OECD framework: 29 key characteristics refined from 88 criteria
- Risk vs incident distinction: incident = realized harm, risk = potential precursor

**Why aviation as model**: Aviation's exceptional safety record directly attributable to mandatory reporting. Near-miss reporting catches problems before crashes.

## Why it matters
- Without standardized reporting, lessons from incidents remain siloed
- Same mistakes get repeated across different labs
- Aviation's safety record demonstrates value of mandatory, protected reporting
- Creates early warning system for emerging AI risks

## Current state
- **Status**: Pilot/Deployed (international framework developing)
- **Recent developments (2025)**:
  - OECD released "Towards a Common Reporting Framework for AI Incidents" (February 2025)
  - OECD AI Incidents Monitor (AIM) tracks incidents in real-time from press reports since 2023
  - AI Incident Database (AIID) free and open-source
- **Bottlenecks**: Lab buy-in for proactive reporting; liability concerns; regulatory implementation

## Who's working on it
- **OECD**: Common reporting framework and AI Incidents Monitor (AIM)
- **Responsible AI Collaborative**: AI Incident Database (AIID)

## Sources
- [Peregrine 2025 #87: Incident Reporting](../sources/peregrine-2025/interventions/peregrine-087.md)
