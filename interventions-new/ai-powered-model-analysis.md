# AI-powered model analysis

**Tag**: Science

## What it is
Training specialized AI systems to analyze the vast data generated by frontier models, enabling evaluation at scale beyond human capacity.

**Mechanism**:
- Train investigator AI agents on labeled examples of concerning vs. benign model behavior
- Agents analyze: neuron activations across millions of parameters, training data patterns, interaction logs
- Focus on holistic understanding of capabilities in real-world contexts (not just predefined benchmarks)
- Can process data volumes impossible for human analysts (millions of neuron activations per inference)
- Identifies capability patterns that emerge only at scale or in specific contexts

**Why AI-based vs human evaluation**: Current benchmarks test predefined task sets and miss critical behaviors until deployment. Human analysts can't process the data volume from modern models.

## Why it matters
- Scale of data from modern AI systems exceeds human analysis capacity by orders of magnitude
- AI evaluators could catch risks that current benchmark approaches miss
- Enables continuous monitoring rather than periodic evaluation
- Can detect emergent capabilities before intentional elicitation

## Current state
- **Status**: Research
- **Bottlenecks**: Training data for evaluator AI (what constitutes "concerning" behavior); avoiding Goodhart's law (models learning to fool evaluators); evaluator capability relative to models being evaluated

## Who's working on it
- **Research direction**: Active at multiple labs, no single dedicated organization

## Sources
- [Peregrine 2025 #45: Training AI Evaluators](../sources/peregrine-2025/interventions/peregrine-045.md)
