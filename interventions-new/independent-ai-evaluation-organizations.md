# Independent AI evaluation organizations

**Tag**: Science

## What it is
Dedicated third-party organizations performing AI safety evaluations, including capability assessments, security audits, and dangerous capability red-teaming.

**Mechanism**:
- Organizations operate independently from AI developers (no conflicts of interest)
- Perform specialized evaluations: cybersecurity, autonomous capabilities, scheming detection, biosecurity risks
- Partner with governments for official evaluation capacity
- Develop standardized evaluation methodologies that can be applied across labs
- Small investments ($5-20M) with outsized ecosystem impact (evaluation is a bottleneck)

**Why third-party vs lab-internal**: Labs have incentives to underreport concerning findings. Independent evaluation creates accountability and trust.

## Why it matters
- Without dedicated evaluation organizations, ecosystem lacks capacity to properly assess AI systems
- Prevents evaluation from becoming a chokepoint in safety infrastructure
- Creates external accountability for AI developers
- Enables governments to verify lab claims independently

## Current state
- **Status**: Deployed (multiple organizations active)
- **Bottlenecks**: Funding sustainability; access to frontier models for evaluation; keeping pace with capability advancement

## Who's working on it
- **METR**: Autonomous capability evaluations; task-length metric showing AI agent capability doubles every ~7 months
- **Apollo Research**: Scheming and deception evaluations; safety case frameworks
- **Redwood Research**: AI control evaluations; demonstrated LLMs can strategically fake alignment
- **UK AI Safety Institute**: Government evaluation capacity
- **US AI Safety Institute (NIST)**: Federal evaluation infrastructure
- **SecureBio**: Biosecurity-specific evaluations
- **Future of Life Institute**: AI Safety Index scoring frontier developers

## Sources
- [Peregrine 2025 #58: Evaluation Companies](../sources/peregrine-2025/interventions/peregrine-058.md)
