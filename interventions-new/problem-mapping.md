# Problem mapping

**Tag**: Science

## What it is
Comprehensive list of critical AI safety problems that, if solved, would result in aligned AI systems. Goes beyond lists like Open Philanthropy's RFP by presenting an overarching framework justifying why solving the listed problems would lead to alignment. Systematically identifies research areas that collectively guarantee safety.

## Why it matters
- Current safety research is fragmented: problems studied without knowing if solving them produces alignment
- Comprehensive map enables strategic resource allocation and identifies neglected areas
- Provides confidence that solving the right problems actually works

## Current state
- **Status**: Idea
- **Bottlenecks**: Not specified

## Who's working on it
- **Open Philanthropy/Coefficient Giving**: Partial (RFP list without overarching framework)

## Sources
- [Peregrine 2025 #30: Problem Mapping](../sources/peregrine-2025/interventions/peregrine-030.md)
