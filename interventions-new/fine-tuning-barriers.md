# Fine-Tuning Barriers

**Tag**: Security

## What it is
Technical and policy barriers that make unauthorized fine-tuning of advanced AI models extremely difficult. Technical measures include secure hardware enclaves, cryptographic verification of model origins, and detection systems for identifying unauthorized derivatives. Policy frameworks establish legal consequences for circumventing protections while providing legitimate access paths for authorized research.

## Why it matters
- Without barriers, safety measures can be fine-tuned away by anyone with access to model weights
- Negates the value of alignment work if safety training is easily removed
- Creates enforcement mechanism for maintaining safety properties

## Current state
- **Status**: Research
- **Bottlenecks**: Not specified

## Who's working on it
- Not specified

## Sources
- [Peregrine 2025 #93: Barriers to Fine-tuning](../sources/peregrine-2025/interventions/peregrine-093.md)
- [Atlas AI Resilience Gap Map: Post-Deployment Modification](../sources/atlas-ai-resilience/proposals/post-deployment-modification.md)
