# Michael Nielsen: How to be a wise optimist about science and technology?

**Source:** [michaelnotebook.com/optimism](https://michaelnotebook.com/optimism/index.html) (December 2024)

## Main Thesis

The central problem of the 21st century is how civilization co-evolves with science and technology. Nielsen argues that the right response to existential risk is neither denial (naive optimism) nor paralysis (doomerism), but *wise optimism*: accepting that serious risks exist, then actively and imaginatively working to overcome them.

The key insight: good futures are made by wise optimists; bad futures are made by foolish optimists. Denying xrisk while building powerful technologies is not optimism---it's the intellectual equivalent of responding to a cancer diagnosis with denial.

## Key Concepts

### The Armageddon vs. Kumbaya Heuristics

Two competing models of how technology affects civilizational risk:

**Armageddon:** As understanding deepens, individuals gain ever more power to do harm, eventually reaching catastrophic thresholds. Science democratizes Armageddon.

**Kumbaya:** While individual power grows, so do our coordination mechanisms, empathy, and pro-social institutions. The effective ability to do harm stays bounded because we keep bending the curve with better safety systems.

Nielsen's position: both are partially true. The Kumbaya graph is a *tremendous achievement* of existing institutions, not an inevitable outcome. The question is whether we can continue bending that curve, especially if ASI dramatically accelerates capability growth.

### Recipes for Ruin

A simple, inexpensive, easy-to-follow recipe that a typical individual can use to cause catastrophic damage. Whether these exist is a crux: those who find them plausible worry about xrisk; those who don't often dismiss it.

**Dominant recipes for ruin** are recipes against which there are no plausible defenses except suppressing the underlying knowledge. Grey goo-style scenarios have happened before (the great oxygenation event), suggesting the universe permits such things.

### The Alignment Problem (Generalized)

Nielsen expands "alignment" beyond AI systems to civilization-level: "the problem of aligning a civilization so it ensures differential technological development, while preserving liberal values."

This isn't a one-time problem but an ongoing, systematic challenge. Even a pure machine civilization would face it---what prevents the Terminators from turning on each other? The extent to which any society flourishes is determined by how well it solves this.

### Market vs. Non-Market Safety

**Market-supplied safety:** Safety that aligns with commercial interests. When dangers are immediate and legible to consumers (airplane crashes, product defects), markets supply safety well.

**Non-market safety:** Safety that opposes or isn't served by commercial interests---externalities, long-term risks, diffuse harms, damage to commons. This is systematically undersupplied.

Technical AI alignment work mostly falls into market safety: it makes systems more attractive to consumers, which *accelerates* development. The harder parts---non-market safety---are what get neglected, and what the companies are incentivized to deny exist.

### Coceleration

Nielsen's alternative to the accel/decel binary: "the development of both safety and capabilities, subject to safety being kept sufficiently well supplied." A verb form of Differential Technological Development.

Cocelerationists take xrisk seriously (like decels) while also taking seriously the enormous promise of ASI and the damage that excessive caution can cause (like accels). They believe non-market safety is currently undersupplied and that increasing its supply is the key challenge.

### Hyper-entities

Imagined future objects that don't exist but coordinate collective action toward their creation. AGI is a hyper-entity---people organize their lives around something currently imaginary. The supply of good hyper-entities determines which futures we can collectively pursue. Imagination is upstream of prediction.

## Relevance to Flourishing

Nielsen's vision of a good future: "a loving plurality of posthumanities"---humans coexisting with many new types of sentience, in a state of flourishing. Not human extinction, but human inclusion alongside vast new possibilities.

What makes technological futures good:
- Safety adequate to the power available
- Continued ability to solve the Alignment Problem
- Liberal values preserved (pluralism, individual freedom, distributed power)
- Imagination and meaning, not just material abundance (he notes tension: "abundance is intrinsically in tension with meaning, since important scarcities concentrate attention")

The Kumbaya heuristic matters because it highlights that flourishing isn't about preventing all risk---it's about continuously bending the curve, adapting faster than dangers emerge. A civilization that merely freezes in place stagnates and fails differently.

## Contrast with Doomerism AND Naive Optimism

### vs. Naive Optimism (Accels)

Accels treat the Kumbaya outcome as near-inevitable rather than as a tremendous achievement requiring ongoing effort. They free-ride on safety work while celebrating the decimation of safety teams. Their optimism is "foolish" because it denies real risks---like the asbestos and leaded gasoline optimists of earlier eras.

"A certain kind of free market believer tends to believe safety will be well supplied by the market... This is well illustrated by partial failures of the point." The market supplies safety well for immediate, legible harms; poorly for diffuse, long-term, or externalized ones.

### vs. Doomerism (Decels)

Decels correctly take xrisk seriously but are often pessimistic that anything can be done except stopping. Nielsen's shift: he began as instinctively decelerationist but now sees coceleration as worth developing.

Key critique of doomer epistemics: "There's often hubris in pessimism. It's easy to confuse 'I [and my friends] don't know of a solution' with 'there is no solution.'" Civilizational problems are often solved in ways anticipated by very few people in advance.

Just-in-time safety is scary, but historically wildly successful. Nuclear war, ozone hole, acid rain---all seemed existential to earlier generations, and all have been partially addressed by mechanisms few predicted.

### The Shared Failure

Both camps fail to take seriously that safety is not a system property---it's a property of a system *in a particular environment*. Technical alignment, even if fully solved, means little if non-aligned systems are easily built by others. "Governance" elides that the environment includes the entire world, including the laws of physics and biology.

## Concrete Suggestions

### Institutional

- **Pigouvian AI danger taxes:** Tax the externality of AI development to internalize harms. Hard because the danger is illegible, but worth working on in narrow cases (misinformation, security vulnerabilities).
- **Provably Beneficial Surveillance:** Can surveillance + policing be compatible with liberal values AND sufficient to prevent catastrophic threats? Not obvious but worth exploring. Cryptographic tools (homomorphic encryption, zero-knowledge proofs) may help balance power.
- **Vision Prize:** A prize for beneficial visions of the future, to increase the supply of good hyper-entities.

### For Individuals

- Don't work for AGI organizations unless they're making *major sacrifices for non-market safety right now*. "We'll do the right thing in the future" is cheap. Nielsen is skeptical any leading companies currently pass this test.
- Work on differential technological development broadly---climate, biosafety, nuclear safety, improving the commons.
- Develop a "safety litmus test" for organizations: Are they willing to sacrifice growth for non-market safety? Not in the future, but now?

### For Research

Nielsen lists several projects in the intersection of precautionary and posthuman worlds:
- How could Pigouvian AI danger taxes work?
- What does good governance of AI organizations look like?
- Is open source AI net positive for safety? (His tentative answer: yes, because it democratizes safety research)
- How does space travel enable differential technology development?

## Notable Quotes

"The future is made by optimists: they're the people with the vision and drive to act. Good futures tend to be made by wise optimists, whereas bad futures are made by foolish optimists."

"For progress there is no cure." (von Neumann)

"Technological power, technological efficiency as such, is an ambivalent achievement. Its danger is intrinsic... useful and harmful techniques lie everywhere so close together that it is never possible to separate the lions from the lambs."

"Safety isn't a property of a system, it's a property of a system in a particular environment."

"I don't think it would have made sense to prohibit the specific discovery... There's a kind of rising sea of discovery, and you can't prohibit specific discoveries so much as work to prohibit entire paths of discovery."

## Connection to Resilience/Flourishing Framings

Nielsen's piece is fundamentally about what "flourishing" means in a technologically accelerating world. His answer: it requires ongoing adaptive capacity, not a fixed endpoint. The Alignment Problem is never "solved"---it's continuously re-solved as capabilities grow.

This maps well onto resilience framings: the goal isn't to prevent all shocks but to build systems that adapt and recover. The Kumbaya heuristic is essentially a statement that human civilization has been resilient to technology shocks so far. The question is whether that resilience can be maintained or enhanced as shocks become faster and more powerful.

His emphasis on non-market safety and the limits of technical alignment is particularly relevant for anyone thinking about biosecurity or other x-risk domains. Safety capture, safetywashing, and the incentive structures of commercial organizations are genuine concerns, not paranoid framings.
