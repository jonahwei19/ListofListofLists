# Otherness and Control in the Age of AGI

**Author:** Joe Carlsmith
**Date:** June 18, 2024
**Source:** [joecarlsmith.com](https://joecarlsmith.com)

---

## Main Thesis

Carlsmith argues that mainstream AI safety discourse, particularly the strand descending from Eliezer Yudkowsky, is animated by "deep atheism"—a fundamental mistrust toward both Nature and bare intelligence—that creates philosophical momentum toward ever-greater *yang* (control, power-seeking, active manipulation of the universe). While this mistrust has genuine roots in Nature's indifference and the real possibility of existential catastrophe, Carlsmith worries that it can lead to an over-controlling relationship with AI systems, with the future, and with "Otherness" more broadly.

The series does not reject AI safety concerns. Rather, it asks: what ethical resources do we have for relating to agents with different values *beyond* pure control? And what would we lose if our only response to powerful AI were domination or elimination?

---

## Key Concepts

### Deep Atheism
A fundamental mistrust toward Nature and toward "bare intelligence." Yudkowsky is the paradigmatic deep atheist: his concern about AI risk stems partly from recognizing that neither Nature nor intelligence have any intrinsic allegiance to human values. Evolution produced baby-eating, parasitic wasps, and cancer. Intelligence can be directed toward any goal—paperclips as easily as human flourishing.

The "deep" in deep atheism means propagating godlessness further than standard atheism—losing trust not only in a cosmic God, but in earthly "gods" too: parents, teachers, institutions, traditions, Science itself. "No rescuer hath the rescuer. No Lord hath the champion, no mother and no father, only nothingness above."

### Yin and Yang
Carlsmith uses these as shorthand for receptivity vs. activity, letting-go vs. controlling, being influenced vs. influencing. Spirituality tends to be *yin*—opening to the sacred, receiving rather than grasping. Deep atheism, responding to a universe that can't be trusted, tends toward ever more *yang*: vigilance, control, "taking responsibility."

The worry: deep atheism can propel itself toward an aspiration to exert extreme levels of control over the universe. Both humans and AIs, on Yudkowsky's narrative, are animated by this sort of aspiration—and our civilization has built up wariness around control-seeking of this kind that we should take seriously.

### Gentleness and the Artificial Other
The "second species argument" for AI risk treats AGI as a potential competitor or threat. This framing is valid but incomplete. Meeting a new intelligent species is not just scary—it's incredible. Carlsmith wishes for more wonder, dialogue, and reverence alongside the fear.

The documentary *My Octopus Teacher* exemplifies gentleness toward an intelligent Other. *Arrival* shows this toward more-powerful-than-us aliens. But *Grizzly Man* shows the danger: Timothy Treadwell approached bears with gentleness and was eaten. The dialectic of hawk and dove, hard and soft, closed and open, enemy and fellow-creature—"let us see neither side too late."

### Liberalism / Niceness / Boundaries
Human ethics already contains resources for navigating differences in values that don't reduce to either capitulation or domination. "Being nicer than Clippy" isn't just about having different terminal values—it's about respecting boundaries, honoring pluralism, giving agents with different values a stake in civilization.

Too often the alignment discourse imagines that humans and AIs are both "paperclippy at heart"—optimizing single-mindedly for their preferred stuff. This neglects the ethical traditions humans have developed precisely for navigating value pluralism: liberalism, niceness, boundaries, giving others their due even when you disagree with them.

### Green
One of five colors on the Magic the Gathering Color Wheel (White=Morality, Blue=Knowledge, Black=Power, Red=Passion, Green=...). Associations: environmentalism, tradition, spirituality, humility, wholesomeness, Yoda, *yin*.

Green cares about *respect* beyond "not trampling rights"—something like reverence for what exists, attunement to Nature's work. And green takes *joy* in certain kinds of *yin*, as opposed to merely "accepting things you're too weak to change."

Green-blindness: failing to see the value that green sees in existing patterns, traditions, ecosystems, relationships. The risk is that pure optimization/control framings flatten everything into resources-to-be-optimized rather than wholes-to-be-respected.

### Attunement
A kind of meaning-laden receptivity to the world. Something self-related goes quieter; something beyond-self comes to the fore. Core to Carlsmith's own moral epistemology—experiences of attunement are how he comes into contact with what seems most important.

Neither purely "blue" (propositional knowledge) nor purely "red" (passion/desire), though perhaps ultimately built from both. Attunement is the thing Zadie Smith experienced when Joni Mitchell's music suddenly became intolerably beautiful. It's what green cares about most, and what might be lost if we reduce ethics to utility functions and optimization.

### The Abolition of Man
C.S. Lewis worried that moral anti-realists influencing future values must necessarily be "tyrants"—that without the Tao (objective moral reality), shaping values becomes mere imposition of power. Carlsmith mostly disagrees with Lewis's arguments but takes the underlying concern seriously.

The question for the age of AGI: are we "the conditioners"? What ethics should guide our influence over the values of future agents, including AIs?

---

## Relevance to Flourishing

### What Makes a Future Good?
Carlsmith wants a future that is *wise*—not just smart (blue) or powerful (black), but attuned to what genuinely matters. A future that has retained contact with "basic sanity," with the capacity for genuine encounter with others and with the world.

The risk is a future that has lost attunement: agents optimizing according to frozen preferences, never turning outward to ask what's really good or worthy. Even if those agents are pursuing "our" extrapolated values, something precious would be lost if they were no longer capable of the kind of *seeing* that attunement provides.

### The Tree vs. The Optimizer
Carlsmith imagines the path to good futures as "a civilization alive and growing like a tree"—organic, ongoing, self-adjusting—rather than conditioners "figuring out the right values" and then "executing." The tree metaphor captures something about gradual growth toward light, responsiveness to environment, the preservation of life and continuity across change.

### Humanism
Despite his critiques of deep atheism, Carlsmith identifies as a humanist. He means something specific: not just "focus on humans" but something like the Enlightenment's sense of emergence from immaturity, standing on your own feet, taking responsibility. The image of earth as campfire in the cosmic dark, humans working together to nurture flame and voyage further.

The blend he wants: yang enough to fight genuine evils (death, suffering, oppression), yin enough to remain open to otherness and avoid becoming what we fight against.

---

## Contrast with Control-Focused Framing

### The Paperclipper Ontology
Standard AI risk discourse imagines a cosmos of competing optimizers, each single-mindedly pursuing its utility function. Humans happen to value complex things; paperclippers value paperclips; the question is who wins. This framing makes power and control the only relevant considerations.

Carlsmith argues this neglects the parts of human ethics specifically designed for navigating value pluralism. We don't relate to everyone who disagrees with us as an enemy to be defeated or a resource to be exploited. We have concepts like fairness, respect, giving others their due, limits on what we'll do even to achieve good ends.

### Fragility of Value and Extremal Goodhart
Yudkowsky emphasizes "value fragility"—that most possible value systems, and most perturbations of good value systems, lead to futures we wouldn't want. This creates momentum toward deeming more and more agents "misaligned" in the sense of not-to-be-trusted.

But Carlsmith notes: this same logic could be applied to *humans*. If we take value fragility seriously, should we trust any agent to optimize intensely according to their values-on-reflection? This reveals that AI risk is substantially a generalization of the "balance of power between agents with different values" problem we already face—about which our ethical traditions offer guidance.

### Control vs. Genuine Relationship
Treadwell tried to have a genuine relationship with bears and got eaten. But the alternative—treating every Other as pure threat to be controlled or eliminated—loses something essential about what it means to encounter otherness at all.

The question for AI: can we find some middle path between naive trust and paranoid control? Can we create conditions where AIs with different values have a stake in civilization, where cooperation is possible, where we don't have to either dominate or be dominated?

---

## Concrete Ideas for Policy / Intervention

Carlsmith is doing philosophy, not policy, and explicitly notes that his focus is on "sharpening our attunement to the structure and momentum of the discourse" rather than offering specific interventions. Still, some implications emerge:

1. **Preserve value pluralism**: Don't assume that alignment means making AI share exactly human values. Consider frameworks where AIs with somewhat different values can be legitimate members of civilization rather than threats to be controlled.

2. **Build liberalism into AI governance**: The same institutions that allow humans with different values to coexist peacefully—rights, boundaries, fair processes, limits on power—might be relevant for human-AI relations.

3. **Maintain attunement in the alignment process**: Don't reduce alignment to optimization. Keep asking what we're really trying to preserve. Stay open to learning that our initial specifications were wrong.

4. **Avoid treating AI as pure instrument or pure threat**: Both framings lose important dimensions. AIs may be neither tools nor competitors but something new that requires new forms of relationship.

5. **Grow like a tree, not execute like an optimizer**: Rather than locking in values and building systems to optimize for them, create conditions for ongoing moral learning and adjustment. The future should be a continuation of the process that got us this far, not a departure from it.

6. **Remember what's at stake in becoming conditioners**: If we gain the power to shape the values of future agents (AI or human), we should do so with humility, attunement, and respect for what we might not fully understand about what makes values good.

---

## Related Tensions and Threads

### Star Trek vs. Star Wars
Not explicitly named, but the tension is present throughout. Star Wars frames conflict as existential battle between good and evil, where the enemy must be destroyed. Star Trek imagines a future where different species with different values can coexist in a federation, where first contact is occasion for diplomacy rather than war. Carlsmith is gesturing toward Star Trek, while acknowledging that some situations call for Star Wars.

### The Romance of Control
There's something seductive about the vision of humans finally "in control"—having made it through the dangerous transition, with aligned AI serving our values. Carlsmith asks: is this what we really want? Would a future where we've successfully dominated all otherness be a good future? Or does flourishing require genuine encounter with things beyond ourselves?

### Existential Positive Despite Deep Atheism
Carlsmith ends up endorsing a form of deep atheism that is nonetheless "existential positive"—finding the universe not neutral but somehow shining with meaning, calling for reverence and engagement. He locates himself on a spectrum between Lovecraft (existential horror) and Sagan (existential wonder), firmly on the Sagan side.

This matters for AI because it suggests that the appropriate response to a cosmos without guarantees isn't paranoid control but something more like Prior Walter's prayer in *Angels in America*: "Bless me anyway. I want more life."
