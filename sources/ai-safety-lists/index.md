# List of Lists of AI Safety Projects

**Source**: https://www.lesswrong.com/posts/mtGpdtDdmkRC3ZBuz/list-of-lists-of-project-ideas-in-ai-safety
**Date processed**: 2026-01-11
**Type**: Meta-list

## Overview

LessWrong post aggregating AI safety project idea lists. Continuously updated. Covers evals, alignment, interpretability, control, and governance.

## Sub-Lists Identified

### High Priority (adding as separate sources)

| List | URL | Focus | Why Add |
|------|-----|-------|---------|
| 100+ Concrete Projects in Evals | [Alignment Forum](https://www.alignmentforum.org/posts/LhnqegFoykcjaXCYH/100-concrete-projects-and-open-problems-in-evals) | Eval design, benchmarks, red teaming | Marius Hobbhahn, comprehensive eval coverage |
| Redwood Research Project Proposals | [Alignment Forum](https://www.alignmentforum.org/posts/RRxhzshdpneyTzKfq/recent-redwood-research-project-proposals) | AI control protocols | Leading control research lab, specific technical problems |
| Ten AI Safety Projects (Julian Hazell) | [LessWrong](https://www.lesswrong.com/posts/vxA2BnCPTaPfnJjti/ten-ai-safety-projects-i-d-like-people-to-work-on) | Fundable org ideas | Funder perspective, concrete org proposals |

### Medium Priority (may add later)

| List | URL | Notes |
|------|-----|-------|
| Timaeus Project Ideas | [timaeus.co/projects](https://timaeus.co/projects) | DevInterp/SLT specific, narrow audience |
| Machine Unlearning Open Problems | [arXiv](https://arxiv.org/abs/2501.04952) | Research paper, not project list per se |
| Emergent Misalignment Follow-up | Paper-specific | Narrow scope |

### Lower Priority (skip for now)

| List | Notes |
|------|-------|
| Various fellowship/course lists | Career guidance, not project ideas |
| Individual researcher idea threads | Too scattered |

## Mapping to Existing Entries

Based on search results, the sub-lists cover:

### Evals (100+ projects list)
- Benchmark design → [[safety-benchmark-development]]
- Red teaming → [[detect-evaluation-sandbagging]], [[verify-ai-alignment]]
- Capability measurement → [[safety-benchmark-development]]

### Control (Redwood proposals)
- Control protocols → [[contain-unsafe-ai]]
- Untrusted monitoring → [[multi-agent-threat-monitoring]]
- Honeypotting/sandbagging detection → [[detect-evaluation-sandbagging]]
- Collusion prevention → [[contain-unsafe-ai]]

### Fundable Orgs (Hazell list)
- AI agent behavior monitoring org → [[multi-agent-threat-monitoring]]
- Technical AI governance research → [[hardware-enabled-governance]]
- Lab activity tracking → [[responsible-scaling-commitments]]
- AI fact-checking tools → [[crowdsourced-verification]]

### Interpretability (Timaeus)
- Developmental interpretability → [[extract-insight-from-model-internals]]
- Singular learning theory applications → [[extract-insight-from-model-internals]]

## Key Insights

1. **Significant overlap with existing entries**: Most project ideas fit under our 4 problem areas (verify, make, contain, defend)

2. **Evals as dominant theme**: Heavy emphasis on evaluation design across multiple lists, reinforcing importance of [[safety-benchmark-development]] and [[verify-ai-alignment]]

3. **Control gaining traction**: Redwood's control research has spawned many follow-up project ideas, validating [[contain-unsafe-ai]] as key intervention area

4. **Org-building focus**: Hazell's list is notably about creating new organizations, not just research projects - could inform implementer sections

## New Entries to Consider

None identified. All project categories map to existing problem/intervention entries. The lists provide implementation detail rather than new problems.

## Summary

Meta-list pointing to multiple AI safety project lists. 3 high-priority sub-lists added as sources. Heavy overlap with existing taxonomy - lists validate rather than expand our coverage. Strong emphasis on evals and control protocols.
