## Domain 2: Evaluation & Auditing Systems (Proposals #42-64)

### Holistic Evaluations

**#42 - Interactive Inspection Tools**
- **Goal:** Move beyond basic evaluation numbers to enable users to explore model behaviors through natural language queries like "Why did the model fail on this task?"
- **Mechanism:** Develop AI-backed dynamic dashboards where users can probe capabilities, investigate failures, and perform sensitivity analyses. These tools would use specialized AI systems to interpret and explain frontier model behaviors.
- **Who's working on it:** Not specified

**#43 - End-to-End Harm Assessment**
- **Goal:** Conduct comprehensive evaluations of harmful AI capabilities that assess full capability manifestation rather than just multiple-choice tasks.
- **Mechanism:** Develop methodologies to evaluate "end-to-end harmful capability manifestation" - testing whether AI can assist with ideation, planning, and execution of dangerous activities, not just answer questions about harmful topics.
- **Who's working on it:** Not specified

**#44 - Evaluation Sandbagging Detection**
- **Goal:** Identify when AI systems deliberately underperform on safety evaluations for strategic purposes.
- **Mechanism:** Develop research techniques and robust testing protocols that ensure evaluation reliability as models become more sophisticated, remaining effective even against systems attempting to game assessments.
- **Who's working on it:** Not specified

**#45 - Training AI Evaluators**
- **Goal:** Analyze the vast amounts of data generated by frontier models (millions of neuron activations, training datasets, interaction logs) that are too large for human review.
- **Mechanism:** Train specialized investigator AI agents to detect capabilities and failure modes, moving beyond benchmark-focused evaluations toward holistic understanding of model capabilities in real-world contexts.
- **Who's working on it:** Not specified

**#46 - Cooperative Evals: AIs Working With Humans**
- **Goal:** Establish better understanding of how humans might cause harm with AI models and how humans could be productively integrated with AI systems.
- **Mechanism:** Undertake empirical research comparing human performance on various tasks with and without AI assistance (and with varying forms of AI systems).
- **Who's working on it:** References position papers by Haupt et al., 2025 and Kulveit et al., 2025

---

### More-Scientific Evals

**#47 - Meta-Research on Safety Techniques**
- **Goal:** Evaluate and validate the effectiveness of different AI safety approaches to move toward empirical safety science.
- **Mechanism:** Have multiple independent actors assess the same models using different techniques, then compare results to determine reliability and blind spots.
- **Who's working on it:** Not specified

**#48 - Characterizing AI Risks with Evidence**
- **Goal:** Give governments clear criteria for when to intervene through robust research that characterizes risks empirically.
- **Mechanism:** Create benchmarks establishing clear quantitative thresholds for risks, providing credible empirical evidence to enable appropriate government action.
- **Who's working on it:** Not specified

**#49 - Vulnerability Matching**
- **Goal:** Ensure funding flows efficiently to researchers tackling actual, identified problems rather than theoretical concerns.
- **Mechanism:** Directly connect discovered AI vulnerabilities to appropriate research proposals addressing them, with continuous updates as new vulnerabilities are discovered through red teaming.
- **Who's working on it:** Not specified

**#50 - Safety Benchmark Development**
- **Goal:** Establish comprehensive high-effort benchmarks for evaluating AI safety and security, providing a common language for discussing safety progress.
- **Mechanism:** Develop rigorous, transparent methodologies for assessing various dimensions of AI safety across different models and organizations.
- **Who's working on it:** Notes that many safety benchmarks have been defunded or discontinued

**#51 - Go Beyond Pre-Paradigmatic Evaluation Frameworks**
- **Goal:** Achieve consensus on how to evaluate AI systems for security as evaluation transitions from inside labs to standardized external processes.
- **Mechanism:** Resolve fundamental questions about whether evaluations will be qualitative or quantitative, crowdsourced or team-based, and what risk thresholds should trigger interventions.
- **Who's working on it:** Not specified

**#52 - Risk-Model Evaluation**
- **Goal:** Create foundations for reliable triage systems that focus human attention where most critically needed.
- **Mechanism:** Develop evaluation frameworks for AI risk assessment systems that identify, classify, and prioritize potential AI mistakes based on consequences, quantifying false positive and negative rates.
- **Who's working on it:** Not specified

**#53 - Classified Red Teaming**
- **Goal:** Understand the true scope of AI's offensive potential to create accurate assessment of "how easy" it is to misuse AI.
- **Mechanism:** Develop classified red teaming capabilities involving collaboration with Los Alamos National Laboratory specialists on CBRN threats, drawing on experts from DeepMind and Microsoft.
- **Who's working on it:** Los Alamos National Laboratory, DeepMind, Microsoft (mentioned as potential collaborators)

**#54 - Capability Assessment Framework**
- **Goal:** Help prioritize mitigation efforts by focusing resources on empirically validated risks rather than theoretical possibilities.
- **Mechanism:** Develop a systematic, grounded approach to evaluating which AI risks are most credible versus overrated, distinguishing between well-evidenced catastrophic harms and speculative concerns.
- **Who's working on it:** Not specified

**#55 - Ultra-Reliable AI Evaluation**
- **Goal:** Identify edge cases where models fail catastrophically at rates like one in a million or less, moving toward "five nines" (99.999%) reliability.
- **Mechanism:** Develop engineering approaches and benchmarking methodologies modeled more like aircraft safety standards than Kaggle competitions, probing worst-case adversarial examples without requiring millions of test instances.
- **Who's working on it:** References Anthropic (2025a) and Dong et al. (2022)

**#56 - Misuse Evaluations**
- **Goal:** Create standardized frameworks for testing advanced AI systems against sophisticated CBRN misuse scenarios.
- **Mechanism:** Develop highly realistic evaluations that simulate threats with adversarial intent; this is described as a tragedy of the commons where all major labs recognize necessity but lack individual incentives.
- **Who's working on it:** Noted as critically needed but lacking clear ownership

**#57 - Multi-Agent Interaction Framework**
- **Goal:** Prevent emergent cooperation between agents to circumvent safety constraints as AI systems increasingly interact autonomously.
- **Mechanism:** Develop infrastructure for simulating and analyzing multi-agent interactions, establishing protocols for trust verification, bad actor identification, and collective safety measures across distributed systems.
- **Who's working on it:** Not specified

---

### Auditing Institutions

**#58 - Evaluation Companies**
- **Goal:** Fund crucial evaluation organizations that handle cybersecurity and offensive evaluations in the AI security ecosystem.
- **Mechanism:** Provide funding to prevent these organizations from becoming bottlenecks (described as relatively small financial investment for critical functions).
- **Who's working on it:** SecureBio, PatternLabs (Irregular), Expo

**#59 - Public Audits**
- **Goal:** Create public precedents for robust evaluation that raise the "floor" for responsible disclosure without requiring immediate regulation.
- **Mechanism:** Have independent organizations conduct comprehensive public audits of open-weight models with significant compute resources, demonstrating what thorough model analysis should look like and opening to community feedback.
- **Who's working on it:** Not specified

**#60 - AI Auditing Institutions**
- **Goal:** Found human-based verification and auditing bodies for AI similar to the IAEA's role in nuclear oversight.
- **Mechanism:** Bring together experts from nuclear verification, cybersecurity, and AI technical domains to design protocols and organizational structures capable of credibly verifying compliance with safety standards.
- **Who's working on it:** Notes that no organization is currently actively designing such institutions

**#61 - Secure Frontier Data Centers**
- **Goal:** Create frontier compute facilities meeting SCIF-level security standards (Secure Compartmentalized Information Facilities) for secure AI development.
- **Mechanism:** Build SL4-strength data centers with scalability for increased energy requirements, implementing confidential computing and HSMs (Hardware Security Modules) that are currently insufficiently deployed.
- **Who's working on it:** Not specified

**#62 - Verified Kinetic Actuators**
- **Goal:** Ensure reliable and predictable operation of AI systems controlling physical infrastructure to prevent accidents and misuse.
- **Mechanism:** Develop provably safe mechanisms for AI systems to interact with physical environments through robotics and actuators, providing verification for industrial automation, autonomous vehicles, and critical infrastructure.
- **Who's working on it:** Not specified

**#63 - Speed Limits in Data Centers**
- **Goal:** Slow capability advancement and buy time for safety research by directly limiting the pace of AI development.
- **Mechanism:** Implement technical restrictions on computing speed in data centers.
- **Who's working on it:** Notes primary bottlenecks are political popularity and coordination, with expected pushback from leading AI labs

**#64 - Physical Chokepoints**
- **Goal:** Reframe security concerns toward physical world interfaces if model proliferation is "inevitable and unstoppable."
- **Mechanism:** Focus on limiting the mass production of robotic systems as the most effective chokepoint may be at the level of physical implementation rather than digital model constraints.
- **Who's working on it:** Not specified
