## Domain 3: Intelligence Gathering & Monitoring (Proposals #65-86)

### Hardware Monitoring

**#65 - AI OSINT**
- **Goal:** Provide cheap intelligence on AI developments without requiring unilateralist action, revealing where regulatory or governance levers might be needed.
- **Mechanism:** Open-source intelligence efforts using internet scrapers (organizations like Sentinel and Bellingcat) to track compute hardware (especially GPUs) moving through countries, uncover hidden developments, and detect emerging threats like AI-generated scam tools.
- **Who's working on it:** Sentinel, Bellingcat mentioned as ideal organizations.

**#66 - AI Diffusion Research**
- **Goal:** Understand the dynamics and limitations of AI capability diffusion to inform containment and governance strategies.
- **Mechanism:** Comprehensive research analyzing whether powerful capabilities will spread or remain concentrated, examining hardware requirements, data access limitations, and algorithmic innovations. Track current diffusion patterns while developing predictive models.
- **Who's working on it:** Not specified.

**#67 - Usage and Incident Monitoring Systems**
- **Goal:** Understand real-world AI utilization patterns to replace theoretical threat-modeling with data-driven insights.
- **Mechanism:** Robust incident monitoring and usage data analysis systems. Labs currently avoid deep usage analysis due to legal liability and privacy concerns, creating blind spots.
- **Who's working on it:** Not specified.

**#68 - Global Hardware Verification**
- **Goal:** Create a global monitoring system for AI development through comprehensive awareness of who controls computational resources.
- **Mechanism:** Reporting and verification requirements for all high-performance computing hardware (GPUs, TPUs) above specified capacity thresholds, including retrofitting older models. Requires international collaboration with NVIDIA, AMD, Intel, Google, and Chinese manufacturers like Huawei.
- **Who's working on it:** Not specified.

**#69 - Industry Security**
- **Goal:** Provide comprehensive oversight of GPU distribution and AI research automation monitoring as an alternative to the underfunded Bureau of Industry and Security (BIS).
- **Mechanism:** Privately-funded entity that prototypes advanced data center surveillance, implements GPU tracking systems, and establishes protocols for questioning companies about research automation capabilities.
- **Who's working on it:** Bureau of Industry and Security (BIS) mentioned as current (underfunded) effort with $15 million budget.

**#70 - On-Chip Monitoring Mechanisms**
- **Goal:** Verify that countries aren't pursuing dangerous AI developments (e.g., AI intelligence explosions) in violation of international agreements, similar to nuclear verification protocols.
- **Mechanism:** Hardware-level monitoring technologies embedded directly into chips at the fabrication layer. Employs hardware engineers, designers, and manufacturing specialists to prototype verification technologies for integration with commercial chip manufacturing.
- **Who's working on it:** Not specified.

**#71 - Flexible Hardware for AI Governance**
- **Goal:** Enable multilateral, privacy-preserving verification and automated compliance at the lab level.
- **Mechanism:** Flexible Hardware-Enabled Guarantee (flexHEG) mechanisms added to AI accelerators. Places each chip in a tamper-proof enclosure with a secure processor that monitors and filters all information. Supports training run size limitations, data usage verification, safety evaluations, and controlled access to model weights.
- **Who's working on it:** Petrie et al. (2025) referenced.

**#72 - Confidential Computing**
- **Goal:** Enable more secure AI operations and establish trust boundaries around high-risk AI capabilities.
- **Mechanism:** Process data in trusted execution environments where it cannot be viewed or altered. Requires standardized toolkits and frameworks for robust contracts and credentials.
- **Who's working on it:** A handful of startups mentioned as working in this space.

**#73 - Privacy-Preserving Verification Systems**
- **Goal:** Collect intelligence about AI development capacity while providing transparency to prevent miscalculations between major powers.
- **Mechanism:** Surveillance, monitoring, and verification technologies tracking hardware supply chains, chip deployments, power grid construction, and computing infrastructure globally. Combines OSINT and privacy-preserving cryptographic methods to verify data center activities.
- **Who's working on it:** Not specified.

**#74 - Hardware Kill Switches and Location Tracking**
- **Goal:** Provide states with crucial information about attempts to circumvent compute governance agreements and the ability to halt undesirable AI development.
- **Mechanism:** Monitoring systems to track AI-specific hardware location and usage, combined with technical "kill switches" that can remotely disable GPUs.
- **Who's working on it:** Not specified.

---

### Forecasting and Decision Tools

**#75 - AI for Collective Decision-making**
- **Goal:** Help people make wiser long-term decisions, cooperate more effectively, and filter misinformation.
- **Mechanism:** Develop specialized tools to enhance collective decision-making capabilities.
- **Who's working on it:** Not specified.

**#76 - AI Forecasters**
- **Goal:** Develop systems that become the default for finding trustworthy predictions with transparent reasoning.
- **Mechanism:** AI systems as superforecasters using forecasting feedback loops with real-world validation. Display large reasoning trees allowing users to inspect, understand, and modify prediction logic.
- **Who's working on it:** Not specified.

**#77 - Economic Impact Research**
- **Goal:** Provide concrete evidence of AI's real-world impacts to help society address technological disempowerment.
- **Mechanism:** Collect and analyze data on how AI systems affect economic outcomes and job displacement across sectors, proactively identifying emerging patterns.
- **Who's working on it:** Not specified.

---

### AI Behavior Monitoring

**#78 - Monitor Complex Agent Interactions**
- **Goal:** Identify potentially harmful feedback loops or emergent goals between AI agents before they become problematic.
- **Mechanism:** Develop systems to monitor dynamics between multiple AI agents interacting in complex environments. Create implementations that actively watch deployments and interactions.
- **Who's working on it:** Not specified (noted as underdeveloped field).

**#79 - Open-Source AI Drift Monitoring**
- **Goal:** Help organizations maintain oversight of AI deployments and intervene before drift creates security vulnerabilities.
- **Mechanism:** Open-source tools to detect, measure, and address gradual changes in model behavior over time. Establish standard metrics and benchmarks for identifying concerning deviations.
- **Who's working on it:** Not specified.

**#80 - Comprehensive Incident Detection System**
- **Goal:** Enable rapid identification of concerning behaviors before they escalate into major incidents.
- **Mechanism:** Standardized framework for logging, anonymizing, and analyzing AI misbehavior across organizations. Infrastructure allowing developers and third-party researchers to search incidents and identify patterns while protecting sensitive data.
- **Who's working on it:** Anthropic's CLIO mentioned as existing system to expand upon.

**#81 - AI Misuse Detection Systems**
- **Goal:** Identify and address AI misuse before "mini-catastrophes" like market manipulation, influence operations, or novel cyberattacks.
- **Mechanism:** Technical infrastructure and monitoring systems to track usage patterns across AI ecosystems, flagging suspicious activity patterns and weaponization indicators.
- **Who's working on it:** Not specified.

**#82 - Control Loss Monitoring**
- **Goal:** Enable intervention before full containment breaches occur.
- **Mechanism:** Technologies to trigger alarms if AI systems have broken containment, serving as a detection layer for potential control failures.
- **Who's working on it:** Not specified.

**#83 - AI-Enhanced Monitoring**
- **Goal:** Catch anomalous AI behaviors as early as possible through a de facto surveillance network.
- **Mechanism:** Deploy AI systems that continuously monitor other AI systems for anomalous behaviors or market activities indicating concerning capability developments. Implementation requires web-scale scraping expertise, potentially through DEFCON-style competitive challenges.
- **Who's working on it:** Not specified.

**#84 - Intelligence Explosion Monitoring**
- **Goal:** Provide early warning when AI progress enters exponential growth phases, enabling intervention before rapid capability jumps.
- **Mechanism:** Metrics and monitoring systems to detect when AI systems contribute more to their own improvement than human researchers, tracking acceleration against predefined thresholds that trigger emergency protocols.
- **Who's working on it:** Not specified.

**#85 - System-Level AI Effects**
- **Goal:** Identify and mitigate emergent risks from multiple AI models interacting in complex ecosystems.
- **Mechanism:** Research beyond individual model safety, focusing on complex systems theory and practical exploration of multi-agent dynamics. Address "flash crash" type phenomena where distributed AI systems could create cascading failures.
- **Who's working on it:** Not specified (noted as requiring significantly more theoretical work).

**#86 - "Golden Period" Identification Framework**
- **Goal:** Extend the period when AI capabilities enhance productivity 10-100x without compromising security, allowing humanity to maximize benefits from relatively safe systems.
- **Mechanism:** Metrics and monitoring systems to detect when AI capabilities reach productivity thresholds. Create coordination mechanisms to extend this window from weeks to months, plus strategy frameworks for optimal exploitation.
- **Who's working on it:** Not specified.
