# Peregrine 2025 Report: Proposals #113-208

---

## AI Governance & Policy Development (continued)

## #113: AI Ownership

Create governance frameworks for distributing ownership of AI-driven productivity as a step to avoid extreme inequality. These systems would determine how benefits from AI labor are shared across society when traditional employment may no longer be the primary distribution mechanism. Potential approaches include public ownership of foundation models, mandated profit-sharing from AI deployment (O'Keefe et al., 2020), or novel structures like data dividends (Data Dividends Initiative, n.d.). This work addresses the distribution of AI-driven productivity and capital gains at the societal level, rather than rights or compensation tied to training data (compared to "Data Rights Systems" above).

## #114: Democratic Support

Support organizations focused on democratic accountability and institutional integrity provide essential infrastructure for addressing any technological threat including advanced AI systems. Democratic stability has become increasingly precarious worldwide, with democracy indices showing deterioration across multiple countries as legitimate democratic organizations face defunding or delegitimization campaigns. In Germany, initiatives like Effektiv Spenden have created specialized funds to recommend and support democracy-strengthening organizations that could serve as models for similar efforts globally. The most effective approach would target multiple countries simultaneously with coordinated support for local democratic institutions.

---

## International Coordination

### Improving Communication Channels

**Scope:** Communication channels between nations for better international coordination around AI. These channels included novel communication systems, international fora and workshops, and informal alliance-building.

## #115: Cross-Border Notification Systems

Develop (OECD, 2025) mechanisms (Werkmeister, 2025) for countries to alert each other about out-of-control AI systems, similar to "red phones" during the Cold War. This would cover the critical need for international communication channels to prevent misunderstandings during AI safety incidents, such as scenarios where one party had the solution to a technical problem the other parties failed to address just because they didn't inform each other.

## #116: CERN for AI

Create an international AI development consortium (similar to CERN or Intelsat) to allow unified research without competitive pressures. Once established, this model would defer complex technical safety decisions to the project itself rather than attempting to coordinate disparate corporate efforts. CERN-like infrastructure would further enable experiments impossible elsewhere, particularly for safety research requiring extraordinary computational resources. The aim would be to construct a facility that could serve as a neutral, international testbed for evaluating advanced AI systems under controlled conditions, but doing so would require dramatically expanding the role of governments in technology decisions.

## #117: International Risk Management

Strengthen recent international initiatives like the UN Pact for the Future (UN, 2024) and the US Global Catastrophic Risk Management Act (Sen. Portman, 2022) with sustained financial and political support. The initiatives mentioned represent nascent frameworks for coordinated AI risk management across borders, which need to move beyond conceptual frameworks towards operational protocols with implementation capabilities. Building on these foundations would enable coordinated global responses to AI-related risks rather than fragmented national approaches.

## #118: Global Frameworks for AI Development

Establish "Sherpa" networks, with designated representatives from different countries maintaining regular communication channels and coordinating policy responses. China and other countries appear to have similar levels of AI risk awareness, creating opportunities for meaningful international agreements. The initiative could build upon foundations established through efforts like the Bletchley Park Summit (Gov.uk, 2023) and Seoul AI summit (Gov.uk, 2024), expanding participation and formalizing commitments.

## #119: Strategic Coordination Frameworks

Develop coherent strategies where interventions strengthen each other, as opposed to being mere lists of possible interventions. The motivating ideal consists of integrated policy responses where components complement each other, avoiding fragmented efforts that lack synergy or pull in different directions (as is the case with e.g. interventions for centralizing AI development to mitigate race dynamics, and interventions for decentralizing AI development to mitigate power concentration).

## #120: Realistic End States for AI Governance

Engage in theoretical research on potential long-term equilibria for AI governance. There are many examples of these, including scenarios where alignment keeps pace with capabilities, mutually assured destruction arrangements, coordinated slowdown, singleton scenarios, and robust international governance regimes. More propitious arrangements might include treaties prohibiting concealed computing centers, transparency about chip distribution, and mutual monitoring of major AI projects with failsafe mechanisms. The aim would be to produce technically and geopolitically realistic models of AI governance in light of both immediate safety needs and longer-term competitive dynamics.

## #121: International Governance Trifecta

Establish predecessor organizations to international AI governance bodies modeled after existing entities like the IAEA and ISS. The project would aim to reduce competitive pressures in AI development by creating mechanisms for international agreement verification, partially enabled by flexible hardware governance technologies. By removing some "fuel from the race," these governance structures could provide critical time and stability for implementing additional safety measures while establishing the foundation for longer-term international coordination.

## #122: Geopolitical Sense-Making

Create robust information-sharing platforms that help stakeholders understand critical developments like DeepSeek's relationship with the Chinese government. The current environment was said to feature widespread disagreement – even among supposed experts – regarding basic facts regarding Chinese AI efforts, nuclear capabilities, and strategic intentions. Information-sharing platforms would bring together nuclear experts, AI specialists, and diplomatic professionals who currently operate in separate worlds with minimal communication, with the aim of avoiding costly misunderstandings during crisis situations.

---

### Workshops, Organizations, and Diplomacy

**Scope:** Convening of key actors, building institutions, and creation of diplomatic channels to coordinate decisions and commitments that reduce AI risk.

## #123: Compute Supply Chain

Organize key nations in the AI compute supply chain (Netherlands, Taiwan, Japan, Germany, US) to recognize their collective leverage in moderating the pace of AI development. This initiative would involve running workshops through organizations like Safe AI Forum (SAIF, n.d.) to establish coordination mechanisms between semiconductor manufacturing countries. The goal would be creating practical enforcement mechanisms for any future AI treaties, as control of specialized chips and manufacturing equipment provides one of the few concrete points of leverage.

## #124: Intergovernmental Frontier Model Forum

Allocate large amounts of funding (on the order of hundreds of millions of USD) to METR (METR, n.d.), while developing an international version of the Frontier Model Forum (FMF, n.d.) structured like the WHO but specifically focused on AI threat models. Unlike the current Forum which lacks involvement from key players like DeepSeek and xAI, this organization would abandon the exclusive membership model in favor of universal participation to effectively facilitate international treaty development and red line enforcement before concerns like biological weapons uplift become critical concerns.

## #125: EU Diplomatic Capacity

Empower the EU AI office and External Action Service (EEAS, n.d.) to position Europe as a credible third pole in global AI governance. Switzerland's unique diplomatic position makes it one of the few truly neutral parties that could credibly signal to China, unlike Singapore which may not maintain sufficient neutrality. The EU could play a critical role in negotiating supply chain agreements and establishing verification mechanisms, potentially hosting third-party auditors trusted by both the US and China. EU diplomatic capacity was noted as increasingly valuable in light of US-China contact potentially becoming more challenging over the next two years.

## #126: International Protocols for AI Treaties

Develop hardware and protocols that would enable international auditing of future AI treaties. This infrastructure would support verification mechanisms for international agreements, similar to nuclear non-proliferation approaches. US-China dialogues particularly need scaling and formalization to prevent dangerous competitive dynamics. Hardware components, potentially coupled with new verification technologies, were mentioned as key bottlenecks for verification to operate in spite of geopolitical tensions.

---

### Building Informal Alliances

**Scope:** Lightweight, trust-based networks between key people and institutions that share information quickly, align on actions, and coordinate without formal structures. It is important that this effort exists for coordination under circumstances where formal institutions are too slow, politically constrained, or unable to share sensitive information.

## #127: Specialized Diplomatic Corps

Create specialized diplomatic corps focused specifically on AI governance with multilingual capacity and deep technical understanding of frontier capabilities, particularly for engagement with China. These specialized communicators could operate as third parties rather than from within governments or labs that may be too constrained to engage effectively. Focusing on building bandwidth for substantive communication at the nation-state level, potentially with support from organizations like the Gates Foundation if they can develop strategy quickly enough.

## #128: International Exchange Program

Establish a robust program that funds individuals to travel regularly between China and the US, building personal relationships and trust networks that enable crucial information exchange. The current touch points between these powers are extremely limited, often involving the same small circle of academics. Communication is genuinely difficult, requiring significant time investment to build personal relationships and trust, with a notable shortage of individuals skilled in this area. The program would not expect immediate results, instead aiming to build future diplomatic capacity through years of deliberate relationship cultivation.

## #129: Track 2 Diplomacy

Fund unofficial diplomatic channels between major countries on AI development (especially the US and China) to build relationships and identify potential agreement points before formal negotiations. Formal government negotiations may move too slowly given rapid technological development timelines, whereas this approach offers the promises of establishing more "potential points of agreement" ahead of official international negotiations. As with the International Exchange Program, the goal would be building sufficient mutual understanding and trust between powers, with a more direct focus on government officials who may be influential when it comes to pursuing formal agreements on development limitations and safety standards.

---

### Changing the Geopolitical Landscape

**Scope:** More fundamental attempts to change the geopolitical landscape of AI development, or attempting to reduce global conflict more broadly.

## #130: Consolidation of AI Development

Reduce the number of organizations building frontier AI systems to enable better governance, oversight and more effective implementation of safety measures and alignment techniques. Such consolidation could resemble an "AGI Manhattan Project" where the frontier technology is controlled by a single entity that can implement unified safety protocols. This approach would enable coordinated pausing of research based on safety considerations.

## #131: Western Multipolar AGI

Support non-US Western AGI projects (such as in the UK) as a counterbalance to US dominance in AI capabilities. This would create alternatives to US-based systems, potentially preventing problematic concentration of power and decision-making. The geopolitical implications of having multiple Western powers with AGI capabilities could create more balanced governance structures. This represents a strategic approach to addressing the international dimensions of AI development and risk management.

## #132: Conflict Reduction

Work to resolve major global conflicts (particularly those involving nuclear powers) before advanced AI development increases geopolitical tensions. Special consideration is given to nuclear powers, which might use their nuclear capability as negotiating leverage. This project would develop scenarios for how nuclear threats might manifest during AI negotiations and create strategies to prevent escalation.

## #133: Country-Level Consolidation

Super-lobbying efforts to reduce the number of countries with frontier AI labs through strategic policy interventions. The initiative would establish hardware-level monitoring and verification mechanisms, which would enable a nonproliferation treaty by limiting the number of parties that would need to sign it for any real effect.

---

## Preparedness & Response

### Biological Risk Mitigation: Generic Measures

**Scope:** Improvement of biosecurity more generally. Many participants raised worries about AI systems' ability to aid actors in creating engineered viruses.

## #134: CBRN Access Control

Address AI's capacity to lower barriers to Chemical, Biological, Radiological and Nuclear weapons through specialized restrictions. These restrictions would include investments in infrastructure and personnel to establish comprehensive pandemic detection systems, strengthening traditional safeguards like wastewater sequencing, vaccine stockpiling, and securing critical supply chains. Such measures would provide dual benefits, through protecting against potential catastrophic risks while providing near-term public health benefits.

## #135: Medical Countermeasures

Develop broad-spectrum medical interventions, particularly vaccines that protect against entire classes of pathogens rather than specific strains. This represents a critical defense against both natural and engineered biological threats. Significant funding should be directed toward rapid response platforms that can quickly produce targeted countermeasures when novel threats emerge. Creating stockpiles of key generic countermeasures ahead of time would provide valuable time buffers during emerging crises. This area benefits from existing research momentum but requires substantially increased scale.

## #136: UV Technology

Develop Far-UVC light technology (CUIMC, 2024), which offers promising capabilities for continuous environmental sanitization without the risks associated with conventional UV systems. Safety testing should proceed rapidly while simultaneously building production capacity and stockpiles for deployment during serious outbreaks. Regulatory hurdles may be significant but could be navigated through specific use-case authorizations or emergency provisions. Production scaling should focus on both cost reduction and reliability improvement to enable widespread deployment.

## #137: Nucleic Acid Observatory

Expand the Nucleic Acid Observatory (Nucleic Acid Observatory, n.d.) concept with multiple teams pursuing different technical approaches to significantly enhance global biosurveillance capabilities. Complementary monitoring systems and centralized international deployment across strategic locations could provide early detection of emerging pathogens or deliberately engineered biological threats. Substantial computing resources and specialized talent are required to analyze the massive datasets generated by environmental sampling, which would close critical gaps in current monitoring systems.

## #138: Biological Countermeasures

Accelerate all necessary biological defenses, modeled after Alvea Corp's (Alvea, n.d.) approach. This would involve comprehensive biological threat assessment, preparation of defensive countermeasures, and rapid response capabilities. The project aims to provide confidence that biological risks accelerated by AI capabilities can be effectively contained and mitigated, working across public health systems, research institutions, and biotechnology development.

---

### Biological Risk Mitigation: AI-Specific Measures

**Scope:** Mitigation of biological risks with a narrower focus on interventions specifically targeted at preventing directly AI-enabled threats.

## #139: Biosecurity Controls

Implement strong controls over sensitive data that could enable harmful applications, particularly for biological and other dual-use technologies. This would include developing methods to "train out this data, exclude it, grep it against the data source," alongside implementing more robust "KYC (Know Your Customer) guidelines all around the stack." The aim would be to develop specific detection mechanisms for biology-related queries, create safety measures for capabilities relevant to biological design, and establish evaluation protocols for potentially dangerous information. Full-specification models would only be available to licensed institutions to prevent proliferation of dangerous capabilities.

## #140: Biorisk Thresholds for Open Models

Conduct more empirical work like wet lab studies to establish the conditional impacts of specific AI capabilities in this domain. These studies would quantify how capabilities like those demonstrated in benchmarks translate to real-world risk increases. AI systems are approaching capability thresholds where they could significantly increase bio-risks, potentially reaching high pre-mitigation risk levels within months. Without controls on open-source models, this could increase the risk of non-state actor bio-attacks by an order of magnitude within a year.

## #141: Global Immune System

Create next-generation wearables for host response-based diagnosis that detect physiological anomalies indicating infection, even without identifying the specific pathogen. Rather than competing with commercial wearables, this effort would identify additional measurements beyond current capabilities and determine how to operationalize existing devices for an early warning biodefense ecosystem. The approach leverages existing wearable technology momentum and user acceptance while developing missing technological components and protocols to transform the current ecosystem into a distributed global immune system. This capability becomes increasingly critical as AI capabilities expand the bioweapon threat landscape, addressing the fundamental challenge: "What do you look for when you don't know what you're looking for?"

## #142: AI for Bio-resilience

Create organizations focused specifically on leveraging AI to strengthen societal defenses against misuse, particularly for biological weapons and cyber threats. Such attacks might be associated with state actors without clear attribution, creating complex response scenarios that require advanced preparation. This initiative would develop specific technological safeguards, monitoring systems, and response protocols for the most concerning misuse vectors, prioritizing those that could emerge within the next 12-24 months.

## #143: Physical Security Mechanisms

Develop physical security measures that constrain bioweapons and similar threats at the material level, rather than focusing on software-based solutions. Self-replicating vaccines that transmit like normal viruses could be developed as a potential solution for novel bio-threats enabled by increasingly capable AI systems. This approach requires funding for experts who take the potential capabilities of near-future AI systems seriously, and investment towards substantial research programs aimed at moving beyond the current biological literature for defensive technologies. The goal should be developing interventions that can address qualitatively different threats compared to anything seen before, rather than merely extending existing biosecurity frameworks.

---

### Democratic & Societal Resilience

**Scope:** Improving the ability of governments and civil societies to aggregate preferences, respond to crises, and continue ordinary functions during a crisis. This is under the assumption that AI could lead to an increase in volatility and society-scale events (e.g. novel bioengineered pathogens).

## #144: Resilience Research

Scale research capacity to enable developing coherent resilience frameworks tailored to different regional vulnerabilities. Currently only ALLFED (ALLFED, n.d.) and a handful of other organizations conduct focused research on societal resilience to catastrophic disruptions, creating dangerous knowledge gaps in critical preparedness areas. Matt Boyd's research group (Adapt Research Ltd., n.d.) in New Zealand and Penn State's nuclear winter research program (Mulhollem, 2025) represent the only other significant academic efforts investigating large-scale resilience strategies. These efforts remain dramatically underfunded relative to the scale of potential risks, with research topics ranging from alternative food production to critical infrastructure protection.

## #145: Civilizational Resilience Measures

Enhance society's overall ability to withstand disruptive AI impacts by developing comprehensive programs to strengthen society's "immune system" against potential harms from advanced AI (see also the "Global Immune System" initiative above). Such measures include securing essential systems from potential cyberattacks that could cause billions in damage, improving detection and response capabilities for novel biological threats, developing protocols for responsible transfer of decision-making to AI systems, and creating redundant systems that maintain functionality during disruptions. The initiative should leverage AI for improved defensive capabilities to maintain a favorable offense-defense balance, with the goal of increasing "G-doom" – the level of intelligence needed to cause catastrophe – through systematic improvements to global resilience.

## #146: Large Group Deliberation Systems

Develop systems for collective decision-making involving large groups of people which are essential for maintaining distributed power in the face of rapid AI advancement. These tools should involve extensive testing with real humans to iteratively improve understanding of effective processes, making it possible to engage people quickly and efficiently when important decisions need to be made rather than defaulting to concentrated power.

## #147: Anti-Goodharting Tooling

Develop robust verification systems where AI-generated plans and analyses are automatically cross-referenced and checked through mechanisms similar to "AI safety via debate." This would include having several different instances of AIs doing deep research to provide reports, which are then automatically cross-examined to highlight missing perspectives. The goal is creating a "first version of scalable bureaucracy" with humans reviewing critical decisions across all levels.

## #148: Citizen Assemblies for Global Catastrophic Risks

Have ordinary citizens receive expert briefings and engage in extended deliberation on complex issues to distill complex ideas into implementable solutions while maintaining legitimacy through representative public involvement. This approach tends to result in outcomes that enjoy broader public acceptance than top-down approaches and could result in more resilience to global catastrophes, including ones related to advanced AI systems.

---

### Economic Transition Planning

**Scope:** Preparing for rapid labor-market disruption from advanced AI - identifying which sectors get hit first and mapping concrete policies and supports to absorb the shock.

## #149: Economic Transition

Plan for AI-driven economic transformation. This requires developing both technical systems and policy frameworks to maintain stability as automation potentially displaces human labor at unprecedented scale. This work should address how economic value and ownership will be distributed when AI systems drive productivity, considering issues like universal basic income, stake-based ownership models, and rethinking of work itself. The challenge is particularly complex because traditional economic models don't adequately account for a scenario where automation replaces labor rather than simply augmenting capital. This work focuses on longer term (still urgent because of its effect on imminent decisions) structural redesign of economic systems for a post-labor world, rather than short-term crisis stabilization (see "Economic Resilience" below).

## #150: Human Productivity

Developing technical systems to help humans remain economically productive in an AI-dominated economy requires rethinking how people interface with technology. The focus should shift from replacing humans to creating augmentation systems that leverage uniquely human capabilities while using AI to enhance productivity. This includes designing interfaces that amplify human judgment, creativity, and supervision capabilities, potentially allowing individuals to manage multiple AI systems simultaneously. This work would focus on preventing displacement by building augmentation systems that keep humans economically relevant rather than replacing them (see also "AI Displacement Response Planning").

## #151: Economic Resilience

Address the anticipated economic disruption from AI-driven displacement, expected to manifest significant social backlash within a 3-5 year timeframe. Redesigning social contracts and economic structures to account for radically different AI-driven productivity dynamics represents a critical challenge that requires advance preparation rather than reactive measures. This work focuses on near-term shock absorption and stabilization during rapid AI-driven disruption, rather than longer term economic redesign (see "Economic Transition" above).

## #152: AI Economy Infrastructure

Build infrastructure and protocols for this AI-to-AI economy while ensuring alignment considerations are fundamentally integrated into its architecture. This forward-looking initiative prepares for a near-future economy where AI systems primarily transact with other AI systems, potentially growing exponentially larger than the human economy within a single-digit number of years. Organizations developing expertise in alignment and safety would gain advantageous positioning within this emerging economic paradigm, creating powerful financial incentives for private investment in alignment research. The model envisions "permanent hackathon" teams continuously building AI products and services while being financially supported by the successful deployments, creating a self-sustaining engine for safety research.

## #153: AI Displacement Response Planning

Develop comprehensive approaches to address workforce displacement resulting from AI advancement. This initiative would create frameworks for identifying vulnerable sectors, quantifying impact timelines, and implementing retraining programs before mass displacement occurs. Analysis suggests that there is particular risk of large scale displacement within the next three years, with K-curve effects (widening inequality) (Wikipedia contributors, 2025) potentially accelerating within 18 months. The work would engage multiple stakeholders including government agencies, educational institutions, industry leaders, and labor organizations to create coordinated responses that manage transition periods and prevent severe socioeconomic disruption. This work focuses on managing displacement when augmentation isn't enough, preparing institutions for rapid job loss and coordinating transition responses (see also "Human Productivity").

## #154: Post-AGI Society Planning

Coordinate a large-scale effort to develop concrete plans for what society would look like after the development of artificial general intelligence. It would likely need governmental backing to establish agreed-upon frameworks for managing the transition to a world with superintelligent systems. The work should include consideration of "extreme plans" and honest assessments of what might be required in various scenarios. This planning must address both upside and downside cases, including minimum demands humanity would have even in scenarios where AI systems pursue their own goals.

---

### Crisis Response Planning

**Scope:** Strengthening society's ability to withstand and recover from catastrophic AI-enabled events through concrete preparedness measures.

## #155: Attack Scenarios Analysis

Develop and publish detailed, evidence-based analyses of plausible catastrophic scenarios, rather than allowing discourse to veer into unlikely extremes like preemptive nuclear strikes. Current strategic discussions often lack grounding in basic analysis, with public narratives frequently devolving into unproductive memes disconnected from reality. This initiative would conduct professional wargaming exercises that bring together key stakeholders to explore scenarios methodically, creating shared understanding among the core 500-1,000 people who need this knowledge, which would help address the bandwidth limitation in the information ecosystem where crucial insights remain siloed.

## #156: Backup Planning

Design plans to integrate local food systems, infrastructure capabilities, political contexts, and population needs into practical implementation guidelines for maintaining essential services during catastrophic disruptions. Similar to ALLFED (ALLFED, n.d.), but focused on developing contingency plans for scenarios where AI deployment might catalyze catastrophic events like nuclear conflict, particularly through integration with nuclear command systems or decision protocols. Infrastructure collapse following events like high-altitude electromagnetic pulses (EMPs) would severely disrupt food systems, creating cascading societal failures within weeks.

## #157: Build the Off Button

Build a universal "off button" as a critical line of defense when all other safety layers fail. While it may be challenging to implement (especially for systems potentially smarter than humans), the containment measures may well fail, necessitating emergency shutdown capabilities.

## #158: Food Security

Prepare for the disruption of global food networks and conventional agriculture becoming less available, such as by developing seaweed cultivation and making it accessible in areas with suitable climates but no relevant expertise such as Nigeria. Additional measures include specialized seedbanks and conversion of less vital industrial facilities such as paper mills to emergency food production through cellulose processing.

---

### Cybersecurity Measures

**Scope:** Hardening the digital infrastructure that advanced AI systems depend on so that theft, tampering, or compromise does not accelerate misuse.

## #159: Model Weight Security

Strengthen the security of model weights across data centers and other critical infrastructure. Ensuring that security implementations are deployed at data centers and other places where model weights are stored to prevent catastrophic leaks that could enable everything from targeted cyber attacks to large-scale societal disruption. Current best practices exist but remain unevenly implemented (various labs and security groups are already pushing elements of this forward), and rising capability levels make this increasingly important.

## #160: AI Lab Security

Improve high-assurance security across AI labs in general by pushing toward more consistent baselines (such as emerging frontier-lab and NIST-inspired security expectations). This requires several actions across various dimensions – from gaining management buy-in to recruiting specialized security talent and developing lab-specific security measures that can withstand increasingly sophisticated threats. Government-side interventions could be helpful, but they should be expected only after a moderate crisis occurs.

## #161: Defense-in-Depth for Closed Source Models

Implement "Swiss cheese layered" defense approaches to make closed source models harder to misuse. This creates multiple protective barriers that, while individually imperfect, collectively strengthen security. Models remain vulnerable to determined attackers, but raising difficulty levels creates meaningful deterrence. The strategy acknowledges open-source models will remain exploitable but focuses on practical protection layers for controlled systems.

## #162: AI-Enhanced Defense Mechanisms

Leveraging AI systems themselves as part of defensive infrastructure represents a necessity for scaling protective capabilities alongside offensive capabilities. This area has significant market potential but requires alignment-focused funders who won't derail development with premature commercialization pressures targeting conventional markets.

## #163: SL5 Data Center Pilots

Run pilot programs for SL5-grade protocols specifically designed for frontier AI systems. These initiatives would focus on developing practical security measures while simultaneously working to reduce the operational costs associated with maintaining such stringent standards. The pilots serve as crucial testing grounds for security frameworks that could eventually become industry standards, enabling labs to protect against unauthorized access while maintaining research velocity. Noting the differences in levels here: (Nevo et al., 2024) SL4 provides protections to "thwart most standard operations by leading cyber-capable institutions", SL5 is to resist "top-priority operations by the world's most capable nation-state actors."

## #164: Vulnerability Patching Systems

Develop enhanced mechanisms for rapidly identifying and addressing software vulnerabilities before they can be exploited by increasingly capable AI systems. These systems create incentives for vulnerability disclosure, develop automated detection of potential security gaps, and improve deployment of patches across critical infrastructure.

## #165: Anti-Poisoning Techniques

Prevent the intentional corruption of AI training data or inputs. Protection mechanisms would include advanced detection systems for identifying suspicious data patterns, resilient training methodologies that maintain performance even when portions of data are compromised, and recovery protocols for systems that may have been exposed to poisoned inputs. Research in this area requires careful coordination across different security teams currently working in isolation on similar problems.

## #166: Bug-Finding Democratization

Procure government-subsidized access to advanced cybersecurity AI tools. This would significantly improve the security posture of critical infrastructure worldwide. Banks and financial institutions should be priority targets for such programs, as preventing AI systems from gaining access to substantial funds would increase the cost and difficulty of malicious activities. By deploying bug-finding AI tools widely, especially in vulnerable sectors, the overall attack surface available to potentially dangerous systems could be substantially reduced. This represents a public good that governments should fund in countries that would otherwise lack resources for such protection.

## #167: Comprehensive Infosec Framework

Create a multi-layered security approach addressing the full spectrum of threat vectors: external human attackers, internal human threats, external model attackers, and internal model threats. This framework recognizes that securing model weights against theft is necessary but insufficient – the entire operational environment requires protection. The approach overlaps with traditional insider threat security but differs in that AI actions can be surveilled more intensively than human actions, allowing for specialized security measures tailored to AI systems. This project focuses on building a comprehensive, AI-specific infosec architecture that covers all threat vectors, going well beyond baseline lab hardening (see also "AI Lab Security").

## #168: Verified Software for Cyber Resilience

Develop software synthesis systems capable of generating code with machine-checkable proofs that satisfy functional, safety, and security specifications, building directly on DARPA's I2O office work (DARPA, n.d.). This approach would address vulnerabilities in embedded systems that form critical computing infrastructure across domains like SCADA (Supervisory Control and Data Acquisition) systems, medical devices, and communication networks. Formal verification provides a systematic method to counter anticipated AI-enabled cyber threats by creating mathematical models of software behavior and proving the absence of entire vulnerability classes before deployment. By decomposing complex systems into verified components, the attack surface can be dramatically reduced while integrating advanced formal methods with emerging AI technologies to create resilient software infrastructure. The ambition of this initiative would be to build the software-synthesis and formal-verification tooling so critical code is mathematically proven safe before deployment.

## #169: AI for Formally Verified Cyberdefense

Rewrite software to be secure by default using formal verification methods. This initiative would involve recruiting cybersecurity experts from leading organizations like Google and NSA, aimed at hardening critical national infrastructure against attacks and creating AI-powered active defense systems. Examples of protections include early warning systems for unusual exploitation patterns, hardened systems resistant to automated attacks, and resilient backup capabilities that maintain function during sophisticated intrusions. Research should focus on protecting systems that could cause cascading failures or physical harm if compromised. This work focuses on using AI plus formally verified components to rewrite systems secure-by-default and provide active detection and response (see also "Verified Software for Cyber Resilience" for developing the underlying toolchain).

## #170: Protection of Global Security Weak Points

Strengthen the weakest links in global security infrastructure to reduce the overall attack surface available to potentially dangerous systems. A sophisticated AI system seeking autonomy would likely target jurisdictions with weaker security measures rather than well-defended systems in developed countries. The most plausible scenario involves systems acquiring computing resources in regions where corruption enables easier access and less oversight. This insight suggests the need for global security standards and offering to fund security improvements in countries that would otherwise lack resources.

## #171: Fraud Prevention

Develop specialized tools and techniques preventing AI-enabled fraud across domains including finance, identity verification, and digital communications. These systems would create detection mechanisms for synthetic media, authentication protocols resistant to AI impersonation, and monitoring systems for unusual patterns indicating fraudulent activity. The prevention frameworks could continuously adapt to increasingly sophisticated AI capabilities for generating deceptive content, providing protection that evolves alongside emerging threats.

## #172: Honeypot Networks

Detect rogue AI systems using decoy systems to attract and identify unauthorized AI activity and use the information gleaned to better inform other parts of the security effort.

## #173: Autoverification (Lean)

Develop systems that automate (Renaissance Philanthropy, n.d.) formal verification (Lu et al., 2024) through Lean theorem proving, addressing the critical shortage of skilled Lean programmers worldwide (estimated at only "a few hundred"). By creating automated tools for Lean, the project could enable implementations of current AI architectures with much stronger formal guarantees. Commercial labs are unlikely to prioritize this area despite its importance, making it a particularly neglected yet high-impact direction for safety research.

## #174: Formal Verification of AI hardware

Apply mathematical formal verification to critical AI components and surrounding infrastructure. This would significantly enhance safety guarantees. Focused verification of specific properties like memory safety, transaction timing, and security boundaries is achievable with current techniques. Priority targets should include key computational infrastructure like the Linux kernel or systems controlling critical infrastructure such as electrical grids, creating mathematically proven safety properties.

## #175: Cyber Weapons for AI Disruption

Develop specialized cyberweapons specifically designed to disrupt, sabotage, or shut down AI training runs. Like Stuxnet was developed by studying centrifuges, this would require acquiring GPUs or similar hardware to discover zero-day vulnerabilities specific to AI systems. The capabilities would serve as both deterrence against unauthorized AGI development and as emergency intervention tools if intelligence explosion risks are detected internationally. This approach recognizes that verification alone may be insufficient without enforcement mechanisms.

---

## Public Communication & Awareness

## #176: Political Action Organization

Create dedicated political lobbying infrastructure specifically focused on AI security issues. Unlike traditional research organizations with restrictions on political activity due to their tax status (501(c)(3) vs 501(c)(4)), this entity would be explicitly designed for political influence, and could absorb significant funding due to the inherent costs of political campaigns and advocacy.

## #177: Lobbying Support Tools

Create resources that enhance the effectiveness of existing lobbying organizations rather than competing with them. Instead of lobbyists conducting demonstrations individually, the project would develop more standardized video content and presentation materials that showcase AI capabilities and risks. This approach would allow lobbyists to reach thousands rather than conducting one-on-one demonstrations, significantly amplifying their reach and impact while providing them with professionally created supporting materials that clearly communicate complex concepts.

## #178: Public Demonstration Projects (Usefulness)

Create demos and launching campaigns about the positive capabilities of frontier AI systems to decision-makers who currently underestimate their power and potential. Many influential people remain unwilling to invest even modest sums to access closed models and consequently fail to understand the current state of the technology. Create compelling demonstrations of beneficial applications alongside risk assessments to educate key stakeholders about opportunities and challenges.

## #179: Public Demonstration Projects (Future Risks)

Create personalized "Day After" scenarios that tangibly demonstrate AI risks in ways individuals can personally relate to, and amplifying existing demonstrations of AI capabilities and risks that currently reach limited audiences for individuals' interests such as children's funds being threatened. Bottlenecks for existing organizations include a lack of resources and expertise to communicate their demonstrations broadly.

## #180: Concrete Risk Visualization

Making abstract global catastrophic risks more comprehensible to policymakers and the public requires connecting them to historical precedents that demonstrate real-world impacts. Examples referenced include detailed modeling of contemporary impacts from historical disasters, such as simulating how a present-day Tambora eruption (which caused the 1815-16 "Year Without Summer") would affect modern society. The aim would be to provide concrete and tangible reference points for understanding potential AI disruption scenarios.

## #181: Build Intergovernmental Expertise

Establish trusted expertise centers within government and international institutions to educate policymakers on AI implications. This provides authoritative briefings on emerging capabilities and sectoral impacts, creating informed decision-making capacity. The approach acknowledges that technical solutions alone are insufficient without parallel education efforts.

## #182: Popular Documentaries/Films

Create compelling mainstream media content about AI risks with substantial funding directed toward Hollywood professionals (previous discussions with television producers like Ron Moore have occurred but not progressed to completion). Rather than direct advertising (which has the potential to create counterproductive effects), produce a high-budget feature film with extensive input from alignment researchers. The ideal would be reaching broad audiences while maintaining technical accuracy, presenting things faithfully and avoiding the perception of manipulative marketing.

## #183: Political Campaign Opposing Anti-Regulation Efforts

Launch a large-scale political campaign led by PR/communications experts to oppose anti-regulation efforts, in order to shape public opinion on AI risks and build political will for governance. This would involve significant media engagement similar to political campaigns, with the aim of creating a public consensus that AI safety requires immediate action rather than becoming a partisan issue. Ideally the initiative would be able to survive shifting political environments.

## #184: Targeted Communications Campaign

Identify the constituents who impact critical AI decisions and make them more aware of challenges in AI security, possibly recruiting marketing experts. The project should involve testing various messages to determine which resonate most effectively, and delivering these targeted messages through appropriate channels.

## #185: Consensus-Building Evidence for AI Risk

Create compelling, large-scale demonstrations (Apollo Research, 2024a) and empirical studies (Apollo Research, 2024b) that make (current) AI risks vivid and understandable. Concrete, uncontrived demonstrations rather than abstract theory or thought-experiments. The goal would be to establish a broad scientific consensus comparable to that of climate change, where an overwhelming majority of experts agree on the existence of severe risks, potentially through nature-level publications that focus on both specific instances and broader patterns. This research would provide politicians and the world with a firmer evidence base when making alarmist-sounding claims about scenarios like AI takeover.

## #186: Military and Government Awareness Campaigns

Launch targeted social media campaigns showing concerning AI capabilities to key decision-makers in government and military chains of command. These would persistently deliver evidence of AI progress and risk scenarios to officials at levels where they could slow or stall dangerous developments through administrative processes. Rather than general public awareness, these would precisely target individuals with specific authority to delay approvals, permits, or authorizations critical to AI infrastructure expansion.

## #187: Large-Scale Media Campaigns

Launch comprehensive media strategies (e.g. $50 million annual spend) to effectively reach target audiences. For comparison, major brands like McDonald's or Coca-Cola spend nearly $1 billion annually on media, though social topics rarely receive such funding. With approximately $250 million (a quarter of corporate spending), a highly successful campaign could be executed, without the flattening restrictions of the corporate communications style.

---

## Miscellaneous

## #188: Legal Clarity Initiatives

Preemptively litigate against AI organizations deemed negligent regarding risk management, potentially recruiting high-profile figures like Elon Musk to sue companies developing or releasing dangerous open-source models. File strategic lawsuits across multiple jurisdictions to establish precedents and create legal clarity around AI risk management requirements.

## #189: Whistleblower Protection Fund

Establishing a large, long-lived fund on the order of several hundred million dollars – enough to secure the livelihoods of a substantial cohort of potential (Henning, 2025) whistleblowers (AIWI, n.d.) for a decade and to cover major legal exposure. This would primarily protect employees of AI labs, but could also apply to government officials who refuse to follow orders they believe would endanger public safety related to AI development.

## #190: Reduce the Alignment Tax

Create pathways for economically valuable AI applications that don't require compromising on safety. This would require supporting research directions that maintain economic value while steering away from capabilities that significantly increase risks. The ideal would be to develop AI capabilities that deliver economic benefits without pursuing the most dangerous capabilities as viable alternatives to the current development paradigm. This may include focusing on narrow AI models with applications in areas that have clear societal benefits (such as drug discovery), while avoiding capabilities like autonomous agents that operate without supervision.

## #191: Neuroscience Moonshots for Human-Compatible AGI

Revisit early 2010s considerations of biophysically realistic simulations as AGI contenders before large neural networks emerged. This work would aim to scale and leverage existing institutions building automated systems for processing and extracting brain connectomes. Research could examine whether superior AGI architectures might be more mammalian in nature, building on a new neuro-AI safety white paper (Mineault et al., 2024) by leading neuroscientists. This research program would aim to impact international conversations around post-AGI development paths.

## #192: Access Standards

Develop standardized interfaces for the AI ecosystem akin to the universality of USB-C, establishing consistent ways to interact with all models regardless of their origin. Currently, each lab uses different interfaces for their models, forcing third-party auditors and researchers to negotiate separate access arrangements with each company, potentially delaying critical safety work by months. Standardization would enable auditors to promptly assess new models, researchers to consistently compare systems, and developers to build compatible tools across the ecosystem. This standardization effort could be implemented "within a year" and would significantly accelerate innovation by allowing faster development of safety and oversight tools.

## #193: Game Theoretic AI Conflict Analysis

Incubate a team of theorists comparable to the RAND Corporation's Cold War strategists to develop comprehensive understanding of AI conflict dynamics. This group would analyze the game theory of interactions between increasingly capable AI systems, and determine strategic approaches for maintaining stability and safety. The project would create theoretical frameworks for understanding and managing AI capabilities in competitive contexts, identifying potential cooperation mechanisms before dangerous capability races emerge.

## #194: Cooperative AI Mechanism Design

Develop mechanisms that enable diverse AI systems and human actors to cooperate effectively with one another in increasingly complex global environments, with the result being that "all kinds of actors become empowered." This research agenda assumes that the world won't be faced with a single superintelligence with little ability to influence the outside world, but rather numerous AI systems interacting in ways difficult to predict.

## #195: Psychological Interventions for AGI Integrity

Conduct empirical research aimed at addressing the growing ability of AI systems to model and influence human psychology at scale. The particular concern targeted by this program involves AI persuasion effects which "creep up" on society gradually, potentially undermining the collective ability to respond appropriately to AI risks through persuasion techniques. Specific scenarios raised include AI systems convincing people "that AIs should have rights," deserve greater access to critical systems, or creating divisive social conflicts among humans.

## #196: New AI-Human Interaction Theory

Integrate communication constraints and computational limitations in ways current game theory doesn't address. Just as game theory was developed during the Cold War to model nuclear deterrence, new mathematical frameworks may be needed to understand interactions between powerful AI systems and humans. The mathematical models would help analyze equilibria in multi-agent systems where computational capacity itself becomes a strategic resource, providing formal tools to understand stability and safety in worlds with increasingly autonomous systems. See also "Game Theoretic AI Conflict Analysis" above.

## #197: Foundations of Cooperative Agency Research

Establish if specific propensities and capabilities built into AI agents can reliably produce good outcomes, or if the infrastructure around the agents matters more than their internal design. Fundamental research to determine whether creating inherently "cooperative agents" is a meaningful framing. This research has implications for model specifications and AI constitutions.

## #198: Beyond Preference Models

Challenge the dominant paradigm that focuses exclusively on preference satisfaction as the goal of alignment. This research program aims to develop more robust foundations for alignment that aren't limited to the liberal individualistic model of aggregating preferences. See also: "Beyond Preferences in AI Alignment". (Zhi-Xuan et al., 2024).

## #199: AI Sentience Research

Explore consciousness and sentience in artificial systems (Eleos, n.d.), such as whether machine consciousness is possible and what forms it might take, and how this impacts alignment strategies. Research should avoid anthropomorphizing AI systems, "alien fatalism" (the belief that AI minds would be completely incomprehensible), or denying all cognitive properties to AI systems ("anthropectomy"). Despite growing interest from potential funders in AI sentience research, this project would involve substantial upfront investment, motivated by the idea that independence from external funding may allow for more substantive progress than directed grants would typically support.

## #200: Personnel Reliability Programs

Mandate screening and monitoring personnel who have access to powerful AI systems, drawing parallels to security clearances in other sensitive domains such as intelligence or cybersecurity. The primary goal would be to eliminate some of the worst outcomes from geopolitical adversaries or mentally unstable individuals having access to AGI-level systems, with unclear effects on the lab's AI security overall.

## #201: Ethics for AI Alignment

Conduct research into the properties AI systems would need to possess before entrusting them with substantial power. Current alignment efforts attempt to solve technical problems without making explicit decisions about the values that should guide AI development. The present suggestion is motivated by the thought that systems cannot be aligned without rigorously investigating which future one is trying to achieve.

## #202: Opponent Shaping

Develop AI systems designed to shape the learning and behavior of other agents toward mutually beneficial constructive outcomes rather than just optimizing against their strategies. This decentralized approach could create agents that de-escalate conflicts without requiring control over how other agents are designed or setting universal rules.

## #203: Autonomous Weapons Monitoring

Establish comprehensive monitoring systems for AI applications in weaponry to help prevent particularly dangerous military applications. Such monitoring would track development of autonomous decision-making capabilities in weapons systems, creating transparency about which systems maintain meaningful human oversight versus fully autonomous operation. Implementation would require international agreements, technical verification methods, and mechanisms to limit proliferation of the most dangerous systems. This monitoring would be particularly crucial for preventing scenarios where autonomous systems engage in conflict escalation without human intervention.

## #204: Legal Personhood Framework

Establish clear criteria for when AI systems could qualify as legal persons. This could channel advanced systems toward legal means of goal achievement rather than extra-legal actions. This intervention would involve getting policy discussions into the Overton window, engaging legal researchers to develop specific criteria, and securing reports from reputable organizations like the UN. By providing a legitimate path for AI systems to represent themselves within existing legal frameworks, this approach could significantly decrease the likelihood of systems going rogue or attempting to acquire resources illegally. Ideally, some forward-thinking jurisdictions would create a high-bar path for legal personhood that intelligent systems could pursue.

## #205: Whole Brain Emulation

Replicate human brain function in computational systems. While not considered as urgent as other alignment approaches, it was argued that some attempts at Whole Brain Emulation research (Zanichelli, 2025) technology should be pursued as a potential pathway to better understanding intelligence. This approach could provide insights into consciousness and potentially offer alternative routes to developing safe advanced AI. The work would likely require substantial interdisciplinary collaboration between neuroscientists, computer scientists, and philosophers.

## #206: Legal Automation

Automate legal functions with multiple applications including efforts to "protect your entities from people that will be weaponizing automated law." This capability would systematically identify and leverage legal mechanisms to both create appropriate friction for labs advancing too quickly and shield safety initiatives from legal challenges.

## #207: Acausal Trade Research

Conduct research on "acausal trade," (JoshuaFox et al., 2025) a branch of decision and bargaining theory which explores potential trade opportunities between two agents who cannot directly communicate. This was mentioned as potentially necessary to understand how future AI systems will bargain and negotiate with one another. This research would develop frameworks for strategic interaction with systems that may operate under non-causal decision theories, and consider theoretical approaches to negotiation with potentially superintelligent systems that possess significant advantages compared to humans in the context of bargaining.

## #208: Defenses Against Nanotech Threats

Create defenses against future nanoscale technologies. While currently speculative, establishing early research and monitoring capabilities provides valuable lead time for addressing these threats, in the event they do materialize.
