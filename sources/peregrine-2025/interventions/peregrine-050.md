# Safety Benchmark Development

**Source**: Peregrine 2025, Proposal #50
**Original**: https://riskmitigation.ai/wp-content/uploads/The-2025-Peregrine-Report.pdf
**Specificity**: Specific technique

## What it is
Establish comprehensive high-effort benchmarks for evaluating AI safety and security. Many safety benchmarks have been defunded or discontinued, leaving organizations without clear targets or metrics to aim for. Additionally, skepticism has been raised about the ability of benchmarks to provide reliable signals about actual safety properties. This initiative would develop rigorous, transparent methodologies for assessing various dimensions of AI safety, aiming to provide a common language for discussing safety progress across different models and organizations.

## Why it matters
Without agreed-upon benchmarks, there's no way to measure safety progress or compare different approaches. Labs can claim their models are "safe" without any standard for what that means. High-quality benchmarks would create accountability by providing objective measures that enable meaningful comparison across models and over time.

## Current state
- **Status**: Idea
- **Who's working on it**: Not specified (many safety benchmarks have been defunded/discontinued)

## Tags
Science
