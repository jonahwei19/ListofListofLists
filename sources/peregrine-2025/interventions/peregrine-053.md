# Classified Red Teaming

**Source**: Peregrine 2025, Proposal #53
**Original**: https://riskmitigation.ai/wp-content/uploads/The-2025-Peregrine-Report.pdf
**Specificity**: Specific technique

## What it is
Develop classified red teaming capabilities to understand the true scope of AI's offensive potential. This involves collaboration with Los Alamos National Laboratory specialists focused on chemical, radiological, biological, and nuclear threats (CBRN), drawing on key red-teaming experts from organizations like DeepMind and Microsoft to provide insights on systematic vulnerabilities and failure patterns.

## Why it matters
Creates an accurate assessment of exactly "how easy" it is to misuse AI for harmful purposes before developing appropriate countermeasures. Current public evaluations may underestimate true risk due to inability to test classified scenarios.

## Current state
- **Status**: Idea
- **Who's working on it**: Los Alamos National Laboratory, DeepMind, Microsoft (mentioned as potential collaborators)

## Tags
Security
