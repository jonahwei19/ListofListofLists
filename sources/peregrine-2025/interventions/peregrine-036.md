# Open Model Mitigations

**Source**: Peregrine 2025, Proposal #36
**Specificity**: Field/category

## What it is
Develop functional safeguards for open-source AI models (a currently unsolved problem). Current research focuses on finding technical controls that could limit harmful applications without restricting legitimate use cases. There's significant uncertainty whether such mitigations can be implemented quickly enough in short timeline scenarios, especially as existing models may already present above-baseline risks.

## Why it matters
Open-source models can't be controlled after release: anyone can fine-tune away safety measures. If open models reach dangerous capability levels, there's currently no technical solution. Developing mitigations that work even for open models would address one of the hardest problems in AI governance.

## Current state
- **Status**: Research
- **Who's working on it**: Not specified

## Tags
Security
