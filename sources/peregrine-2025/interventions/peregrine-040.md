# Direct Interpretability and Model-Level Interventions

**Source**: Peregrine 2025, Proposal #40
**Original**: https://riskmitigation.ai/wp-content/uploads/The-2025-Peregrine-Report.pdf
**Specificity**: Field/category

## What it is
Understanding what AI models are doing internally through mechanistic interpretability remains underdeveloped. Activation studies and AI control approaches need more exploration to develop viable interventions at the model layer.

## Why it matters
Most safety interventions operate at the input/output level (prompting, filtering) rather than the model level. Model-level interventions could be more robust because they modify how the model actually computes rather than just constraining its interface. But this requires understanding model internals well enough to make targeted modifications.

## Current state
- **Status**: Research
- **Who's working on it**: Not specified

## Tags
Science
