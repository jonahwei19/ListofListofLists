# Human Labeling Service

**Source**: Peregrine 2025, Proposal #6
**Original**: https://riskmitigation.ai/wp-content/uploads/The-2025-Peregrine-Report.pdf
**Specificity**: Specific technique

## What it is
Create a non-profit human labeling service offering billions of high-quality preference labels to replace proxies in reinforcement learning from human feedback (RLHF). Alignment techniques like RLHF use human data to train a proxy model of human preferences. With a suitable asynchronous setup to await human inputs, this approach could reduce misalignment by providing more reliable data. The project would need verification systems to prove it isn't conducting data poisoning attacks, and could aim to provide at-cost, high-quality data that labs would naturally want to use.

## Why it matters
RLHF relies on proxy models of human preferences because collecting enough real human feedback is expensive. But proxy models introduce misalignment since they're imperfect representations. Providing massive amounts of genuine human preference data at scale could reduce this misalignment while giving safety-focused organizations leverage over training processes.

## Current state
- **Status**: Idea
- **Who's working on it**: Not specified

## Tags
Science
