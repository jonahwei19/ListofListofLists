# Safety by Design - Scientist AI

**Source**: Peregrine 2025, Proposal #10
**Original**: https://riskmitigation.ai/wp-content/uploads/The-2025-Peregrine-Report.pdf
**Specificity**: Field/category

## What it is
Accelerate development of model-based approaches to AI, as represented by research programs like Yoshua Bengio's Scientist AI. The Scientist AI agenda aims to create non-agentic question-answering systems with Bayesian uncertainty quantification, interpretable causal theories, and mathematical convergence properties, which become safer with more compute rather than increasingly misaligned. The strategic objective would primarily be to develop a system that can act as a guardrail for agentic and potentially misaligned AI, as well as to demonstrate sufficient progress to convince key stakeholders that safer alternatives are possible.

## Why it matters
Current AI scaling trends suggest systems may become more dangerous as they become more capable. An alternative paradigm where systems become safer with more compute would fundamentally change the risk calculus. Even if not the primary AGI path, such systems could serve as guardrails for monitoring more dangerous agentic systems.

## Current state
- **Status**: Research
- **Who's working on it**: Yoshua Bengio, Mila

## Tags
Science
