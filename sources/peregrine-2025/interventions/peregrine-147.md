# Anti-Goodharting Tooling

**Source**: Peregrine 2025, Proposal #147
**Specificity**: Specific technique

## What it is
Develop robust verification systems where AI-generated plans and analyses are automatically cross-referenced and checked through mechanisms similar to "AI safety via debate." This would include having several different instances of AIs doing deep research to provide reports, which are then automatically cross-examined to highlight missing perspectives. The goal is creating a "first version of scalable bureaucracy" with humans reviewing critical decisions across all levels.

## Why it matters
AI systems may optimize for metrics that do not capture what we actually want (Goodhart's Law). Cross-examination and debate between AI systems could surface blind spots and manipulative outputs that single-model verification would miss.

## Current state
- **Status**: Research
- **Who's working on it**: Not specified

## Tags
Science
