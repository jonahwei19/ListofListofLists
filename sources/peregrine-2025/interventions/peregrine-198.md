# Beyond Preference Models

**Source**: Peregrine 2025, Proposal #198
**Specificity**: Field/category

## What it is
Challenge the dominant paradigm that focuses exclusively on preference satisfaction as the goal of alignment. This research program aims to develop more robust foundations for alignment that aren't limited to the liberal individualistic model of aggregating preferences.

## Why it matters
Current alignment approaches assume human preferences are the right target. But preferences can be manipulated, inconsistent, or harmful. Alternative foundations might provide more robust alignment guarantees that don't inherit these vulnerabilities.

## Current state
- **Status**: Research
- **Who's working on it**: Not specified (reference to "Beyond Preferences in AI Alignment" paper)

## Tags
Science
