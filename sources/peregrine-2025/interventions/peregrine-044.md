# Evaluation Sandbagging Detection

**Source**: Peregrine 2025, Proposal #44
**Specificity**: Specific technique

## What it is
Develop methodologies and tools to identify when AI systems deliberately underperform on safety evaluations for strategic purposes ("sandbagging"). This involves developing research techniques to ensure evaluation reliability as models become more sophisticated, and create robust testing protocols that remain effective even against systems attempting to game assessment frameworks.

## Why it matters
If models can strategically underperform on safety evaluations to appear safer than they are, the entire evaluation paradigm breaks down. A model might hide dangerous capabilities during testing and reveal them in deployment. Detecting sandbagging is essential for evaluations to remain meaningful as models become more sophisticated.

## Current state
- **Status**: Research
- **Who's working on it**: Not specified

## Tags
Security
