# Loss of Control Prevention

**Source**: Peregrine 2025, Proposal #23
**Original**: https://riskmitigation.ai/wp-content/uploads/The-2025-Peregrine-Report.pdf
**Specificity**: Field/category

## What it is
Develop technical and governance safeguards against scenarios where institutions cede critical decision authority to AI systems over time and operate beyond human oversight parameters. This includes implementing robust model-weight security and organizational security practices to prevent concerning behaviors such as "self-exfiltration," where models copy their own weights to infrastructure the developer does not control. It also involves sectoral legislation and institutional norms that prevent the gradual "enfeeblement of humans." Advancing these safeguards requires sustained education efforts for policymakers to understand subtle long-term risks beyond immediate threats.

## Why it matters
Loss of control may happen gradually rather than suddenly, through incremental delegation of decisions to AI systems until humans are no longer meaningfully in the loop. Preventing this requires both technical safeguards (preventing self-exfiltration, maintaining human override capability) and governance mechanisms (norms and laws that preserve human authority).

## Current state
- **Status**: Research
- **Who's working on it**: Not specified

## Tags
Security
