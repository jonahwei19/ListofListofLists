# Safety by Design - Formal Guarantees

**Source**: Peregrine 2025, Proposal #9
**Specificity**: Field/category

## What it is
Invest more substantially into research initiatives aimed at creating mathematical foundations for provably safe artificial general intelligence, such as David Dalrymple's "Safeguarded AI". This approach starts by constructing a first-principles 'secure-by-design' framework that would embed safety properties at the architectural level rather than adding them afterward. These approaches have relatively little funding and struggle to attract talent compared to mainstream AI.

## Why it matters
Current safety approaches are largely reactive: build a capable system, then try to make it safe. This approach offers a potential middle ground between completely halting AI development and building superintelligent systems with unacceptable risk profiles by designing safety into the architecture from the start with mathematical guarantees.

## Current state
- **Status**: Research
- **Who's working on it**: David Dalrymple (Safeguarded AI)

## Tags
Science
