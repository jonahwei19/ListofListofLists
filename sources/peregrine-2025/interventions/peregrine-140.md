# Biorisk Thresholds for Open Models

**Source**: Peregrine 2025, Proposal #140
**Original**: https://riskmitigation.ai/wp-content/uploads/The-2025-Peregrine-Report.pdf
**Specificity**: Specific technique

## What it is
Conduct more empirical work like wet lab studies to establish the conditional impacts of specific AI capabilities in this domain. These studies would quantify how capabilities like those demonstrated in benchmarks translate to real-world risk increases. AI systems are approaching capability thresholds where they could significantly increase bio-risks, potentially reaching high pre-mitigation risk levels within months. Without controls on open-source models, this could increase the risk of non-state actor bio-attacks by an order of magnitude within a year.

## Why it matters
There is currently no empirical basis for determining what level of AI biological capability is dangerous. Wet lab studies could establish concrete thresholds that inform when open model releases pose unacceptable biosecurity risks.

## Current state
- **Status**: Research
- **Who's working on it**: Not specified

## Tags
Security
