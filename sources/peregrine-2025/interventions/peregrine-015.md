# Pluralistic Alignment

**Source**: Peregrine 2025, Proposal #15
**Specificity**: Field/category

## What it is
Develop alignment research agendas that move beyond an overreliance on a particular worldview (described as "orthogonalist, individualistic") that may be too culturally and temporally specific. A more robust approach in future research would aim to embody a more ecumenical philosophical perspective rather than defaulting to a single framework based around preference-maximization.

## Why it matters
Current alignment work largely assumes a specific philosophical framework (preference utilitarianism, individualism) that may not be universal or even correct. If AI systems are aligned to a narrow philosophical view, they may conflict with other legitimate value systems. More pluralistic foundations could produce systems that work across diverse cultural contexts and remain robust to moral uncertainty.

## Current state
- **Status**: Idea
- **Who's working on it**: Not specified

## Tags
Science
