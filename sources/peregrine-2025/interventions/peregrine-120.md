# Realistic End States for AI Governance

**Source**: Peregrine 2025, Proposal #120
**Original**: https://riskmitigation.ai/wp-content/uploads/The-2025-Peregrine-Report.pdf
**Specificity**: Field/category

## What it is
Engage in theoretical research on potential long-term equilibria for AI governance. There are many examples of these, including scenarios where alignment keeps pace with capabilities, mutually assured destruction arrangements, coordinated slowdown, singleton scenarios, and robust international governance regimes. More propitious arrangements might include treaties prohibiting concealed computing centers, transparency about chip distribution, and mutual monitoring of major AI projects with failsafe mechanisms. The aim would be to produce technically and geopolitically realistic models of AI governance in light of both immediate safety needs and longer-term competitive dynamics.

## Why it matters
Without clear visions of what stable AI governance looks like, it is difficult to know what to aim for. Developing realistic models of end states helps orient current efforts and identify which near-term interventions lead toward desirable long-term outcomes.

## Current state
- **Status**: Research
- **Who's working on it**: Not specified

## Tags
Society
