# Limits of Model Distillation

**Source**: Peregrine 2025, Proposal #24
**Specificity**: Specific technique

## What it is
Investigate the theoretical limits of AI capabilities, including whether there are fundamental limits to model distillation. This would examine whether small, easily-diffused models will always be able to capture the capabilities of larger systems, or if there are physics-based constraints that naturally limit capability diffusion. The project would also analyze potential hardware bottlenecks, such as GPU interconnect innovations or test-time compute limitations, to better predict capability jumps.

## Why it matters
If dangerous capabilities can always be distilled into small, easily-shared models, containment strategies are fundamentally limited. But if there are physics-based limits to distillation, certain capabilities might require infrastructure (large compute, specialized hardware) that can be monitored and controlled. Understanding these limits is crucial for knowing which governance strategies are even possible.

## Current state
- **Status**: Idea
- **Who's working on it**: Not specified

## Tags
Science
