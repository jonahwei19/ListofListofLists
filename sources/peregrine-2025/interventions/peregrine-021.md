# AI Control Systems

**Source**: Peregrine 2025, Proposal #21
**Original**: https://riskmitigation.ai/wp-content/uploads/The-2025-Peregrine-Report.pdf
**Specificity**: Field/category

## What it is
Develop defensive systems specifically designed to contain potentially harmful AI systems even if alignment fails. This focuses on creating hardened technical environments where AIs operate - combining traditional information security with specialized machine-learning defenses to provide layered protection against "scheming AI" that may attempt to escape constraints. The emphasis is on building secure runtimes, strong sandboxing around the entire agent environment, and additional scrutiny at critical interaction points.

## Why it matters
Alignment might fail. If it does, the only remaining defense is containment. AI control assumes the model may be adversarial and builds systems that remain safe regardless. This provides a fallback that doesn't depend on successfully solving alignment, hedging against our uncertainty about whether alignment is even possible.

## Current state
- **Status**: Research
- **Who's working on it**: Redwood Research

## Tags
Security
