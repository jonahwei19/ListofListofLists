# Low-Stakes Alignment Settings

**Source**: Peregrine 2025, Proposal #20
**Specificity**: Specific technique

## What it is
Create environments where multiple consecutive malicious actions would be required to produce harmful outcomes. This approach doesn't aim to develop or test for perfectly aligned models, but create systems where AI control mechanisms have multiple opportunities to detect problematic behavior. This research project would involve solid sandboxing around agent environments to ensure that no bad outcome results except when the sandbox is deliberately breached, and develop security frameworks which create multiple layers of protection.

## Why it matters
Perfect alignment may be unachievable, but if harmful outcomes require a chain of malicious actions rather than a single failure, oversight systems have multiple chances to intervene. This defense-in-depth approach provides practical safety even with imperfectly aligned systems by creating multiple points where bad behavior can be caught.

## Current state
- **Status**: Research
- **Who's working on it**: Not specified

## Tags
Security
