# Misalignment Definitions and Measures

**Source**: Peregrine 2025, Proposal #25
**Original**: https://riskmitigation.ai/wp-content/uploads/The-2025-Peregrine-Report.pdf
**Specificity**: Specific technique

## What it is
Sharpen formal understanding of how AI systems pursue goals in ways that undermine user intent so we can have shared taxonomies and operationalize better. This includes refining frameworks to detect specification gaming and goal drift, clarifying what counts as an alignment failure, and developing practical ways to measure when a system is exploiting loopholes in its instructions.

## Why it matters
Without clear definitions of misalignment, the field can't measure progress or even agree on whether systems are aligned. Shared taxonomies enable coordination across research groups, regulatory bodies can set clear standards, and practical metrics allow tracking whether alignment is improving as capabilities scale.

## Current state
- **Status**: Research
- **Who's working on it**: Not specified

## Tags
Science
