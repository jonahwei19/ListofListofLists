# Interpretable Frontier Models

**Source**: Peregrine 2025, Proposal #41
**Specificity**: Specific technique

## What it is
Implement a moonshot research project focused on building interpretability constraints directly into model training from the beginning, rather than trying to analyze black-box models after the fact. This would be extremely expensive but potentially transformative for AI security. Some interpretability researchers have considered this approach viable but haven't pursued it due to the prohibitive costs involved. With sufficient funding (in the billions), this could become feasible.

## Why it matters
Post-hoc interpretability is fighting an uphill battle: models weren't designed to be understood. Training models with interpretability as a constraint from the start could produce systems that are inherently understandable, fundamentally changing the safety landscape by making opacity a choice rather than a given.

## Current state
- **Status**: Idea
- **Who's working on it**: Not specified

## Tags
Science
