# Problem Mapping

**Source**: Peregrine 2025, Proposal #30
**Specificity**: Specific technique

## What it is
Create a comprehensive list of critical AI safety problems that, if solved, would result in aligned AI systems. As an example, Open Philanthropy's (now Coefficient Giving) RFP is a good list but does not present an overarching framework or justification for thinking that solving the problems listed would lead to aligned AI systems. The goal of problem mapping is to systematically identify research areas that collectively guarantee safety, beyond the current piecemeal approach.

## Why it matters
Current safety research is fragmented: many problems are studied without knowing whether solving them would actually produce aligned AI. A comprehensive map showing which problems are necessary and/or sufficient for alignment would enable strategic resource allocation, identify neglected areas, and provide confidence that solving the right problems will actually work.

## Current state
- **Status**: Idea
- **Who's working on it**: Open Philanthropy/Coefficient Giving (partial)

## Tags
Science
