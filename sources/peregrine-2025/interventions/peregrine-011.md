# Theoretical Alignment Research

**Source**: Peregrine 2025, Proposal #11
**Specificity**: Field/category

## What it is
Conduct fundamental investigations into the mathematical and conceptual foundations of AI alignment outside frontier labs. This research would develop formal verification methods and theoretical frameworks ensuring AI systems remain aligned with human values across different architectures and development approaches.

## Why it matters
Without solid theoretical foundations, alignment work risks being ad-hoc patches that fail as systems scale. Mathematical frameworks could provide guarantees that hold across architectures, rather than empirical fixes that might break with the next capability jump. Independent research outside labs avoids conflicts of interest in corporate safety work.

## Current state
- **Status**: Research
- **Who's working on it**: Not specified (academic researchers)

## Tags
Science
