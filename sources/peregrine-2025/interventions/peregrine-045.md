# Training AI Evaluators

**Source**: Peregrine 2025, Proposal #45
**Original**: https://riskmitigation.ai/wp-content/uploads/The-2025-Peregrine-Report.pdf
**Specificity**: Specific technique

## What it is
Train specialized investigator AI agents specifically to analyze the vast amounts of data generated by frontier models, including millions of neuron activations, extensive training datasets, and complex interaction logs. Current approaches to detecting AI capabilities and failure modes are severely limited, relying on simplistic evaluations that miss critical behaviors until they emerge in deployment. For instance, cybersecurity evaluations involve testing models against a small set of predefined tasks rather than comprehensively assessing models across wider contexts. Research must move beyond benchmark-focused evaluations toward holistic understanding of model capabilities in real-world contexts.

## Why it matters
The scale of data from modern AI systems exceeds human analysis capacity. Training AI systems specifically to evaluate other AI systems could enable more thorough assessment than humans could perform, catching risks that current evaluation approaches miss. This is essentially using AI to scale oversight.

## Current state
- **Status**: Research
- **Who's working on it**: Not specified

## Tags
Science
