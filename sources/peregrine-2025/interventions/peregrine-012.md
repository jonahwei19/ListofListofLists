# Cooperative Alignment

**Source**: Peregrine 2025, Proposal #12
**Specificity**: Specific technique

## What it is
Investigate ways to structurally align AI systems' values with human values by creating fundamental overlaps between their respective utility functions. This approach has demonstrated some early results in reducing AI deception tendencies. See for instance Self-Other Overlap research.

## Why it matters
Rather than constraining AI systems externally, this approach tries to make cooperation intrinsically beneficial to the AI by designing utility functions where human and AI interests naturally align. If successful, this could produce systems that are cooperative by design rather than by constraint, reducing the adversarial dynamic of current safety approaches.

## Current state
- **Status**: Research
- **Who's working on it**: Self-Other Overlap researchers

## Tags
Science
