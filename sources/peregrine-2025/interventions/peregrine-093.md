# Barriers to Fine-tuning

**Source**: Peregrine 2025, Proposal #93
**Specificity**: Specific technique

## What it is
Develop technical and policy barriers that make unauthorized fine-tuning of advanced AI models extremely difficult, preventing proliferation of dangerous capabilities. These barriers would combine technical measures like secure hardware enclaves, cryptographic verification of model origins, and detection systems for identifying unauthorized derivatives. Policy frameworks would establish legal consequences for circumventing protections while providing legitimate access paths for authorized research.

## Why it matters
Without barriers, safety measures can be fine-tuned away by anyone with access to model weights, negating the value of alignment work.

## Current state
- **Status**: Research
- **Who's working on it**: Not specified

## Tags
Security
