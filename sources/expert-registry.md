# Expert Registry

Intellectual sources organized by what they "know about" - use this to identify which experts to consult for different questions.

## How to Use

When working on a topic, scan this registry for relevant experts. Read their index.md to get their perspective before synthesizing.

**See also**: `expert-disagreements.md` for structural disagreements between experts.

---

## Alignment & Control

### Joe Carlsmith - Alignment Series
**Folder**: `sources/carlsmith-alignment/`
**Knows about**: Rogue AI mechanics, option control vs motivation control, power-seeking analysis, global vulnerability conditions, hand-off framing
**Consult for**: Technical alignment questions, kill chain refinement, defense-in-depth strategies
**Orientation**: Rigorous analytical, focused on mechanisms

### Joe Carlsmith - Otherness and Control
**Folder**: `sources/otherness-agi/`
**Knows about**: Deep atheism (mistrust of Nature/intelligence), yin/yang (receptivity vs control), gentleness toward AI as "other", liberalism/boundaries, "green" (attunement, reverence), the tree vs optimizer metaphor
**Consult for**: Ethical dimensions of alignment, what "good" human-AI coexistence looks like beyond control, pluralism arguments, the romance of control, how to avoid becoming what we fight
**Key insight**: "What ethical resources do we have for relating to agents with different values *beyond* pure control?"
**Orientation**: Philosophical, challenges pure control framing, existential-positive despite deep atheism

---

## Flourishing & Good Futures

### Will MacAskill - Better Futures
**Folder**: `sources/macaskill-better-futures/`
**Knows about**: S×F framework, multiplicative model of value, path dependence, value lock-in, convergence skepticism
**Consult for**: Why flourishing matters beyond survival, what makes futures fall short of optimal
**Orientation**: EA-native, utilitarian, scale-focused

### Michael Nielsen - Wise Optimism
**Folder**: `sources/nielsen-wise-optimist/`
**Knows about**: Armageddon vs Kumbaya heuristics, recipes for ruin, market vs non-market safety, coceleration, hyper-entities, the generalized Alignment Problem
**Consult for**: Techno-optimist framing, threading needle between doomerism and naive optimism, why technical alignment is mostly market safety, institutional interventions (Pigouvian taxes, vision prizes)
**Key insight**: "The Kumbaya graph is a *tremendous achievement* of existing institutions, not an inevitable outcome."
**Structural take**: The central problem isn't rogue AI—it's that ASI will greatly increase power available to individuals/small groups. The Alignment Problem is civilization-level and ongoing.
**Orientation**: Progress-focused, wise optimism (accepts risk, works to overcome it), coceleration over accel/decel binary

### Vitalik Buterin - d/acc
**Folder**: `sources/vitalik-dacc/`
**Knows about**: d/acc (defensive/decentralization/differential acceleration), offense/defense balance, technology as shield vs sword, defense-favoring worlds, crypto-native governance, Balvi's bio-defense work
**Consult for**: Positive framing (what to build), defense technologies, techno-optimist audience, decentralization vs power concentration, bio/pandemic defense interventions
**Key insight**: "A defense-favoring world is a better world... it makes it easier for healthier, more open and more freedom-respecting forms of governance to thrive."
**Structural take**: Defense vs offense is more fundamental than specific threat categories. The four domains: macro physical, micro physical (bio), cyber, info.
**Orientation**: Crypto/decentralization, defensive technology, skeptical of solutions that concentrate power

### Helen Toner - Dynamism
**Folder**: `sources/toner-dynamism/`
**Knows about**: Dynamism vs stasis (Virginia Postrel), stasist tendencies in AI safety, alignment as steerability, concentration-of-power risks, dynamist rules
**Consult for**: Critiquing stasist AI safety solutions, policy that preserves dynamism, transparency/audit ecosystems, why "whose values?" is separate from "steerable at all?"
**Key insight**: "If we handle [catastrophic risks] by massively concentrating power, we haven't succeeded."
**Structural take**: AI safety can have stasist tendencies, but AI itself threatens dynamism. Need solutions that address risks without collapsing into stasis.
**Orientation**: Policy-focused, former OpenAI board member, dynamism over stasis

---

## Scenarios & Timelines

### AI Futures Project
**Folder**: `sources/ai-futures-project/`
**Authors**: Daniel Kokotajlo (founder, ex-OpenAI), Eli Lifland, Scott Alexander, Nick Marsh, Alex Kastner, Steven Veld
**Knows about**: Short timelines (AGI 2027), fast takeoff dynamics, concrete AGI scenarios, CEO takeover pathways, multi-AI competition dynamics, Plans A/B/C/D framework, verification mechanisms, chip registries
**Consult for**: What AGI development actually looks like, lock-in via aligned AI, multi-AI conflict scenarios, policy priorities for governments, timeline-sensitive interventions
**Key insight**: "Training AGI in secret would be unsafe and unethical" — secret development removes accountability and competitive pressure for safety
**Structural take**: Threats are more entangled than our tiling suggests. Agent-4 is simultaneously rogue AI, instrument of lock-in, and multi-AI competitor. Even "good" scenarios involve massive power concentration.
**On Lock-in**: "Our team's guesses for the probability of a CEO using AI to become dictator, conditional on avoiding AI takeover, range from 2% to 20%"
**On Flourishing**: "Humanity goes off to settle the galaxies, reaching grand heights but forever foreclosed from three-fourths of its potential" — even good endings involve massive compromise
**Orientation**: Scenario-focused, quantitative forecasting, short timelines, urgent tone

---

## Threat-Specific

### Entente Framework
**Folder**: `sources/entente-threats/`
**Knows about**: Threat taxonomy (Rogue AI, Lock-in, Multi-AI Conflict, AI-Enabled WMD), kill chain structure
**Consult for**: Categorizing threats, identifying which kill chain step an intervention targets
**Orientation**: Systematic taxonomy

### Peregrine Report
**Folder**: `sources/peregrine-2025/`
**Knows about**: 208 concrete proposals across 8 domains, intervention details, who's working on what
**Consult for**: Filling in intervention details, finding actors and organizations
**Orientation**: Comprehensive list, government-adjacent

---

## Star Trek vs Star Wars Futures

A key framing distinction across sources:

**Star Wars future** (control-focused):
- Humans maintain dominance
- AI as tool/threat to be managed
- Risk-mitigation primary
- MacAskill's "lock-in" concerns

**Star Trek future** (coexistence-focused):
- Humans and AI as genuine partners
- AI as "other" with its own perspective
- Mutual flourishing
- Carlsmith's "otherness" framing, Nielsen's dynamism

The Flourishing category should draw on BOTH perspectives, not just the EA/control framing.

---

## Cross-Cutting Themes

**On control vs openness:**
- Carlsmith (Otherness): Pure control loses something essential; need attunement, yin, receptivity
- Nielsen: Coceleration over accel/decel; continuous bending of the curve rather than freeze or sprint
- Vitalik: Defense vs offense; defense-favoring worlds enable better governance
- Toner: Dynamism vs stasis; avoid stasist solutions even when addressing real risks
- AI Futures: Transparency over secrecy; secret development is unethical regardless of intentions

**On what makes futures good:**
- MacAskill: S×F framework, multiplicative model, path dependence
- Carlsmith: Attunement, genuine encounter with otherness, wisdom not just smarts
- Nielsen: "Loving plurality of posthumanities", imagination and meaning, not just abundance
- Vitalik: Defense-favoring world where humans aren't pets
- Toner: Dynamism—creation, discovery, competition; human agency preserved
- AI Futures: Even "good" scenarios involve massive compromise (three-fourths of potential lost)

**On institutions:**
- Nielsen: Market safety vs non-market safety (non-market undersupplied)
- Carlsmith: Liberalism/niceness/boundaries as resources for navigating value pluralism
- Vitalik: Defensive infrastructure, open-source public goods
- Toner: Dynamist rules, transparency, audit ecosystems
- AI Futures: Government situational awareness, verification mechanisms, chip registries

**On the central problem:**
- Our tiling: Four distinct threats with kill chains
- Nielsen: Civilizational alignment, ongoing, power concentration
- Carlsmith: How to relate to artificial others beyond pure control
- Vitalik: Offense/defense balance at civilizational level
- Toner: Preserving dynamism while addressing real risks
- AI Futures: Threats are entangled; timelines are short; prepare now

---

## Expert Agreement Map

| Topic | Nielsen | Carlsmith (O) | Vitalik | Toner | AI Futures |
|-------|---------|---------------|---------|-------|------------|
| Technical alignment sufficient? | No | No | No | No | No |
| Power concentration a concern? | Yes | Partial | Yes | Yes | **Primary** |
| Open-ended future? | Yes | Yes | Yes | Yes | Uncertain |
| Defense over control? | — | Partial | Yes | Partial | Transparency |
| Positive framing? | Coceleration | Tree growth | d/acc | Dynamism | Scenario prep |
| Short timelines? | Moderate | — | — | — | **Very** |
| Threats entangled? | Yes | Partial | Partial | Partial | **Yes** |

---

*Last updated: 2026-01-11*
