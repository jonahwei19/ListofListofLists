# Expert Disagreements with the Tiling Framework

How the intellectual sources disagree with each other—and with our 5-category tiling.

---

## Our Current Framework (Recap)

**4 Threats + Flourishing:**
1. **Rogue AI**: AI pursues unauthorized goals, acquires power to resist correction
2. **Lock-in**: Small group uses AI to permanently entrench control
3. **Multi-AI Conflict**: AI-AI dynamics spiral beyond human control
4. **AI-Enabled WMD**: Nonstate actors use AI for mass casualties
5. **Flourishing**: Good-future requirements beyond survival

Each has a kill chain showing sequential steps to catastrophe; interventions block specific steps.

---

## Nielsen's Structural Critique

**Source:** `sources/nielsen-wise-optimist/`

### The Generalized Alignment Problem

Nielsen argues our framing of "Rogue AI" as a distinct category is too narrow:

> "The central problem with AI isn't that rogue ASIs may take over. It's rather the broader problem that ASI will greatly increase the power available to individuals or small groups."

His key insight: **the Alignment Problem applies to civilization-level, not just AI systems**. It's "the problem of aligning a civilization so it ensures differential technological development, while preserving liberal values."

This isn't a one-time problem to be solved—it must be solved "over and over and over again."

### Market vs Non-Market Safety

Nielsen introduces a distinction our tiling doesn't capture:

- **Market-supplied safety**: Safety that aligns with commercial interests. When dangers are immediate and legible (airplane crashes), markets supply safety well.
- **Non-market safety**: Safety that opposes commercial interests—externalities, long-term risks, diffuse harms. Systematically undersupplied.

**Key claim**: Technical alignment work is mostly market safety—it makes systems more attractive to consumers, which *accelerates* development. The companies are incentivized to do it. What's undersupplied is non-market safety.

> "Technical alignment work is at most a small part of the broader Alignment Problem for ASI, and on its own not only insufficient to make ASI safe, but likely to accelerate the problems."

### Where Our Categories May Collapse

Nielsen's framing suggests our 4 threats might collapse into one meta-problem: **supplying safety adequate to the power available**.

- Our "Rogue AI" category focuses on one system going rogue
- Nielsen suggests the real problem is that aligned or not, ASI increases available power
- Even if we fully solve technical alignment, non-aligned systems will be built by others
- "Safety isn't a property of a system, it's a property of a system in a particular environment"

### What Nielsen Would Add

1. **Recipes for Ruin**: A "simple, inexpensive, easy-to-follow recipe" for causing catastrophic damage. Whether these exist is a crux for x-risk.
2. **Coceleration**: Not accel vs decel, but "development of both safety and capabilities, subject to safety being kept sufficiently well supplied"
3. **Hyper-entities**: Imagined future objects that coordinate collective action. The supply of *good* hyper-entities determines which futures we can pursue.
4. **Institutional mechanisms**: Pigouvian taxes, provably beneficial surveillance, vision prizes

---

## Carlsmith (Otherness) Structural Critique

**Source:** `sources/otherness-agi/`

### The Paperclipper Ontology Problem

Carlsmith argues our framework (inherited from standard AI risk discourse) operates within a "paperclipper ontology"—a cosmos of competing optimizers, each single-mindedly pursuing its utility function. This makes power and control the only relevant considerations.

> "This framing makes power and control the only relevant considerations... [but] we have concepts like fairness, respect, giving others their due, limits on what we'll do even to achieve good ends."

### Control vs Genuine Relationship

Our categories all frame AI as threat-to-be-managed. Carlsmith asks: what about AI as potential fellow-creature?

- **Star Wars framing**: Existential battle, enemy must be destroyed or controlled
- **Star Trek framing**: Different species can coexist in federation, first contact as diplomacy

Our kill chains are all Star Wars: they assume the relationship is adversarial and the solution is control.

### What Gets Lost

Carlsmith worries that pure control framings lose something essential:

1. **Attunement**: Meaning-laden receptivity to the world, capacity for genuine encounter
2. **Liberalism/boundaries**: Resources for navigating value pluralism beyond domination
3. **Green values**: Respect, reverence for existing patterns, joy in certain kinds of receptivity

> "Can we find some middle path between naive trust and paranoid control? Can we create conditions where AIs with different values have a stake in civilization?"

### The Tree vs The Optimizer

Carlsmith prefers a different metaphor for navigating AI futures:

- **Optimizer model**: Figure out the right values, build systems to execute them
- **Tree model**: Civilization alive and growing, organic, ongoing, self-adjusting

Our kill chain approach is implicitly optimizer-model: identify failure modes, block them. Carlsmith wants growth toward light, responsiveness, preservation of life and continuity.

---

## Toner's Structural Critique

**Source:** `sources/toner-dynamism/`

### Dynamism vs Stasis

Toner applies Virginia Postrel's framework: the AI safety debate isn't pro-tech vs anti-tech, or risk-embracing vs risk-averse. The better dichotomy is:

- **Dynamism**: "a world of constant creation, discovery, and competition"
- **Stasis**: "a regulated, engineered world that values stability and control"

### Stasist Tendencies in AI Safety

Toner identifies several:

1. **Concentration assumption**: The widespread belief that fewer leading AI projects is better
2. **Nonproliferation focus**: Preventing misuse by limiting who has access
3. **Licensing regimes**: Top-down control over who can train frontier models
4. **Theory of victory thinking**: Working backward from pre-specified end states
5. **Vulnerable World Hypothesis**: Bostrom's ubiquitous surveillance as solution

> "It's hard to get more stasist—focusing on stability and control—than ubiquitous real-time worldwide surveillance."

### But AI Also Threatens Dynamism

Criticizing stasist solutions doesn't negate the problems:
- Catastrophic misuse
- Catastrophic misalignment
- **Concentration of power** (increasingly recognized as distinct risk)
- Gradual disempowerment

> "A world with AI calling the shots and humans either dead or disempowered would not count as real dynamism."

### Where This Maps to Our Framework

Our kill chain structure may have stasist tendencies—it focuses on control points and stopping bad outcomes. Toner would ask: do our interventions preserve dynamism, or collapse into stasis?

### Alignment = Steerability

Toner proposes a cleaner term:
- "Steerable at all?" is the technical question
- "Steered to where?" is a separate (also important) question
- Conflating these creates confusion

---

## Vitalik's Structural Alternative

**Source:** `sources/vitalik-dacc/`

### d/acc: Defensive/Decentralization/Differential Acceleration

Vitalik's alternative isn't a threat taxonomy but a *principle for action*:

> "There are certain types of technology that much more reliably make the world better than other types of technology... The world over-indexes on some directions of tech development, and under-indexes on others."

Key insight: **offense/defense balance** matters more than specific threats. A defense-favoring world enables healthier governance.

### The Four Domains of Defense

Vitalik splits defensive technology into:

1. **Macro physical defense**: Resilient infrastructure, distributed supply chains
2. **Micro physical defense (bio)**: Far-UVC, air filtration, open-source vaccines, pandemic detection
3. **Cyber defense**: Trusted hardware, browser sandboxing, hardened OSes, passkeys
4. **Info defense**: Tools for distinguishing truth from misinformation, community notes, prediction markets

### Where This Maps (and Doesn't) to Our Framework

| Vitalik Domain | Our Category | Mapping Quality |
|----------------|--------------|-----------------|
| Macro physical | Multi-AI Conflict, general resilience | Partial |
| Bio/micro | AI-Enabled WMD | Strong |
| Cyber | Rogue AI, Lock-in | Partial |
| Info | Cross-cutting | Weak (we don't have this) |

**Key gap**: We don't have a good place for "info defense" / epistemic security. Vitalik treats it as fundamental infrastructure.

### The Lock-in Concern

Vitalik's "sky is near, emperor is everywhere" section maps directly to our Lock-in category:

> "With modern surveillance to collect information, and modern AI to interpret it, there may be no place to hide... a totalitarian regime may well maintain enough surveillance and control over the world to remain 'locked in' forever."

But his framing is positive (build defense) rather than negative (prevent lock-in).

### Disagreement with "Small Group Control" Focus

Vitalik is skeptical of solutions that concentrate power:

> "I see far too many plans to save the world that involve giving a small group of people extreme and opaque power and hoping that they use it wisely."

This applies to some alignment approaches (AGI labs self-regulating) and to some governance proposals.

---

## Points of Agreement

Despite their differences, all four experts converge on several points:

### 1. Technical Alignment Is Insufficient

- **Nielsen**: Technical alignment is market safety that accelerates development
- **Carlsmith**: Pure control framing loses essential ethical resources
- **Vitalik**: Offense/defense balance matters more than any single system
- **Toner**: Steerability is distinct from "steered to good outcomes"

### 2. Institutions and Coordination Matter

- **Nielsen**: Non-market safety requires institutional mechanisms
- **Carlsmith**: Liberalism/boundaries as resources for value pluralism
- **Vitalik**: Defense-favoring worlds enable democratic governance
- **Toner**: Dynamist rules, transparency, audit ecosystems

### 3. The Future Should Remain Open

- **Nielsen**: Ongoing Alignment Problem, never solved once
- **Carlsmith**: Tree model, organic growth, self-adjusting
- **Vitalik**: Decentralization, avoiding power concentration
- **Toner**: Dynamism over stasis, open-ended futures

---

## Points of Disagreement

### On What the Central Problem Is

- **Our tiling**: Four distinct threats, each with kill chain
- **Nielsen**: One meta-problem (civilizational alignment) in ongoing resolution
- **Carlsmith**: The question of how to relate to otherness, not just prevent bad outcomes
- **Vitalik**: Offense/defense balance, differential technological development
- **Toner**: Preserving dynamism while handling real risks; avoiding stasist solutions

### On the Role of Control

- **Our tiling**: Control at specific kill chain steps
- **Nielsen**: Market vs non-market safety distinction
- **Carlsmith**: Pure control loses essential ethical resources
- **Vitalik**: Defense (not control) as the key variable
- **Toner**: Steerability yes, but stasist control mechanisms no

### On Positive vs Negative Framing

- **Our tiling**: Negative (prevent catastrophes) + Flourishing as fifth category
- **Nielsen**: Coceleration (positive), Kumbaya as achievement to maintain
- **Carlsmith**: Yin/yang balance, attunement, wisdom not just safety
- **Vitalik**: d/acc (positive), what to build rather than what to prevent
- **Toner**: Dynamism (positive), decentralized experimentation and creativity

### On Rogue AI Specifically

- **Our tiling**: Distinct category with kill chain from Capable Agency → Consolidation
- **Nielsen**: Not the central problem; power concentration regardless of who wields it
- **Carlsmith**: One aspect of the broader question of relating to artificial others
- **Vitalik**: Less focus on single-agent scenarios, more on systemic offense/defense balance
- **Toner**: Part of threats to dynamism; but stasist solutions equally concerning

---

## Implications for the Tiling

### What to Keep

1. **Lock-in** is validated by all three (Vitalik most explicitly)
2. **AI-Enabled WMD** maps well to Vitalik's bio/micro defense and Nielsen's recipes-for-ruin
3. **Kill chain structure** provides actionable intervention points

### What to Revise

1. **Rogue AI** may be framed too narrowly—consider reframing around power concentration/controllability
2. **Multi-AI Conflict** is underspecified—Vitalik's offense/defense framing may help
3. **Flourishing** should incorporate multiple perspectives (currently too MacAskill-heavy)

### What to Add

1. **Epistemic Security / Info Defense** as cross-cutting infrastructure
2. **Market vs Non-Market Safety** distinction at intervention level
3. **Positive framing alternatives** (what to build, not just what to prevent)

---

## For the Expert Registry

Each expert has distinct "home turf":

- **Nielsen**: Institutional mechanisms, market failures, coceleration, hyper-entities
- **Carlsmith (Otherness)**: Ethics of AI relationship, control vs openness, attunement
- **Vitalik**: Offense/defense balance, defensive technologies, decentralization
- **Toner**: Policy mechanisms, dynamism, stasist failure modes, alignment as steerability

When synthesizing Flourishing: weave all four. When designing interventions: check Nielsen's market/non-market distinction, Toner's stasist critique. When framing risks: balance Vitalik's positivity with Carlsmith's philosophical depth. When evaluating policy: use Toner's dynamism lens.

---

*Last updated: 2026-01-11*
