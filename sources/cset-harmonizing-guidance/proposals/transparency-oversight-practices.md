# Transparency & Oversight Practices

**Maps to interventions**:
- `public-model-audits.md`
- `rsp-implementation.md`
- `whistleblower-protection-fund.md`
- `human-ai-cooperative-evals.md`

## Relevant CSET Practices (Transparency & Oversight Topic)

### TO-1: User Awareness and Communication
> "Provide users of a system with documented instructions, guidance, and training on its proper use. Convey information on the system's risks and limitations. Build informative alerts and notifications into the operation of the system."

**AI Score**: 100%

### TO-2: Explainability and Interpretability
> "Establish transparency, explainability, and contestability (TEC) requirements for system development and use. Where possible, provide explanations to users on how decisions or outputs were reached."

**AI Score**: 98%

### TO-3: Human Oversight and Accountability
> "Meaningfully incorporate human oversight and agency into the design of models and systems. Ensure that a human-in-the-loop remains accountable for system output and in control of its operation."

**AI Score**: 96%

### TO-4: Data and Safeguard Transparency
> "Put in place mechanisms to flag issues of bias, harmful output, poor performance, and misuse. Provide transparency around the implementation of these safeguards without violating their integrity."

**AI Score**: 98%

### TO-6: Mechanisms and Documentation
> "Establish formal mechanisms to build transparent practices into the organization's development and use of technology. Produce transparency reports and model cards to disclose details about the development or use of AI models."

**AI Score**: 100%

### TO-7: Public Accountability
> "Hold the organization accountable to the public, providing transparency and protecting consumer rights. Engage with stakeholders to ensure that harms caused by the organization or its systems are adequately redressed."

**AI Score**: 100%

## Connection to Existing Interventions

**Public Model Audits**: TO-6's "transparency reports and model cards" and TO-7's "hold the organization accountable to the public" directly support public audit practices. The intervention's goal of "templates for what information should be available about any deployed AI system" aligns with TO-6.

**RSP Implementation**: TO-3's "human oversight and accountability" and TO-7's public accountability mechanisms relate to enforcement and social pressure for RSP compliance.

**Whistleblower Protection**: TO-4's "mechanisms to flag issues of bias, harmful output, poor performance, and misuse" requires protected channels for internal reporting. The report notes the importance of "organizational culture that prioritizes safety" (AU-6).

**Human-AI Cooperative Evals**: TO-3's "meaningfully incorporate human oversight and agency into the design of models and systems" applies to evaluation design as well as deployment.

## Key Quote on Transparency Need

From the report's Insights section:
> "Generally speaking, it is difficult to anticipate all of the possible outputs that a software system will produce. The probabilistic nature of AI, the large input and output spaces of advanced models, and the purposeful inclusion of randomness in many generative AI tools in order to produce non-repetitive results further complicate this problem. In addition, many advanced models, such as neural networks and transformers, are considered to be black boxes in that the decisions that were reached cannot be explained in terms that humans can understand. Existing guidance highlights this gap and strongly recommends that organizations deploying AI systems in decision-making contexts provide mechanisms to provide impacted stakeholders with insight into the decision-making process and mechanisms to contest the output."

## Very High AI Scores

The Transparency & Oversight topic has the highest concentration of 100% AI Score practices, confirming this as uniquely AI-driven guidance rather than general organizational practice.
