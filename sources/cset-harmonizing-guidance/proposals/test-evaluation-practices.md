# Test & Evaluation Practices

**Maps to interventions**:
- `safety-benchmarks.md`
- `evaluation-companies.md`
- `cbrn-misuse-evaluations.md`
- `sandbagging-detection.md`

## Relevant CSET Practices (Test & Evaluation Topic)

### TE-1: Policy and Planning
> "Establish a testing strategy that includes acceptance criteria for new systems and models. Develop a plan for TEVV activities."

**AI Score**: 59%

### TE-2: Life cycle Cadence
> "Define the frequency and specific life cycle stages at which TEVV activities occur. Conduct regular testing both before and after deployment, including when any changes are implemented."

**AI Score**: 80%

### TE-3: Testing and Replication
> "Ensure the reproducibility of system outputs, model training, and testing results. Be able to replicate the results of third-party testing. Record testing results and make them available using replication files."

**AI Score**: 95%

### TE-4: Types of Testing
> "Evaluate system's validity, robustness, repeatability, and domain fit. Verify system changes. Assess third-party claims. Conduct regular red-teaming, penetration, and security testing."

**AI Score**: 81%

### TE-5: Review Results
> "Review the results of the TEVV process and resolve issues within a defined time period. Regularly reevaluate the effectiveness of TEVV metrics and processes."

**AI Score**: 78%

### TE-7: Expert Involvement
> "Engage subject matter and domain experts in the TEVV process. Have external experts conduct testing and validate results of internal testing. Undertake independent acceptance testing."

**AI Score**: 70%

### TE-8: Automating Testing
> "Implement automated testing and validation mechanisms that can verify against known facts or data. Use automated methods or generate synthetic data to expand the comprehensiveness of testing."

**AI Score**: 100%

## Connection to Existing Interventions

**Safety Benchmarks**: TE-3 and TE-5 directly support the need for reproducible, standardized benchmarks. The CSET report emphasizes that "greater testing and evaluation" is one of the five key focus areas of existing AI guidance.

**Evaluation Companies**: TE-7 explicitly calls for "external experts [to] conduct testing and validate results of internal testing." This directly supports the intervention for dedicated AI security evaluation organizations.

**CBRN Misuse Evaluations**: TE-4's mention of red-teaming and security testing aligns with systematic CBRN threat assessment. The report notes "CBRN security concerns" as part of expanded AI risks.

**Sandbagging Detection**: TE-3's emphasis on reproducibility and TE-5's requirement to "regularly reevaluate the effectiveness of TEVV metrics" relates to ensuring evaluations remain meaningful against sophisticated systems.
