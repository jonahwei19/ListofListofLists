# Ten AI Safety Projects (Julian Hazell) - Entry Mapping

**Source**: [Ten AI Safety Projects I'd Like People to Work On](https://www.lesswrong.com/posts/vxA2BnCPTaPfnJjti/ten-ai-safety-projects-i-d-like-people-to-work-on)
**Author**: Julian Hazell (funder perspective)
**Processed**: 2026-01-11
**Total projects**: 10

---

## Project Mapping

| # | Project | Maps To | Notes |
|---|---------|---------|-------|
| 1 | AI Security Field-Building | [[interventions/model-weight-security]] | Talent pipeline for securing weights, preventing exfiltration |
| 2 | Technical AI Governance Research Org | [[interventions/responsible-scaling-commitments]] | Enforcement mechanisms, verifiable auditing methods |
| 3 | Tracking Sketchy AI Agent Behavior | [[interventions/multi-agent-threat-monitoring]] | Real-world misalignment evidence from deployed agents |
| 4 | AI Safety Communications Consultancy | NEW | Communications infrastructure for safety orgs |
| 5 | AI Lab Monitor | [[interventions/responsible-scaling-commitments]] | Independent scrutiny of lab safety practices, scorecards |
| 6 | AI Safety Living Literature Reviews | NEW | Continuously updated synthesis on scheming, policy, etc. |
| 7 | $10 Billion AI Resilience Plan | NEW | Shovel-ready implementation blueprints for scale funding |
| 8 | AI Tools for Fact-Checking | [[interventions/crowdsourced-verification]] | Transparent AI fact-checking with visible reasoning |
| 9 | AI Auditors | [[interventions/responsible-scaling-commitments]] | Automated compliance audits for labs |
| 10 | Economic Impacts Tracker | NEW | Adoption metrics, productivity studies, hiring impacts |

---

## Mapping Details

### Direct matches (5)

**#1 AI Security Field-Building** maps to [[interventions/model-weight-security]]. Security talent pipeline for AI-specific challenges (weight security, data contamination, exfiltration attacks).

**#2 Technical AI Governance Research Org** maps to [[interventions/responsible-scaling-commitments]]. Fellowship program + research on enforcement mechanisms and verifiable auditing. Supports RSP implementation.

**#3 Tracking Sketchy AI Agent Behavior** maps to [[interventions/multi-agent-threat-monitoring]]. Systematically investigate deployed agents via anonymized logs, honeypots, case studies. Grounds policy in real-world evidence.

**#5 AI Lab Monitor** maps to [[interventions/responsible-scaling-commitments]]. Independent org producing quarterly scorecards on lab safety practices, scaling policies, corporate governance.

**#8 AI Tools for Fact-Checking** maps to [[interventions/crowdsourced-verification]]. Transparent AI fact-checking with open-source code and visible reasoning chains.

**#9 AI Auditors** maps to [[interventions/responsible-scaling-commitments]]. AI agents automating compliance audits - documentation review, safety procedure verification. Security-aware implementation.

---

### New intervention candidates (4)

**#4 AI Safety Communications Consultancy**: Specialized comms firm for AI safety orgs. Media training, op-ed placement, messaging frameworks. No existing intervention covers safety-specific communications infrastructure.

**#6 AI Safety Living Literature Reviews**: Continuously updated expert synthesis on topics like scheming, research agendas, policy. Addresses knowledge synthesis gap for policymakers. Related to evaluation-framework-consensus but distinct - focuses on synthesis rather than coordination.

**#7 $10 Billion AI Resilience Plan**: Comprehensive implementation blueprint for large-scale funding deployment. Intervention taxonomies, specific program structures, shovel-ready proposals. Meta-infrastructure for funding mobilization.

**#10 Economic Impacts Tracker**: Research org tracking real-world AI economic transformation (adoption metrics, productivity, hiring). Addresses gap between capability demos and actual deployment.

---

## Summary

| Category | Count |
|----------|-------|
| Maps to existing interventions | 6 |
| New intervention candidates | 4 |
| Total | 10 |

**Key theme**: Org-building and infrastructure for the AI safety field, rather than technical research directly. Heavy emphasis on external accountability (monitors, auditors) and knowledge synthesis.

**Funder signal**: Hazell explicitly wants to fund these, suggesting tractable organizational forms.
