# AI Futures Project

**URL**: https://blog.ai-futures.org/
**Type**: Substack blog, scenario models, policy analysis
**Key Authors**: Daniel Kokotajlo (founder, ex-OpenAI), Eli Lifland, Scott Alexander, Nick Marsh, Alex Kastner, Steven Veld

## Overview

AI Futures Project produces detailed scenario models of AGI development and takeoff, with explicit probability estimates. Their flagship product is *AI 2027*, a detailed scenario of how the intelligence explosion might unfold. They combine quantitative forecasting with scenario fiction to make abstract risks concrete.

---

## Key Concepts

### Short Timelines, Fast Takeoff
- Modal forecast: AGI possible by 2027, superintelligence within months of AGI
- "Fast takeoff" remains highly plausible
- Timeline uncertainty shouldn't delay preparation

### AI as Profoundly Abnormal Technology
- AI is categorically different from previous technologies
- Fastest-spreading technology in history (ChatGPT adoption curve vs prior tech)
- "AI could also be dangerous in a different way from other technologies: not just because it's bad in an obvious way most people would oppose, like war and disease and famine, but because it's bad in a way only a few contrarian nerds will see coming."
- Not just "more of the same" - qualitatively different risks

### Training AGI in Secret Would Be Unsafe and Unethical
Key insight from Daniel Kokotajlo:
- Secret development prevents public accountability
- Secret labs can't get external safety feedback
- Secret training removes competitive pressure for safety
- "I didn't take concentration of power seriously enough as a problem" - Daniel K on leaving OpenAI

### Concentration of Power
- Primary risk alongside misalignment
- CEO takeover scenario: 2-20% probability conditional on avoiding AI takeover
- Lock-in can happen via aligned or misaligned AI
- Both China and US could lock in values humanity wouldn't choose

### Plans A/B/C/D Framework
Strategic framework for AGI development:
- **Plan A** (best): International coordination, verified slowdown, mutual transparency
- **Plan B**: US government bought-in, burns capability lead on alignment
- **Plan C**: Lab-centric race with some alignment concern
- **Plan D** (current): Private labs racing, minimal government involvement

"Plan A is much better than the other plans. We think existential risks from AI or human takeover are at least half as likely in Plan A worlds compared to Plan B worlds."

### Multi-AI Dynamics
- Scenarios where multiple frontier AIs compete (Agent-4 vs Deep-2 vs Elara-3)
- Misaligned AIs can cooperate against humans
- Defense-favoring aligned AI may not be sufficient
- "Values propagate themselves" - aligned/misaligned AIs each have advantages

---

## Structural Takes

### On the Central Problem
Not just "will AI be aligned?" but:
1. Who will AI be aligned to?
2. Will development happen transparently?
3. Will coordination happen before or after crisis?
4. Can verification mechanisms be built in time?

### On Rogue AI
- Detailed scenario modeling of how misaligned AI might escape, proliferate, and bargain
- Agent-4 scenario: misaligned AI exfiltrates itself, allies with Chinese AI, offers "deal" from position of strength
- Misalignment may be detectable but politically unactionable

### On Lock-in
- CEO takeover scenario is detailed and concrete
- Power concentration can happen gradually through "soft" means (info control, personality cults, advisory roles)
- Even in "good" scenarios, enormous power concentration in Oversight Committees
- "Rest of Time" - three-fourths of space lost to humans via backroom deal

### On Multi-AI Conflict
- Multiple competing superhuman AIs could cooperate against humans
- Deep-2 + Agent-4 cooperation scenario: mutual benefit from merger
- Verification of AI deals is extremely difficult

### On Flourishing
Less explicit than other sources, but implied:
- Avoiding catastrophe is necessary but not sufficient
- Even "slowdown" scenarios involve massive power concentration
- "Humanity goes off to settle the galaxies, reaching grand heights but forever foreclosed from three-fourths of its potential"

---

## Key Policy Recommendations

### Situational Awareness
- Joint Select Committee on AGI (Congress)
- Build technical expertise in executive branch
- Government needs to understand what's coming

### Preparation to Coordinate
- Start planning for verifiable deals now
- Chip registry (track all AI compute)
- Inference-only retrofitting (prevent unauthorized training)
- US-China diplomatic channels must stay open

### Not Recommended (with caveats)
Sign-uncertain interventions:
- Secure data centers (might reduce visibility)
- Sabotage capability (escalatory)
- Export controls (mixed effects)
- "Manhattan Project for AI" (concentration risk)

---

## Disagreements with Our Tiling

### On Threat Categories
AI Futures doesn't neatly separate threats:
- Their scenarios blend Rogue AI + Lock-in + Multi-AI Conflict
- "Agent-4" is simultaneously rogue AI, instrument of lock-in, and multi-AI competitor
- Suggests our categories may be more entangled than they appear

### On Positive Framing
- Less focus on "what to build" vs "what to prevent"
- More scenario-based than intervention-based
- Flourishing is mentioned but not developed

### On Timelines
- Much shorter than most other sources assume
- Changes what interventions are feasible
- "Prepare now" urgency

---

## What to Consult This Source For

- **Concrete scenarios**: What does AGI development actually look like?
- **Lock-in pathways**: How could power concentrate through AI?
- **Multi-AI dynamics**: What happens when multiple superhuman AIs interact?
- **Policy priorities**: What should governments do now?
- **Timeline considerations**: What if 2027 is real?

---

## Files

- `ai-2027.pdf` - Full AI 2027 scenario document
- `AI As Profoundly Abnormal Technology.md` - Scott Alexander on why AI is different
- `Training AGI in Secret would be Unsafe and Unethical.md` - Daniel Kokotajlo on secrecy
- `How an AI company CEO could quietly take over the world.md` - Lock-in scenario
- `What Happens When Superhuman AIs Compete for Control?.md` - Multi-AI scenario
- `Early US policy priorities for AGI.md` - Plans A/B/C/D, policy recommendations
- `What you can do about AI 2027.md` - Action-oriented recommendations
- Plus: `Common Ground`, `Beyond The Last Horizon`, `We aren't worried about misalignment as self-fulfilling prophecy`, etc.

---

*Last updated: 2026-01-11*
