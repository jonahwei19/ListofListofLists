  

[

![AI Futures Project](https://substackcdn.com/image/fetch/$s_!iyu_!,w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3409cfd0-9243-479e-80f9-d0e3922c450a_132x132.png)



](https://blog.ai-futures.org/)

# [AI Futures Project](https://blog.ai-futures.org/)

# What you can do about AI 2027

### How to steer toward a positive AGI future

[](https://substack.com/@elifland)

[Eli Lifland](https://substack.com/@elifland)

Jun 28, 2025

[AI 2027](https://ai-2027.com/) features as one of its two endings a race ending in which humanity loses control of its destiny in 2027 and is extinct by 2030. How can we avoid this?

Below we share what we see as the best ways for you to improve humanity’s chances.

## Act with urgency but not certainty

We depicted AGI in 2027 because we think it’s a plausible outcome[1](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027#footnote-1-165282932) and society isn’t acting with anywhere near the appropriate urgency. We may only have 2 years left before humanity’s fate is sealed!

Despite the urgency, please do not pursue extreme uncooperative actions. If something seems very bad on common-sense ethical views, don’t do it.

If you can’t contribute now, keep in mind that AGI timelines are uncertain[2](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027#footnote-2-165282932) Our team’s median timelines range from 2028 to 2032. [AI progress may slow down in the 2030s](https://www.lesswrong.com/posts/XiMRyQcEyKCryST8T/slowdown-after-2028-compute-rlvr-uncertainty-moe-data-wall) if we don’t have AGI by then. Consider preparing to contribute if AGI arrives post-2027.

## Preparing for the intelligence explosion

Let’s imagine the world was up to the task of handling an intelligence explosion. What might that look like?

1. **Governments and the public understand that AGIs will dictate humanity’s future and might arrive soon.** There’s high-quality online discussion about AGI, companies disclose their internal AI capabilities, and governments have invested tens of billions into AGI preparedness. A world in which the public is informed about risks from superintelligence would be a [safer world](https://blog.ai-futures.org/p/training-agi-in-secret-would-be-unsafe).
    
2. **As companies automate AI R&D, governments are on high alert and take action**. Government agencies and nonprofits conduct regular interviews with top researchers at the companies. Companies report their estimates of AI R&D speedups based on surveys and uplift studies.
    
3. **Companies publish detailed [safety cases](https://arxiv.org/pdf/2403.10462) justifying why their AIs won’t cause catastrophic harm.** These are treated with much more seriousness than in industries that don’t pose an existential threat, such as cars. These argue that either (a) their AGIs aren’t adversarially misaligned or (b) even if they were, they wouldn’t be able to put us on a catastrophic path.[3](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027#footnote-3-165282932) The government, external organizations, academia, and the public engage deeply with these safety cases. If the safety cases aren’t strong enough, companies refrain from developing or deploying better AIs.
    
4. **Well-resourced teams inside and outside of AI companies do alignment research to better control AIs’ goals.** Alignment research is seen as a top priority with respect to attention and resourcing.
    
5. **It’s practically impossible for the CEO or POTUS to [use aligned AGIs to seize control of humanity’s future](https://www.forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power).** All of their queries to the models are logged and monitored. The [model spec and system prompt are public](https://blog.ai-futures.org/p/make-the-prompt-public) and red-teamed to prevent coups.
    
6. **The US and China coordinate to reduce competitive pressures, ensuring models aren’t developed without strong safety cases.** If necessary for safety, development is slowed. On-chip verification and inspectors allow for [trustless enforcement](https://intelligence.org/wp-content/uploads/2024/11/Mechanisms-to-Verify-International-Agreements-About-AI-Development-27-Nov-24.pdf) of an international deal.
    

The above is not an exhaustive list, but it covers some of our top priorities.

## If you’re in government or an AGI company

Our next project will have detailed recommendations for governments and AGI companies.[4](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027#footnote-4-165282932) In the meantime, we encourage focusing on steering toward the world described above.

## Learning

You might start by learning more about AGI-relevant topics. Along with [AI 2027](https://ai-2027.com/), we recommend the following regarding AGI forecasting and strategy (more in footnote):[5](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027#footnote-5-165282932)

1. The AGI-relevant episodes of the [Dwarkesh podcast](https://www.dwarkesh.com/) and the [80,000 Hours podcast](https://80000hours.org/podcast/)
    
2. [Situational Awareness](https://situational-awareness.ai/), though we think it underemphasizes international coordination and AGI alignment difficulty
    
3. [Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover](https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to)
    
4. [AI could defeat all of us combined](https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/)
    
5. [AI-Enabled Coups: How a Small Group Could Use AI to Seize Power](https://www.forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power)
    

Consider also going through this [technical AI alignment](https://bluedot.org/courses/alignment) or [AI governance course](https://bluedot.org/courses/governance) with a friend, or registering for the facilitated version, with a focus on the portions relevant to existential risks.

## Types of professional work

Many sorts of work can help. Below we list some of the most common ones along with specific opportunities:[6](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027#footnote-6-165282932)

1. **Governance/policy/forecasting research and advocacy.** Policy research focuses on determining what AI policies are both impactful and tractable, both in the near-term and during AI takeoff. Policy advocacy focuses on getting these policies implemented.
    
    1. Opportunities designed for entering the field include the [Horizon Fellowship](https://horizonpublicservice.org/programs/become-a-fellow/), [IAPS AI Policy Fellowship](https://www.iaps.ai/fellowship), [Pivotal Fellowship](https://www.pivotal-research.org/fellowship), and [ERA Fellowship](https://erafellowship.org/fellowship). We’ll also highlight RAND’s [Technology and Security Policy Fellowship](https://www.rand.org/global-and-emerging-risks/centers/technology-and-security-policy/fellows.html), [GovAI](https://www.governance.ai/opportunities), and our very own [AI Futures Project](https://docs.google.com/document/d/1r5YKOUi6gMUZoZvAEh0tHbbHM9RKYk6ONEkw_Tpizb0/preview?tab=t.0#heading=h.m73o24vkjlza).
        
2. **Technical research, evaluations, and demonstrations.** Research focuses on developing techniques to align and control increasingly capable AIs. Demonstrations and evaluations of AIs’ capabilities and goals help inform decision-makers and the public.
    
    1. The [MATS Program](https://www.matsprogram.org/apply) is for entering the field.[7](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027#footnote-7-165282932) We’ll also highlight [Redwood Research](https://www.redwoodresearch.org/careers), [METR](https://metr.org/careers), and [Apollo Research](https://www.apolloresearch.ai/careers). See also this [video with technical safety career advice](https://www.youtube.com/watch?v=OpufM6yK4Go).
        
3. **Beneficial AI applications:** Some applications of AI are especially beneficial for positive AGI outcomes, e.g. AI for decision-making and AI for coordination. This [blog post](https://www.forethought.org/research/ai-tools-for-existential-security) details some promising applications.
    
4. **Communications and journalism.** Help the public understand when AGI might come and the impact it will have.
    
    1. The [Tarbell fellowship](https://www.tarbellfellowship.org/programme) is for entering AI journalism.
        
5. **Infosecurity:** [Securing AI model weights](https://www.rand.org/pubs/research_reports/RRA2849-1.html) and algorithmic secrets is important for nonproliferation.
    
6. **Operations / other.** AI safety organizations, like others, also need various other skillsets, such as generalist operations staff and management capabilities.
    

[80,000 Hours](https://jobs.80000hours.org/?int_campaign=primary-navigation) and [AISafety.com](https://www.aisafety.com/jobs) have more comprehensive job boards, and 80,000 Hours gives [career advice](https://80000hours.org/speak-with-us).[8](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027#footnote-8-165282932)

## Non-professional activities

There’s also things to do without working full-time on AI safety, or in addition to doing so.

1. **Contribute to public discourse.** As AI improves, the amount of AI discourse will increase and the stakes will rise. Having reasonable voices on blogs, social media, podcasts, etc. will help improve societal decision-making. Organized public advocacy may also play an important role.
    
2. **Private discourse and informing others.** Having open conversations with friends, family, etc. about AGI may have significant effects. If you’re a college student, consider joining your college’s AI safety club or founding one.
    
3. **Donate.** Many AI safety organizations are funding-constrained. [Manifund](https://manifund.org/) contains a bunch of projects’ information (our information is [here](https://manifund.org/projects/ai-forecasting-and-policy-research-by-the-ai-2027-team)), or you can donate to an organization that we listed in the previous section. If you’re interested in donating >$200k [email us](mailto:info@ai-futures.org) and we may be able to advise you.
    

[1](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027#footnote-anchor-1-165282932)

In particular, first author Daniel Kokotajlo thinks it’s the most likely year that AGI will arrive, and is near his median forecast of 2028. My median is roughly 2032, but with AGI by 2027 as a serious possibility (~15-20%).

[2](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027#footnote-anchor-2-165282932)

But keep in mind that people sometimes contribute despite being in a position where it seems difficult! For example, [Encode](https://encodeai.org/) was founded by a [high schooler](https://encodeai.org/team-members/sneha-revanur/).

[3](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027#footnote-anchor-3-165282932)

For example of putting things on a “catastrophic path,” in AI 2027 Agent-4 aligns Agent-5 to itself rather than humanity. While this didn’t immediately cause a visible catastrophe, it did put humanity in a very precarious position due to Agent-5’s capabilities and level of autonomy.

[4](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027#footnote-anchor-4-165282932)

[AI Lab Watch](https://ailabwatch.org/) also has a detailed scorecard regarding how well AGI companies are doing on various safety metrics.

[5](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027#footnote-anchor-5-165282932)

Other reading recommendations left out of the main text for lack of space, pick what looks most interesting! [How AI Takeover Might Happen in 2 Years](https://www.alignmentforum.org/posts/KFJ2LFogYqzfGB3uX/how-ai-takeover-might-happen-in-2-years), [Preparing for the Intelligence Explosion](https://www.forethought.org/research/preparing-for-the-intelligence-explosion), [Is Power-Seeking AI an Existential Risk?](https://arxiv.org/abs/2206.13353), the [Most Important Century series](https://www.cold-takes.com/most-important-century/) and [Implications of the Most Important Century](https://www.cold-takes.com/tag/implicationsofmostimportantcentury/), [Why AI alignment could be hard with modern deep learning](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/), [Scheming AIs](https://arxiv.org/abs/2311.08379), [AGI Ruin: A List of Lethalities](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities) and [Where I agree and disagree with Eliezer](https://www.lesswrong.com/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer), [Will AI R&D Automation Cause a Software Intelligence Explosion?](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion), [Yudkowsky and Christiano discuss "Takeoff Speeds"](https://www.lesswrong.com/posts/vwLxd6hhFvPbvKmBH/yudkowsky-and-christiano-discuss-takeoff-speeds), [Clarifying and predicting AGI](https://www.lesswrong.com/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi)

[6](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027#footnote-anchor-6-165282932)

They’re selective, but err on the side of applying!

[7](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027#footnote-anchor-7-165282932)

MATS also has governance and policy tracks despite being mostly technical.

[8](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027#footnote-anchor-8-165282932)

[This post](https://www.cold-takes.com/jobs-that-can-help-with-the-most-important-century/) also recommends jobs to improve AGI outcomes. 80,000 Hours also has career profiles for AI [technical research](https://80000hours.org/career-reviews/ai-safety-researcher), [governance and policy](https://80000hours.org/career-reviews/ai-policy-and-strategy/), [China-related paths](https://80000hours.org/career-reviews/china-related-ai-safety-and-governance-paths/), [information security](https://80000hours.org/career-reviews/information-security/), and [hardware](https://80000hours.org/career-reviews/become-an-expert-in-ai-hardware/).

[

![AI Futures Project](https://substackcdn.com/image/fetch/$s_!iyu_!,w_96,h_96,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3409cfd0-9243-479e-80f9-d0e3922c450a_132x132.png)



](https://blog.ai-futures.org/)

#### Recommend AI Futures Project to your readers

Preparing for a world with AGI

[](https://substack.com/profile/966510-richard-shannon)[](https://substack.com/profile/35264457-ben-norman)[](https://substack.com/profile/135115-gerd-leonhard)[](https://substack.com/profile/8800260-brenton-milne)[](https://substack.com/profile/5933616-peter-wildeford)

158 Likes∙

[21 Restacks](https://substack.com/note/p-165282932/restacks?utm_source=substack&utm_content=facepile-restacks)

#### Discussion about this post

[](https://substack.com/profile/8631588-nathan-metzger?utm_source=comment)

[Nathan Metzger](https://substack.com/profile/8631588-nathan-metzger?utm_source=substack-feed-item)

[Nathan Metzger](https://41haiku.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)[Jun 28](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027/comment/130233990 "Jun 28, 2025, 2:29 PM")

One thing I would add to the non-professional activities is activism. Political cooperation on an issue is made significantly easier when there is a signal from the public that they view it as important.

I'm a volunteer with PauseAI, and while keeping my unrelated day job, I've been able to lobby state and federal congressional officials, do in-person outreach, host events, and hold a protest. I run a local PauseAI chapter with multiple members, and I was informed by my representative's office that I have personally had a positive impact on their awareness of AI risk, just through repeated phone calls and emails.

As the public becomes more aware of AI risk, it is important for them to know that they can have an outsized positive impact on the future just by speaking up and demanding reasonable regulations, and spreading further awareness of the risks and how to mitigate them.

Like (12)

Reply

Share

[3 replies](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027/comment/130233990)

[](https://substack.com/profile/283044707-gasstationmanager?utm_source=comment)

[GasStationManager](https://substack.com/profile/283044707-gasstationmanager?utm_source=substack-feed-item)

[Jun 28](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027/comment/130240275 "Jun 28, 2025, 2:54 PM")

If you are interested in AI safety, especially as related to coding, I recommend learning more about formal verification, and in particular what a modern interactive theorem prover like Lean can do.

Coding (and math) are domains in which you can (in theory) have trustless guarantees about the safety and correctness of the output of a superintelligent AI. How? Just ask the AI to prove a theorem (in e.g. Lean) about the correctness of its code. You can verify the proof by running it in the Lean proof checker, and only accept the code if the proof passes.

You could even do this across languages, e.g. code in Rust, proof in Lean (see [https://aeneasverif.github.io/](https://aeneasverif.github.io/))

In general, I would love to see more dialogue and exchange of ideas between AI Safety and formal verification & AI theorem proving communities.

Here's my own journey of discovery into this topic: [http://lean4ai.org](http://lean4ai.org/)

Working on the challenge of helping coding AIs be good at proving the correctness of its code.

Related reading: Towards Guaranteed Safe AI [https://arxiv.org/abs/2405.06624](https://arxiv.org/abs/2405.06624)

Like (5)

Reply

Share

[1 reply](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027/comment/130240275)

[25 more comments...](https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027/comments)

[Early US policy priorities for AGI](https://blog.ai-futures.org/p/early-us-policy-priorities-for-agi)

[Near-term AI policy is confusing, except for these two recommendations](https://blog.ai-futures.org/p/early-us-policy-priorities-for-agi)

Dec 8, 2025 • [Nick Marsh](https://substack.com/@zzzdot)

72

12

9

![](https://substackcdn.com/image/fetch/$s_!kyVK!,w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19288615-5e21-4586-83fb-4ed8c4a3ac4d_2565x1115.png)

[AI As Profoundly Abnormal Technology](https://blog.ai-futures.org/p/ai-as-profoundly-abnormal-technology)

[....](https://blog.ai-futures.org/p/ai-as-profoundly-abnormal-technology)

Jul 24, 2025 • [Scott Alexander](https://substack.com/@astralcodexten)

224

47

49

![](https://substackcdn.com/image/fetch/$s_!Avx0!,w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a7e6fb0-72f0-4467-a251-ba0f3a4e4c73_1302x719.png)

[AI Futures Model: Dec 2025 Update](https://blog.ai-futures.org/p/ai-futures-model-dec-2025-update)

[We've significantly improved our model(s) of AI timelines & takeoff speeds!](https://blog.ai-futures.org/p/ai-futures-model-dec-2025-update)

Dec 30, 2025 • [Daniel Kokotajlo](https://substack.com/@danielkokotajlo), [Eli Lifland](https://substack.com/@elifland), [Brendan Halstead](https://substack.com/@brendanhalstead), and [Alex Kastner](https://substack.com/@alexkastner)

149

40

17

![](https://substackcdn.com/image/fetch/$s_!0U8K!,w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2bf7c4b-33b1-4411-acb4-ffe5c2178fa0_1600x906.png)

© 2026 AI Futures Project · [Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start your Substack](https://your.substack.com/publish)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture