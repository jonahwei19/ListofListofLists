# AI Advantage for Offense Over Defense in Cyber

**Source**: Atlas AI Resilience Gap Map
**URL**: (internal document from Atlas Computing)

## Quoted passage

> gap: AI systems appear to be more useful for offensive than defensive cyber capabilities. These risks could arise from current threats being scaled up in volume, or they could be novel in their severity. This asymmetry could enable low-skill adversaries to execute sophisticated broad-sweeping attacks on critical software infrastructure.

> claim: awareness is sufficient to direct resources to this problem

> Index and track possible usages of AI for cybersecurity risks (e.g. MITRE ATLAS). Also track possible future risks

> claim: The AI systems won't be meaningfully better than humans in finding exploits; we need to prevent common exploits asap.

> Rewrite everything in Rust: DARPA TRACTOR, https://ifp.org/the-great-refactor/

> claim: AI red-teaming capabilities will soon surpass those of humans in scale and sophistication, and these capabilities could be available to adversaries; we need to eliminate as many bugs as feasible

> Formal verification is asymmetrically defensive, an AI systems should be able to make formal verification radically cheaper and faster. We need better tools for specification generation and validation, coupled with mechanisms to either autoformalize from specs or prevent anomalous behavior within systems without requiring a system upgrade

> There should (and hopefully soon will) be a FRO that makes it radically simpler to validate formal specifications without deep expertise in formal logic. Separately, there should be companies that can leverage growing AI capabilities in program and proof synthesis to autoformalize software. There may also need to be an entity (likely a nonprofit) to house sets of recommended security specifications that are generalized and reusable (e.g. related to memory safety or proving subroutines terminate)

## Tags
Security
