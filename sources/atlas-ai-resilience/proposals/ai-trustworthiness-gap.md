# AI Capabilities Outpacing Trustworthiness Tools

**Source**: Atlas AI Resilience Gap Map
**URL**: (internal document from Atlas Computing)

## Quoted passage

> gap: Growth in AI capabilities may greatly outpace tools for testing and ensuring model trustworthiness, controllability, and interpretability. We may deploy systems without tools needed to provide confidence that AI systems are free from hidden failure modes, backdoors, or adversarially-inserted behaviors that could manifest in production.

> claim: perhaps meaningfully better, cleaner training data is sufficient for AI systems to be trustworthy, reliable, and robust

> A set of tools that make it cheap and easy for frontier labs to filter training data.

> claim: perhaps known and proposed best practices are sufficient

> There should be an organization that creates a maximally trustworthy AI model far behind the frontier (e.g. 7B param), using a mid 8-figure budget to implement rigorous security and reliability best practices (e.g. filtered training data, continuous evaluation, adversarial testing). This would establish cost/performance trade-offs for high-assurance AI systems, and provide a valuable public good for applications like security evals or classifiers.

> claim: reliability improvements are limited by insufficient data on failure modes and unexpected behaviors

> A set of tools that provide useful metrics and make it easy to upload anonymized reports of unexpected behaviors and failure modes (e.g. for coding agents)

> claim: AI control or formal oversight are an important ingredient to improve deployment confidence

> (partial) New tools that can popularize known AI control or proposed formal oversight mechanisms (e.g. AI reviewing pull-requests) while improving security. This can ease later adoption when these techniques may be load-bearing for securing AI

> claim: mechanistic interpretability and diagnostic tools are an important ingredient to improve system reliability

> (partial) Publicly accessible benchmark AI systems acting as "model organisms for sleeper agent AIs" that can be used for testing diagnostic and interpretability tools (e.g. can you detect models with deliberately introduced failure modes or adversarial behaviors?)

## Tags
Science
