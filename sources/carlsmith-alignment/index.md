# How to Solve the Alignment Problem (Carlsmith)

**Source**: Joe Carlsmith essay series, joecarlsmith.com
**Original**: https://joecarlsmith.com
**Date processed**: 2026-01-11
**Total items**: 9 essays

## Overview

Joe Carlsmith's comprehensive analysis of AI alignment: what the problem is, what conditions create risk, and intervention strategies. Primary source for refining the Rogue AI category and the "hand-off" framing.

## Essays

| Essay | Key Contribution |
|-------|------------------|
| Introduction | Series overview and scope |
| What is it to solve the alignment problem | Definition: AI pursues intended goals without bad side effects |
| When should we worry about AI power seeking | "Global vulnerability conditions" - when alignment becomes critical |
| Paths and waystations in AI safety | Phases of the problem, sequential dependencies |
| Controlling the Options AI's can pursue | "Option control" - restricting what AI can do |
| Giving AI's safe motivations | "Motivation control" - shaping what AI wants |
| How human-like do safe AI motivations need to be | Whether human-like values are necessary or sufficient |
| Can we safely automate alignment research | Using AI for alignment with appropriate safeguards |
| AI for AI safety | Specific applications of AI to safety work |

## Key Ideas

**Rogue vs Misuse**: Clear distinction between AI pursuing unauthorized goals (rogue) and humans using AI for harm (misuse). Used to sharpen Rogue AI definition.

**Hand-off Framing**: Voluntary delegation to aligned AI that remains responsive and correctable is fine. The threat is unauthorized + uncorrectable, not "AI in control" per se. Key refinement to Rogue AI category.

**Option Control vs Motivation Control**: Two complementary strategies:
- Option control: Restrict what AI can do (containment, monitoring, access limits)
- Motivation control: Shape what AI wants (alignment, value learning)

Defense-in-depth requires both.

**Global Vulnerability Conditions**: When humanity's continued empowerment depends on AI not choosing to seek power and/or containment working perfectly. Identifies when we're in the danger zone.

**Power-Seeking Analysis**: Power-seeking isn't automatic given misalignment. Depends on: payout attractiveness, probability of success, cost of failure, path costs, opportunity cost. Shapes kill chain step 3.

## Mapping to Categories

| Concept | Category | Usage |
|---------|----------|-------|
| Rogue vs misuse distinction | Rogue AI | Definition refinement |
| Hand-off framing | Rogue AI | Key clarification (hand-off can be good) |
| Option control | Rogue AI | Defense-in-depth strategy (steps 4-5) |
| Motivation control | Rogue AI | Defense-in-depth strategy (step 2) |
| Global vulnerability conditions | Rogue AI | When does alignment become critical |
| Power-seeking calculus | Rogue AI | Kill chain step 3 analysis |

## Status

Fully integrated into Rogue AI category. Primary intellectual source for:
- Definition (unauthorized + uncorrectable)
- Kill chain structure (6 steps)
- Defense-in-depth framework
- "Hand-off could be good" refinement
